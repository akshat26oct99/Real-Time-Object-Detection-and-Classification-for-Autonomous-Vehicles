{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster-RCNN-Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9393099d07a1430780a8c3aaac1a4cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d84d6ed145846d7b17a9ad71dfb7687",
              "IPY_MODEL_4e8d40d329a948f591547473e7899e57"
            ],
            "layout": "IPY_MODEL_6dcd36a13d594295948d1efe9850d3d9"
          }
        },
        "7d84d6ed145846d7b17a9ad71dfb7687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00c634d48e07497d8ea69868e36432df",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41861cf80b4640528a9e4ac7cea4035a",
            "value": 553433881
          }
        },
        "4e8d40d329a948f591547473e7899e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a169ce683342c9bdf82e5ac756da7a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_78d51fe5653948a4a2a93768e2cb391f",
            "value": " 528M/528M [00:34&lt;00:00, 16.2MB/s]"
          }
        },
        "6dcd36a13d594295948d1efe9850d3d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00c634d48e07497d8ea69868e36432df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41861cf80b4640528a9e4ac7cea4035a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "46a169ce683342c9bdf82e5ac756da7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d51fe5653948a4a2a93768e2cb391f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85b41ad3b8f345bfba92929265da5003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cea653080c0649da82722c239aa3c810",
              "IPY_MODEL_195a8d76326040299576f4acd06a4c85"
            ],
            "layout": "IPY_MODEL_694e425b3597404c8858bba91cff889e"
          }
        },
        "cea653080c0649da82722c239aa3c810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9af17beca2424fe79fec666cff92fb11",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd51a2bfa2d346ae9e1c99b39e5be5f8",
            "value": 553433881
          }
        },
        "195a8d76326040299576f4acd06a4c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_379371b9f84544ccbdab51b92b2fa092",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a92bb52577344380bd46dba72f5b45db",
            "value": " 528M/528M [00:13&lt;00:00, 42.2MB/s]"
          }
        },
        "694e425b3597404c8858bba91cff889e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af17beca2424fe79fec666cff92fb11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd51a2bfa2d346ae9e1c99b39e5be5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "379371b9f84544ccbdab51b92b2fa092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a92bb52577344380bd46dba72f5b45db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5OcuI_Nynhv",
        "outputId": "b36b329a-b872-4478-da73-4779e8a5abed"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "from PIL import Image\r\n",
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torchvision import transforms\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E9eTtvCaWMw"
      },
      "source": [
        "class RPN(nn.Module):\r\n",
        "    \"\"\"Region proposal network\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, channels, n_anchor):\r\n",
        "        super(RPN, self).__init__()\r\n",
        "        \r\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\r\n",
        "        self.reg_layer = nn.Conv2d(channels, n_anchor *4, 1, 1, 0)\r\n",
        "        self.cls_layer = nn.Conv2d(channels, n_anchor *2, 1, 1, 0) ##use softmax here. equally use sigmoid if replacing 2 with 1.\r\n",
        "\r\n",
        "        # conv sliding layer\r\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\r\n",
        "        self.conv1.bias.data.zero_()\r\n",
        "        # Regression layer\r\n",
        "        self.reg_layer.weight.data.normal_(0, 0.01)\r\n",
        "        self.reg_layer.bias.data.zero_()\r\n",
        "        # classification layer\r\n",
        "        self.cls_layer.weight.data.normal_(0, 0.01)\r\n",
        "        self.cls_layer.bias.data.zero_()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv1(x)\r\n",
        "        pred_anchor_locs = self.reg_layer(x)\r\n",
        "        pred_cls_scores = self.cls_layer(x)\r\n",
        "        #print(pred_cls_scores.shape, pred_anchor_locs.shape)\r\n",
        "        #Out:\r\n",
        "        #torch.Size([1, 18, 32, 50]) torch.Size([1, 36, 32, 50])\r\n",
        "        #print(out_map.shape)\r\n",
        "        return pred_anchor_locs, pred_cls_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVozblwPI6SA"
      },
      "source": [
        "class Fast_RCNN(nn.Module):\r\n",
        "    \"\"\"fast RCNN\"\"\"\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(Fast_RCNN, self).__init__()\r\n",
        "        self.roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096),\r\n",
        "                                              nn.Linear(4096, 4096)])\r\n",
        "        self.cls_loc = nn.Linear(4096, (n_classes + 1) * 4) # (n_classes + 1 background. Each will have 4 co-ordinates)\r\n",
        "        self.cls_loc.weight.data.normal_(0, 0.01)\r\n",
        "        self.cls_loc.bias.data.zero_()\r\n",
        "        self.score = nn.Linear(4096, n_classes +1) # (n_classes + 1 background)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        x = self.roi_head_classifier(x)\r\n",
        "        roi_cls_loc = self.cls_loc(x)\r\n",
        "        roi_cls_score = self.score(x)\r\n",
        "        #print(roi_cls_loc.shape, roi_cls_score.shape)\r\n",
        "        #Out:\r\n",
        "        # torch.Size([128, 84]), torch.Size([128, 21])\r\n",
        "        return roi_cls_loc, roi_cls_score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBf7H6MQ1Fyj"
      },
      "source": [
        "def create_fe_extractor():\r\n",
        "    \"\"\" return layers for backbone network and number of channels in the output_map \r\n",
        "    \"\"\"\r\n",
        "    dummy_img = torch.zeros((1, 3, height, width)).float()\r\n",
        "    model = torchvision.models.vgg16(pretrained=True)\r\n",
        "    fe = list(model.features)\r\n",
        "    req_features = []\r\n",
        "    k = dummy_img.clone()\r\n",
        "    for i in fe:\r\n",
        "         k = i(k)\r\n",
        "         #print(k.size())\r\n",
        "         if k.size()[3] < width//sub_sample:\r\n",
        "             break\r\n",
        "         req_features.append(i)\r\n",
        "         out_channels = k.size()[1]\r\n",
        "    #print(len(req_features)) #30\r\n",
        "    #print(out_channels) # 512\r\n",
        "    channels = out_channels\r\n",
        "    \r\n",
        "    return req_features, channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsLW40Ft5dBP"
      },
      "source": [
        "def calc_anchors():\r\n",
        "    \"\"\"\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)\r\n",
        "    #print(anchor_base)\r\n",
        "    ctr_y = sub_sample / 2.\r\n",
        "    ctr_x = sub_sample / 2.\r\n",
        "\r\n",
        "    #print(ctr_y, ctr_x)\r\n",
        "    # Out: (8, 8)\r\n",
        "    for i in range(len(ratios)):\r\n",
        "        for j in range(len(anchor_scales)):\r\n",
        "              h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\r\n",
        "              w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\r\n",
        "\r\n",
        "              index = i * len(anchor_scales) + j\r\n",
        "\r\n",
        "              anchor_base[index, 0] = ctr_y - h / 2.\r\n",
        "              anchor_base[index, 1] = ctr_x - w / 2.\r\n",
        "              anchor_base[index, 2] = ctr_y + h / 2.\r\n",
        "              anchor_base[index, 3] = ctr_x + w / 2.\r\n",
        "\r\n",
        "    fe_size_x = (width//sub_sample)\r\n",
        "    fe_size_y = (height//sub_sample)\r\n",
        "    #print(fe_size_x)\r\n",
        "    #print(fe_size_y)\r\n",
        "\r\n",
        "    ctr_x = np.arange(sub_sample, (fe_size_x+1) * sub_sample, sub_sample)\r\n",
        "    ctr_y = np.arange(sub_sample, (fe_size_y+1) * sub_sample, sub_sample)\r\n",
        "\r\n",
        "    ctr = np.zeros((len(ctr_x) * len(ctr_y),2))\r\n",
        "    index = 0\r\n",
        "    for x in range(len(ctr_x)):\r\n",
        "        for y in range(len(ctr_y)):\r\n",
        "            ctr[index, 1] = ctr_x[x] - 8\r\n",
        "            ctr[index, 0] = ctr_y[y] - 8\r\n",
        "            index +=1\r\n",
        "\r\n",
        "    anchors = np.zeros(((fe_size_x * fe_size_y * 9), 4))\r\n",
        "    index = 0\r\n",
        "    for c in ctr:\r\n",
        "       ctr_y, ctr_x = c\r\n",
        "       for i in range(len(ratios)):\r\n",
        "           for j in range(len(anchor_scales)):\r\n",
        "               h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\r\n",
        "               w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\r\n",
        "               anchors[index, 0] = ctr_y - h / 2.\r\n",
        "               anchors[index, 1] = ctr_x - w / 2.\r\n",
        "               anchors[index, 2] = ctr_y + h / 2.\r\n",
        "               anchors[index, 3] = ctr_x + w / 2.\r\n",
        "               index += 1\r\n",
        "    \r\n",
        "    return anchors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7b6kvGj8NMW"
      },
      "source": [
        "def calc_valid_anchors_and_labels(anchors):\r\n",
        "    \"\"\" returns all anchors which aren't overlap the image border and label\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    index_inside = np.where(#??????????????????????????????????????????????????????????????????????????????????????\r\n",
        "            (anchors[:, 0] >= 0) &\r\n",
        "            (anchors[:, 1] >= 0) &\r\n",
        "            (anchors[:, 2] <= 512) &\r\n",
        "            (anchors[:, 3] <= 800)\r\n",
        "        )[0]\r\n",
        "\r\n",
        "    #print(index_inside.shape)\r\n",
        "    #Out: (4400,)\r\n",
        "\r\n",
        "    label = np.empty((len(index_inside), ), dtype=np.int32)\r\n",
        "    label.fill(-1)\r\n",
        "    #print(label.shape)\r\n",
        "    #Out = (4400, )\r\n",
        "\r\n",
        "    valid_anchor_boxes = anchors[index_inside]\r\n",
        "    #print(valid_anchor_boxes.shape)\r\n",
        "    #Out = (4400, 4)\r\n",
        "\r\n",
        "    return label, valid_anchor_boxes, index_inside\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n52mlZcVMVhV"
      },
      "source": [
        "def calc_ious(bboxes1, bboxes2):\r\n",
        "    \"\"\" Calculate ious with vectorization\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\r\n",
        "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\r\n",
        "    xA = np.maximum(x11, np.transpose(x21))\r\n",
        "    yA = np.maximum(y11, np.transpose(y21))\r\n",
        "    xB = np.minimum(x12, np.transpose(x22))\r\n",
        "    yB = np.minimum(y12, np.transpose(y22))\r\n",
        "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\r\n",
        "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\r\n",
        "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\r\n",
        "    ious = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\r\n",
        "    return ious"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7--_qLWPVnj"
      },
      "source": [
        "def calc_anchor_locations_and_labels(valid_anchor_boxes, ious, label):\r\n",
        "    \"\"\" calculate the anchor locatios and anchor labels\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    #case 1 the highest iou for each gt_box and its corresponding anchor box\r\n",
        "    gt_argmax_ious = ious.argmax(axis=0)\r\n",
        "    #print(gt_argmax_ious)\r\n",
        "    gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\r\n",
        "    #print(gt_max_ious)\r\n",
        "\r\n",
        "    #case 2 the highest iou for each anchor box and its corresponding ground truth box\r\n",
        "    argmax_ious = ious.argmax(axis=1)\r\n",
        "    #print(argmax_ious.shape)\r\n",
        "    #print(argmax_ious)\r\n",
        "    max_ious = ious[np.arange(len(index_inside)), argmax_ious]\r\n",
        "    #print(max_ious)\r\n",
        "\r\n",
        "    gt_argmax_ious = np.where(ious == gt_max_ious)[0]\r\n",
        "    #print(gt_argmax_ious)\r\n",
        "\r\n",
        "    label[max_ious < neg_iou_threshold] = 0\r\n",
        "    label[gt_argmax_ious] = 1\r\n",
        "    label[max_ious >= pos_iou_threshold] = 1\r\n",
        "\r\n",
        "    n_pos = 0.5 * n_sample\r\n",
        "    #positive samples\r\n",
        "    pos_index = np.where(label == 1)[0]\r\n",
        "    if len(pos_index) > n_pos:\r\n",
        "        disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\r\n",
        "        label[disable_index] = -1\r\n",
        "\r\n",
        "    #negative samples\r\n",
        "    n_neg = n_sample * np.sum(label == 1)\r\n",
        "    neg_index = np.where(label == 0)[0]\r\n",
        "    if len(neg_index) > n_neg:\r\n",
        "        disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\r\n",
        "        label[disable_index] = -1\r\n",
        "\r\n",
        "    #For each anchor box, find the groundtruth object which has max_iou\r\n",
        "    max_iou_bbox = bbox[argmax_ious]\r\n",
        "\r\n",
        "    box_height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0] \r\n",
        "    box_width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\r\n",
        "    ctr_y = valid_anchor_boxes[:, 0] + 0.5 * box_height\r\n",
        "    ctr_x = valid_anchor_boxes[:, 1] + 0.5 * box_width\r\n",
        "    base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\r\n",
        "    base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\r\n",
        "    base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\r\n",
        "    base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\r\n",
        "\r\n",
        "    #find the locs\r\n",
        "    eps = np.finfo(box_height.dtype).eps\r\n",
        "    box_height = np.maximum(box_height, eps)\r\n",
        "    box_width = np.maximum(box_width, eps)\r\n",
        "    dy = (base_ctr_y - ctr_y) / box_height\r\n",
        "    dx = (base_ctr_x - ctr_x) / box_width\r\n",
        "    dh = np.log(base_height / box_height)\r\n",
        "    dw = np.log(base_width / box_width)\r\n",
        "    anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\r\n",
        "    #print(anchor_locs)\r\n",
        "\r\n",
        "    #final labels\r\n",
        "    anchor_labels = np.empty((len(anchors),), dtype=label.dtype)\r\n",
        "    anchor_labels.fill(-1)\r\n",
        "    anchor_labels[index_inside] = label\r\n",
        "\r\n",
        "    #final locations\r\n",
        "    anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\r\n",
        "    anchor_locations.fill(0)\r\n",
        "    anchor_locations[index_inside, :] = anchor_locs\r\n",
        "\r\n",
        "    return anchor_labels, anchor_locations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTLBGg8Wr-ep"
      },
      "source": [
        "def generate_regions_of_interest(is_training):\r\n",
        "    \"\"\" generate_region_proposals\r\n",
        "    \"\"\"\r\n",
        "    n_pre_nms = n_train_pre_nms if is_training else n_test_pre_nms\r\n",
        "    n_post_nms = n_train_post_nms if is_training else n_test_post_nms\r\n",
        "    \r\n",
        "    anc_height = anchors[:, 2] - anchors[:, 0]\r\n",
        "    anc_width = anchors[:, 3] - anchors[:, 1]\r\n",
        "    anc_ctr_y = anchors[:, 0] + 0.5 * anc_height\r\n",
        "    anc_ctr_x = anchors[:, 1] + 0.5 * anc_width\r\n",
        "\r\n",
        "    pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()\r\n",
        "    objectness_score_numpy = objectness_score[0].data.numpy()\r\n",
        "    dy = pred_anchor_locs_numpy[:, 0::4]\r\n",
        "    dx = pred_anchor_locs_numpy[:, 1::4]\r\n",
        "    dh = pred_anchor_locs_numpy[:, 2::4]\r\n",
        "    dw = pred_anchor_locs_numpy[:, 3::4]\r\n",
        "    ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\r\n",
        "    ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\r\n",
        "    h = np.exp(dh) * anc_height[:, np.newaxis]\r\n",
        "    w = np.exp(dw) * anc_width[:, np.newaxis]\r\n",
        "\r\n",
        "    roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=np.float32) #pred_anchor_locs.numpy.dtype) KOMISCH ?!?!?!!?!?!?!?\r\n",
        "    roi[:, 0::4] = ctr_y - 0.5 * h\r\n",
        "    roi[:, 1::4] = ctr_x - 0.5 * w\r\n",
        "    roi[:, 2::4] = ctr_y + 0.5 * h\r\n",
        "    roi[:, 3::4] = ctr_x + 0.5 * w\r\n",
        "\r\n",
        "    #img_size = (height, width) #Image size\r\n",
        "    #print(img_size)\r\n",
        "\r\n",
        "    roi[:, slice(0, 4, 2)] = np.clip(\r\n",
        "                roi[:, slice(0, 4, 2)], 0, img_size[0])\r\n",
        "    roi[:, slice(1, 4, 2)] = np.clip(\r\n",
        "        roi[:, slice(1, 4, 2)], 0, img_size[1])\r\n",
        "\r\n",
        "    #print(roi)\r\n",
        "\r\n",
        "    hs = roi[:, 2] - roi[:, 0]\r\n",
        "    ws = roi[:, 3] - roi[:, 1]\r\n",
        "    keep = np.where((hs >= min_size) & (ws >= min_size))[0]\r\n",
        "    roi = roi[keep, :]\r\n",
        "    score = objectness_score_numpy[keep]\r\n",
        "    #print(score.shape)\r\n",
        "\r\n",
        "    ordered_scores = score.ravel().argsort()[::-1]\r\n",
        "    #print(ordered_scores)\r\n",
        "\r\n",
        "    ordered_scores = ordered_scores[:n_pre_nms]\r\n",
        "    roi = roi[ordered_scores, :]\r\n",
        "\r\n",
        "    #print(roi.shape)\r\n",
        "    #print(roi)\r\n",
        "\r\n",
        "    y1 = roi[:, 0]\r\n",
        "    x1 = roi[:, 1]\r\n",
        "    y2 = roi[:, 2]\r\n",
        "    x2 = roi[:, 3]\r\n",
        "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n",
        "    order = ordered_scores.argsort()[::-1]\r\n",
        "    keep = []\r\n",
        "    while (order.size > 0):\r\n",
        "        i = order[0]\r\n",
        "        keep.append(i)\r\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\r\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\r\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\r\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\r\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\r\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\r\n",
        "        inter = w * h\r\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\r\n",
        "        inds = np.where(ovr <= nms_thresh)[0]\r\n",
        "        order = order[inds + 1]\r\n",
        "    keep = keep[:n_post_nms] # while training/testing , use accordingly\r\n",
        "    roi = roi[keep] # the final region proposals\r\n",
        "\r\n",
        "    return roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5jVFQWwEiAM"
      },
      "source": [
        "def create_region_proposals():\r\n",
        "    \"\"\" Create sample rois, ground truth roi labels and ground truth roi locations\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    gt_assignment = ious.argmax(axis=1)\r\n",
        "    max_iou = ious.max(axis=1)\r\n",
        "    #print(gt_assignment)\r\n",
        "    #print(max_iou)\r\n",
        "\r\n",
        "    gt_roi_label = labels[gt_assignment]\r\n",
        "    #print(gt_roi_label) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n",
        "\r\n",
        "    pos_roi_per_image = int(n_sample * pos_ratio)\r\n",
        "    pos_index = np.where(max_iou >= pos_iou_thresh)[0]\r\n",
        "    pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\r\n",
        "    if pos_index.size > 0:\r\n",
        "        pos_index = np.random.choice(\r\n",
        "            pos_index, size=pos_roi_per_this_image, replace=False)\r\n",
        "    #print(pos_roi_per_this_image)\r\n",
        "    #print(pos_index)\r\n",
        "\r\n",
        "    neg_index = np.where((max_iou < neg_iou_thresh_hi) &\r\n",
        "                             (max_iou >= neg_iou_thresh_lo))[0]\r\n",
        "    neg_roi_per_this_image = n_sample - pos_roi_per_this_image\r\n",
        "    neg_roi_per_this_image = int(min(neg_roi_per_this_image,\r\n",
        "                                 neg_index.size))\r\n",
        "    if  neg_index.size > 0 :\r\n",
        "        neg_index = np.random.choice(\r\n",
        "            neg_index, size=neg_roi_per_this_image, replace=False)\r\n",
        "    #print(neg_roi_per_this_image)\r\n",
        "    #print(neg_index)\r\n",
        "    keep_index = np.append(pos_index, neg_index)\r\n",
        "    gt_roi_labels = gt_roi_label[keep_index]\r\n",
        "    gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\r\n",
        "    sample_roi = roi[keep_index]\r\n",
        "    #print(sample_roi.shape)\r\n",
        "\r\n",
        "    bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\r\n",
        "    #print(bbox_for_sampled_roi.shape)\r\n",
        "    #Out\r\n",
        "    #(128, 4)\r\n",
        "    h = sample_roi[:, 2] - sample_roi[:, 0]\r\n",
        "    w = sample_roi[:, 3] - sample_roi[:, 1]\r\n",
        "    ctr_y = sample_roi[:, 0] + 0.5 * h\r\n",
        "    ctr_x = sample_roi[:, 1] + 0.5 * w\r\n",
        "    base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\r\n",
        "    base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\r\n",
        "    base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height\r\n",
        "    base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width\r\n",
        "\r\n",
        "    eps = np.finfo(h.dtype).eps\r\n",
        "    h = np.maximum(h, eps)\r\n",
        "    w = np.maximum(w, eps)\r\n",
        "    dy = (base_ctr_y - ctr_y) / h\r\n",
        "    dx = (base_ctr_x - ctr_x) / w\r\n",
        "    dh = np.log(base_height / h)\r\n",
        "    dw = np.log(base_width / w)\r\n",
        "    gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\r\n",
        "    #print(gt_roi_locs) ############################## now gt_roi_locs and gt_roi_labels\r\n",
        "\r\n",
        "    return sample_roi, gt_roi_labels, gt_roi_locs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTwDFzKrLYDO"
      },
      "source": [
        "def roi_pooling(sample_roi):\r\n",
        "    \"\"\" calculates roi pooling for sample_rois\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    rois = torch.from_numpy(sample_roi).float()\r\n",
        "    roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\r\n",
        "    roi_indices = torch.from_numpy(roi_indices).float()\r\n",
        "    #print(rois.shape, roi_indices.shape)\r\n",
        "    #Out:\r\n",
        "    #torch.Size([128, 4]) torch.Size([128])\r\n",
        "\r\n",
        "    indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\r\n",
        "    xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]  \r\n",
        "    indices_and_rois = xy_indices_and_rois.contiguous()\r\n",
        "    #print(xy_indices_and_rois.shape)\r\n",
        "    size = (7, 7)\r\n",
        "    adaptive_max_pool = nn.AdaptiveMaxPool2d(size)\r\n",
        "    output = []\r\n",
        "    rois = indices_and_rois.data.float()\r\n",
        "    rois[:, 1:].mul_(1/16.0) # Subsampling ratio\r\n",
        "    rois = rois.long()\r\n",
        "    num_rois = rois.size(0)\r\n",
        "    for i in range(num_rois):\r\n",
        "        roi = rois[i]\r\n",
        "        im_idx = roi[0]\r\n",
        "        im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\r\n",
        "        output.append(adaptive_max_pool(im))\r\n",
        "    output = torch.cat(output, 0)\r\n",
        "    #print(output.size())\r\n",
        "    #Out:\r\n",
        "    # torch.Size([128, 512, 7, 7])\r\n",
        "    # Reshape the tensor so that we can pass it through the feed forward layer.\r\n",
        "    k = output.view(output.size(0), -1)\r\n",
        "    #print(k.shape)\r\n",
        "    #Out:\r\n",
        "    # torch.Size([128, 25088])\r\n",
        "    return k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWrNPzGBRb4X"
      },
      "source": [
        "def calc_rpn_loss():\r\n",
        "    \"\"\" calculate the rpns loss \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #rpn classification loss\r\n",
        "    rpn_loc = pred_anchor_locs[0]\r\n",
        "    rpn_score = pred_cls_scores[0]\r\n",
        "    gt_rpn_loc = torch.from_numpy(anchor_locations)\r\n",
        "    gt_rpn_score = torch.from_numpy(anchor_labels)\r\n",
        "    #print(rpn_loc.shape, rpn_score.shape, gt_rpn_loc.shape, gt_rpn_score.shape)  \r\n",
        "    rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long(), ignore_index = -1)\r\n",
        "    #print(rpn_cls_loss)\r\n",
        "\r\n",
        "    #rpn regression loss\r\n",
        "    pos = gt_rpn_score > 0\r\n",
        "    mask = pos.unsqueeze(1).expand_as(rpn_loc)\r\n",
        "    #print(mask.shape)\r\n",
        "    mask_loc_preds = rpn_loc[mask].view(-1, 4)\r\n",
        "    mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\r\n",
        "    #print(mask_loc_preds.shape, mask_loc_preds.shape)\r\n",
        "    x = torch.abs(mask_loc_targets - mask_loc_preds)\r\n",
        "    rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))\r\n",
        "    #print(rpn_loc_loss.sum())\r\n",
        "    N_reg = (gt_rpn_score >0).float().sum()\r\n",
        "    rpn_loc_loss = rpn_loc_loss.sum() / N_reg\r\n",
        "\r\n",
        "    rpn_loss = rpn_cls_loss + rpn_lambda*rpn_loc_loss\r\n",
        "\r\n",
        "    return rpn_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrjJA4_WRg_k"
      },
      "source": [
        "def calc_fast_rcnn_loss():\r\n",
        "    \"\"\" calculate the fast-RCNN classification and regression loss \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #Fast R-CNN classification loss\r\n",
        "    gt_roi_loc = torch.from_numpy(gt_roi_locs)\r\n",
        "    gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\r\n",
        "    #print(gt_roi_loc.shape, gt_roi_label.shape)\r\n",
        "\r\n",
        "    roi_cls_loss = F.cross_entropy(roi_cls_score, gt_roi_label, ignore_index=-1)\r\n",
        "    #print(roi_cls_loss)\r\n",
        "\r\n",
        "    #Fast R-CNN Regression loss\r\n",
        "    n_sample = roi_cls_loc.shape[0]\r\n",
        "    roi_loc = roi_cls_loc.view(n_sample, -1, 4)\r\n",
        "    #print(roi_loc.shape)\r\n",
        "    #Out:\r\n",
        "    #torch.Size([128, 21, 4])\r\n",
        "    roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\r\n",
        "    #print(roi_loc.shape)\r\n",
        "    #Out:\r\n",
        "    #torch.Size([128, 4])\r\n",
        "    n_sample = roi_cls_loc.shape[0]\r\n",
        "    roi_loc = roi_cls_loc.view(n_sample, -1, 4)\r\n",
        "    #print(roi_loc.shape)\r\n",
        "    roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\r\n",
        "    #print(roi_loc.shape)\r\n",
        "    x_roi = torch.abs(gt_roi_loc - roi_loc)\r\n",
        "    roi_loc_loss = (((x_roi < 1).float() * 0.5 * x_roi ** 2) + ((x_roi >= 1).float() * (x_roi - 0.5)))\r\n",
        "    #print(roi_loc_loss.sum())\r\n",
        "\r\n",
        "    gt_rpn_score = torch.from_numpy(anchor_labels)\r\n",
        "    N_reg_roi = (gt_rpn_score > 0).float().sum()\r\n",
        "    roi_loc_loss = roi_loc_loss.sum() / N_reg_roi\r\n",
        "    roi_loss = roi_cls_loss + roi_lambda * roi_loc_loss\r\n",
        "    #print(roi_loss)\r\n",
        "\r\n",
        "    return roi_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5f6HB3t0QKr"
      },
      "source": [
        "input = Image.open(\"/content/drive/MyDrive/Computer Vision/0000f77c-6257be58.jpg\") #800x512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYLxVjs-0ib5",
        "outputId": "eaf564d8-58e3-47da-b405-2e2e1be239ce"
      },
      "source": [
        "input.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1280, 720)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3psriT70rno"
      },
      "source": [
        "height = 512 ##########################################################################################################################\r\n",
        "width = 800 ##########################################################################################################################\r\n",
        "img_size = (height, width)\r\n",
        "n_classes = 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_GpNPXK0sAB"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((height, width)),\r\n",
        "                                     transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m8Rp2Cr0uRn"
      },
      "source": [
        "input_tensor = transform(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "DB8oXD_-0w_M",
        "outputId": "7326cdd1-f5e9-4d06-fa99-0326ba4fec5a"
      },
      "source": [
        "plt.imshow(input_tensor.permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcfd56d8ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD5CAYAAADcDXXiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9WY9l2XXn99vDme4cU2bkPNSQlZmsiWSxyCKpkkhK6m6ipW6jG1D3m21AD4Y/gL+EX/xg2A3YBmwD7TaghgW4KaklSBRFqaWaWMWqrCErh8jKiMiYhzufc/bgh33ujYjMymJRJlsFIhYqKvLGPeM+e6/hv/5rHeG951iO5ViO5Vh+tUT+Q1/AsRzLsRzLsfzi5Vi5H8uxHMux/ArKsXI/lmM5lmP5FZRj5X4sx3Isx/IrKMfK/ViO5ViO5VdQjpX7sRzLsRzLr6D8UpS7EOIfCSE+EkLcEkL8d7+McxzLsRzLsRzL40X8onnuQggF3AR+E1gGXgf+lff+/V/oiY7lWI7lWI7lsfLL8Ny/Btzy3t/x3hfA/wX87i/hPMdyLMdyLMfyGPllKPczwP1Dn5ervx3LsRzLsRzLfybR/1AnFkL8PvD7AFLKrzSaLYQQFEWOtRalJEmc4LzDWY91jimENPktBHjPZwFLQoTfWRpxYraFEg4h/PQLj8BaR1EaitLT7Y5wzrIw3yLLErz30x/nPcZY8nFBnheUR67LgxeAQAqJlAIhHFIJlI5IY027USOK5PQWJtcGAgG46jzgEQdfVvcn8B6sg7I0jPOCsjQ4a/EePJ7qv/DjPd756d8fHhMhRPhB4Dk0tojw/ZE9wlGllGitiaKIJNZorVFK4ZzDe4eSEqUUUsrJM67u1Ve/HY8e2eMReCSliHA+QiLwQpL7MC7CGSSOWHoMCuMjPAIpHEqWWGcYj3JG/QGmzBHOIaa3/fDsmIxQOPZDlzP9KPBIAWmW0m61iOMIMd1fhN+fsu/kGUopjzxDDsGfj16XAA/WhTH0HkbjnOFojLUe58MYOucOfrw79DzDc5ycbvokq7mklWSm0ybLkrCN93gczh29To/EA8YpciJKH+EfPuj0nJ96a0fu8HHy0J0/ItNnIB7eWDyyvf/0C/jc4qfPs1oJxjDo7eDKIcJbrCmx1iOEREtJlqW02y02NjcpSo9UGh1JkiTCI4nrMyCjQ/f5aXf46L0ebH/06j5t78PbffLhT7e89wufduxfhnJfAc4d+ny2+tsR8d7/G+DfAKRZzT999Tq1Wo39/T0GvR71ep2nnnoKawy7+z1mZ+fodrvs7e2RZRllWSKEYG9vjyiKyMscAOfcVLlAmPhKCq4/Mcfvff8bnO5IolQRSQkIvIrpjwruPdhieXPEn/7Ju4xHPf7r//JVXnr+UrUQgqK0TuKsYDgsWF5eY+witrd3+eijD1leXmF3v2CcK/AJSml05hFZTFpv0Ik9/+K3X+K3f/15wFcK0aPVwePL82DYpJRIKcO/tQtz1yusAStiSqcoCsdoULC5tcvy6hpra2vs7e2Rl57cJRRFiTElznmMc5SlwRhTKQYblLFSQVlrjVQS78OCF4CU1dgpBc4ifUlZlhhrEd6yMFvn3LmzPPPMM5w7e5ZEB6M5URiddo16PSNJEqy1eGsxZXHwbITDeoNxHiE1SM2IJgPTBKcYRB3uyjmMm8W5COcHXFQr9I2m55vEQlKTFp302NeSvOjg9ses3fmQD3/8A7Y+ucE4H2F8hBYOjUViKwMTlGVQipMFFJSjQyBxKByRcEjp6cw0eeHZa3zv1W9z8sR8mDeUKAUIixd+OtcminI8HoMQZGmKFBJfPe9g6ByIoypuPMoRQpKmCXiBMZKVB9u89tb7LK9uMSgco3HOYDig3+9TmvA8J4YEQKlgXMMRAefB5AhX0mjE/M7vfo+Xv/o8ibJgC6yxjMc5jUYDISVGZ6RuTL/Q3ClOsOSvsENGqQWRUUgHTrjpvR5ayxzO3R12wuTUD5sYkMrhqERW1yqERMjwDKQ4tI+fqN5HxbmJgft05f5Z+cTJ9TjnQMjwY0tG21v88I/+ZwZr75KZAVvLyxQuJo1qzLUaPHf9Kb73G9/mD//jf+S9pTWMj0gaMfVM0Jm9yPXv/GuSmcsILzA/Axg5Yvw/dQMfHNHPuLfff/nUvcft/stQ7q8DTwkhLhGU+u8B//oz9/CeXq9Hv9/He4+SEab0rK6ss7+/T61eZ2Zmlna7TZ7nzM/Ps7W1RbvdxhiD1ho3cBRFgVLqkYkmpGJ/YPh3f/An1PWYuZMLXLpwjksXL5DUUyw1cqtQSgZDMRbYsiSRB8Pjpacsc1CKKIvwCx2K0vDKCy/xm688S6/f48O7q7zx1k1u3LjP7v6AQeGJiXECIuMpSwU+eLpCKIRwgAMOooOJdyyEwHtPLBVCCKSMcJGnNAZtStJY0kkSFufO8MxTi4zHOZubm3yyss7Sao+NjU3KUuIR9McCKRVRFGGtpfAl1hhMpXDL0iKFQIigHJRWVEsY78B7AUbifIRxAq1i9rol3Q+WuHlrhdmZDtevXuL6tat0Oh2stfQHQ7rdPdI0pVarkSVJOGLljgnvEM5hXYKVMcJD3RdIsYEWYxLXxPldBmKWnjpB39dR9TYiL3HjEmPGWB3TIMZYhVV1ZLPNlWstXnjiAitLP+VHP/4h95ZWcOUQbwfhnEIEX61SHPiJsQmRkZdgEQivsJW3vLU74Ec/fo2bH97i2698nZe++hWyLEKminAQG5xvf6Dk0zTFeU+vP0AJQRxFhyKag6k/ec4ISxRrEA4BRNJx7sw8afpV3rlxkw/uriCUx/kC71OM9RRFMLjWWpyzeC+mRgtkUKMieJTb+2P+73//x/R7I37jWy+RRRFaK+I4rL1Go4FSBoQnViUn9Q6aFe4U82y5+SraMwgxUcefR8Tkv8rwcSTCAJDTCCpEHwgP8vGK+SAKnHz/6ds9rDg/TZEeNoyucky6vV1G+R5KeZQX1Go1ZBmTROnUeCdJwpVnnuAnt24yKhR7w5LFE7Po3pDRaEAya5Ei4TF6+Wde19Hv/377wS9BuXvvjRDivwX+BFDA/+q9v/GZO1XX6WzwbgSCsizZ2toMsIdw7O/vEscJxhj29/dxzjE7O8v29jb1Rh1jDYuLi4xGI3a2d0AcWHbvPY6IXi7Z3sm59WCJN95dotF4k0a7w9yJkySxBC9xApwQDMeOshTBi/AevEN5h7MOYQWxFHQH+0QsUm+lnJzJOHP+BF996QX+/C/e5n/73/8ApTOazRZexgwG+6ytb7K9vUeWpcRxTBRpoCRALhaEDN7XRLkjUCpMfynCxI+1RAkfFK53GGfQEdSTiJnmKS5fOkPfxizfX+bu3bs8WN9idaNgNPJ4r3BeIa2itBZnHc6DtCEcNaZSEIDDTr14AIlEqQipIxAe40siocmN5/7qFpsbG7z51nucPXuW69ev8+SlBZIsotfrs7m9SyNNadZrtJqNgKYFFUbsHcKMENJQak3sLam3xC5GizVKtcnYrzEs2qSqRu6bqHiWUgn6NicebhNLTyRyUGOcGFOfmeVy81WyMy9y//Y73Hn3dVY/egs77ofFIjzexRUg5A5PwSle5isv3nsL1uOM5f7qFv/+D/+I115/k+9891VeeOFLZFmAa0LEyBQmVEoivKDRqLG/u89wMKDZbBJFUfW8J0o4GLsoitFKVUrOg3BIb5np1Hjl5S/TPnGC1994E1MMkUQYJxlrySgXjMcFfjrXD68pEe5BKrxIGeYlf/QnfwWl5buvvkycQBTHeO8ZDUc0Io0RGi897WhMXawxNIq+7VBIQBgg5lFgKcA8hKFlsmSCiamgtep6tKzW1GHFTwWvVmMXdMDhCOBRFV4hstWNuoe++3RFfljhHEAmB0bCWMedu7fZ2V6nbrtEEnQcE8uIRlYDU4C3ZLWUN954h+7eCKFqIcy1EUonaCEZ9feI4xYiSh++6kfu4bMgrOC1//0hp18K5u69/wHwg8+/h0Ag8QRIxbkihLreg/RY73iwfh8lY7xTDIcjtFasrKyQZRlSSLTSJHFCGqdYY5mdnWF/f5+trS28dSE81SmFaoAEKwT7I8/+qMsnq1tYD3HSRIqEMY5bK/tcWNlkvtMiEo5Ie2JtMd4hlMIrx9A4huXE47LE0tGsCU6fbZDUNURNdE1TGk3pIlbXN/no45tkWUaj0aDVatKoZWhdPQYRgQp4sweE1jio4AIQSiB8ghQBPgGIUVMPQWhBJEBLS/3JBS6dbzMcFTxYMyzdu8/qygp7e/t0S8kwDxCTNSB0gYvAe4l3EmsNpijJ8wIhQKkIpTWeABd5D05qrA3PDhGRFyX51pCtrZu8/8ESp041ufL0ZS5dvkS91mJnZ59EJAwZIaQjziJ0pDDeTBe4AhQKJyRCjKlRIohR0jBmi6Jo49QiudUMxQxOtPDJkLLMEcMRUpf4JEJEJaWHonOSMy9+l4tXX2T/1mu8+cZb3L/1U6wd4s0c1oyJ6CNwOGcAGzzT4KbihaxwX4VQEUjB2Dnurm7x7/6fH/Dex7f5tW99g8unF8Mc8DZ43uIw7CNo1OsYY+h2uyilaLZaQZH7Co4QCqE0k3wNhEhReIiwKOX40tOnqCfP8cbrb7OxscXYWhyO0iuUz0BohLOH4Gd/oGlFOK7wCUUp+eGP3kJKz7d//avUkoQk1RSjEeNhThTHIVIEpByx2OjSHW+wZTsUaKQIMKH3E+zEhbVazUfpJmozeOhaqkPe+oGyOgxhCcALX81jPzWQYZfHKTfxmcrxYQV/JJrH4xA474PTBGA8RWHZ3l7D5mPizFEYA7EiEp582KOV1ohjjxWOvb5BRynWWTAWhEAnGbH1dLdWUfNBHz2KmovpVTyctxFHvp+YoKOG68h2P8N5/wdLqB4WpSbQg8Q5h9Ih3J0kjwCoQmS8JM9zxmPHcDjEex9CYOdIkiTgyEpjrWVmZoadnR08nsKB0gk+MgBoHwZPCIN0MiThhMR5hzGG//Ta27x/4yecXVzg5FyHy+dP8+Slk2RphlIaEWus2GRYjGg0FUoGPxAv0SqmXmsgkiZRHGNsWLBZlnH+/HmKoqDX67G1uYVWEfVajTiOqdUyWjN1sloaEl2TMHUqAqkkQkikPPA9Dm8iBBhn0UIQZxn1pM6JZpMvXbxAt9dlfX2D25vb3Fm6z97egCI3WCKcA1sp+3FeMB6WlAVTmMu6EmPsNLyOI31kbjk7weolw3HJ7TsPuH1nlXbrp5xcXOT0XAv53DNc7pxHCEt/OMSaIUmaEEcxUokDiGQy8X1QDEoKtDJos0XmRyS+x4Y6z0jNUpeOIo4Y+wau7MGoB0ISSU0iaogoJotnOPvSt5m5+Cxrq/d457Ufce/DW+AKhAveuyTkFZyI8KjJcAeoapJ8FgIhg0c+HOS89dY73L1zhy9fv843vvE1Fk/No7RCYMGDlBprDSDQWtPpdBiPx+zt7tJoNMiybOqCThVd9VtW2HnIkXhiKTh/5jQzzRY3bnzAR3eXkCpHaU8UQZmXFOMxeZ5XhqXC4LWqRjQQB5xQdAdD/uyHP8YIy2/82rdopDFxKjEmJ89zkiQBITDe0ol6XIhiSl9n20Z4bEiAV8rdUXnnKjw7eYgMMDHaRxKknyITDH76+3MlSB/O8B796uFjPPLZlfS6PTqdDkJAYUrwlplWnU/yMaQCUziwEpsbZAkuTivSRyAzIARSQpbVaTWbSCUYDPbxSUYcRXxaAvjoBT96nw+P0eP2/xyozBdDuadJQqvV4tSpU6yvr7O9s46UgieffBIpJevrqwyGw2oRHGUOAAwGAwDW19cDZqwUM7Od6YK01lEQkegaVjnwHuUcWjik0kgn8bacushSSizBs+/d2eTm7Qe8/tYHLHQyzpw+xYWLF7h44TxjI8gNFDYYBSUkeEWkU7KsicoaOK0AVzGAFHNzc2itKYqCfq/PoDei3++xtbWG946smdJut5npdGg2m2RpgpQHT9K5A2/D+6Px6uSz8h4tNUpJkAJBiZcQyYS59jkuXTnP3nNPs7G+zcrKOsvr2+x1++S5oywdWoFzNaSsQuUK4pqMt3MOU9jKKEsmPAulJHLi1bkYZy3rmwPWN2/zkXa8/9HHPPvcVZ586iLnTy/STDPKvGTQHaC1otFMieKo8mgBL8ALnCvxFFidkLgBZ8SIWr6JT2eQpWRNzLCn2+TRSbybYez3UeN9UtVFqphUeeraM9NuknW+wfmL11hbeos3//rPWb19D5MPidAIrwJT59CSmrCkDrOLFCog7dbS7Y74m797h49uLfHsc1d5+etf5eRcA60VHlk9HxMShVJSr9dJkoThcMh4PKbVbqMq3Pcwy0gcgm6UkMReICJN3Gnwta88S71V5933P0buF0Qaiiiiay3OuUMY/MOOgQYiHCn9fMCf/cXfkuee7/36t2g3MpR2GGsYjUZkWYZDkJgep4RjqBsUQjNyMjjpUgESN4FdJmOmjq5teUR/HXyYjOdkLU/n7y9AHk7wPrqBw5uCYthDdZpYYyjLHCUkxaBHgqO338fkJTVdx1tBVEFmZVlWzmUwpBMMvtvr0ZgHxxgdh3nshHhIfT+cYA1rZ3KtDzOsPss0fB75Qih35w/YI71er/o31Go1er0ep8+c4eOPbzHxBYICl1O2wGRArLVTtslwOJwOlpASrxKuvfASOzvbbG5uUm5vI2xBCMU1eHNo7oUEpFAR0huktxhv2Nwr2e7d592b92g267RqdfZ2c649fYm5Tp1OKwEpUSomTep4FVFwoBiKIsc6RywlWZYRxzGzHYt1C+zvdxkM+vQGYz6+eZtWq029XqfdqNNsNqjX62RZRhTJKWPh6Lo5CGWlCzCgcsEbsroAIIorj8pakpmEucZpnjh7km6Zs7W/z+rqJsv3H7C6sUduLSrSIXdhBUUhMMZgbfBKbQmmNIgANCOkQ3tRUUBBCoUT4Zl4wJaeT9Z2WN36a/769Te5fPY0L155ikuXL3NiYYGyzNne3kNrTa1WD6wRNNOEJQaNwEiJlI5avo6y2wif4dQ+ShUMOEOPGpmKIJqhzKEcFaAdRkEkFcYoomSea19+iTOXLrN8e4V333qd+7feIx/solweCIGV3RQ+oMlikiwNgEUFOWi8E+QWVta2Wd/+Me9/eJNXX3mR559/jnqtXkEYIRI5vIjr9TrWWna2t6nX69Tr9YPF7Q+82MlnhcBL0FISqYjnr19ldmaBN97+gI2tHoORwdkQxRZFwXg8wlpTJe8n4LYMSlkmeGEZFyV/+VevU4xz/vFvf4eZpiZJkuCZGoPWCdI7Mj9gUazQVxHrYh4nJgGHwPkqMnDBCZjC4NWcfIyDOr2uRxOkHJnbU/UmHvpi+tEf+fvkdJ9lJqQA4Uq8LSjHA6I4Is8H1OM6+aBHqiTGSMrCY7wnkjGBGitwzlWee3kQ1VpLr9ulLMd0e1tkcUSRj9G17HMlPqWsGE5CfIrnftggTMbq4VH6dPlCKPeyLBmOhnR7XUwVxgYvRtJotABHLWtw4sQiO9t7WGup1WpsbGwcokMdTBDnHKsrq1PFoyov48WXvsnpi5fY7+6xt7zCnQ/eZ/X+ElvrDxj2d/CVEvFC4oVCaoVwIHxgTxx4GZZuN6e3PWbjwV/z4796nZmZFk8/fZZTZ85S2hpRnFIiUF6gvAHpGRSGwhoiX4Jzgc8gwHtLEsdk2QnaszDOc2bbbdIkotvrs7eyjxSCrFYjyxLq9Rr1RvAAIzUJuyuc14fjShF42l54jAhsDq2jACcXHoUnUoI0ViSkdDoxF0/PMbp2kQebPW7eXmZ5ZYWd7W1GY8NYKHKpKK1AKkWkguLO8xxnPT7yGFzFc1doUcFHUWBXCBTWB89yt2d55/3bLN1eYmF+gatXr/Lkk5c4tdjBeMdub0iSG+ppRCY1SkEkQdocI1NKEVPKOHiN1tBgj5ofUrDNQLbA19j1s4ikjYwaGDNmUO7SHwu0lCgFWsWIxgILVxb59fNPsnf/fT784AZ3P7xBubuJszkCj6+WyGR8cRNHxFCUDiVDYhgsSntu3b7P6sp9Xn/rXV75xstcvfI0aaIqPD5gqFIplAg0VCnllCU2MfhhvKbhSzW3bXimSiK9BiW5eG6RZqPOmz/5KbeXNvAuwtmIsVIIbxkVwYtHTNaGRAhLgPUjnBWMioIf/e07DHLHP//+q3RaGXEisSbHjEeoKAIcLdnltN2g9Am7soMTCuUtAn0Q3VAhxId1zhHm2tEvD7OLDouvbvtRp/+oIp8aBHHwy1fneUTtHaFuSvAR+WjMhzd+ypnTp9nY2eX0/ClmWk32lEA6iZUSfEiUGw9GVhGV9TghGOc5URyhY4VxBmMKVj65Q6uAhcVnH74CEBNdNYl2qgqPad3Np+HrD8NPByDWz5IvhHI3xlCakrX1tZCgQOKcoNvts7+/z5kzZ5ibO8HMzCxSKra2tmm321Pq42AwoCiKqScfrKkLeKoQaC0QzlMaiY871BaaZCfOcuq5r0I5Zri7zdb6Kjfff5u//cs/pUSDDHxjWSl65T3SmDBBhcNaDyKidDAYwWbe5d72DYS/QZa2USpFxRLnQHmD84ZB6TBaU3iP8jYwDCpss3SWNEmQXqIjTb2esrgwy4lT8wyGI0ajEeNxzm63y+r6Jlprsixjfm6Oei0oBa01Wkm0EAH7rSa0FrLKv3iEF1XhUYmSIaJQVuG9wkeedqpp1eucmp9hPH6anZ0d7t5b4fa9Tda3dxkWlsJ4fEWN9AKMsTgZvENrA3zjhSeqipyEEDhXYepVstIJTb/M6a1ucG99m9abb/HMU+d56sknmV+YZ242peYs42EBGJJUY0WGElFgLRFYP0GpaLT3aLFNQ2wzMhFCn8LLOfbUCZSM0WmdvEgo/YiEIZE1aBHjkhmypM65To2Fc9e4+rXvsvL23/H+e6+zubWGNh7rA0/fO4spDDY3WF+glAfhUFKhI0EUCwSW/ZFi751b3Ly1zJeuPs13X/0mF86fQukDT9ZXEV0cx8zNzQFMoZp6oxGS7BOvFo/DgvShqEiEOg0pHCfnavza15+lln3MjQ+XMCXEAoTIkJEgz3PKspwafkTJpNhOSA1CUTrHG+/cRkn4nX/yHWbbCVGkKYc5JYRclocFv4kFrIR92cR5CZVzJRQT/sNUnz8KjTxeIR3w4AnG6CEVdqSA8Wge8ojOF+LhPNX0DEf+HemEdrPO1oM77G54BIrdjVXyUZ+iKKkJSVSvMxyMGZY9lIoxqoXWUWCO6YS8CNFtaQukiujt77I3WOV8Nk+S6IqwfzjkcAeGSAg0E6DmcEHaY4eISUziPw/Hki+Icgem4c3hz/fv38d7T1mWAGitp5O1KEJBTLPZpNPp0Ov1mJ2dZX19nbIsGQwG0+NN8FFbVXMiIRcCFym0Tsmyk1w+OU97tsNrr/0d+f6Ai5eu88LzX2N1+Q6ba/fo723hurvVoqySXVJWiy2E6dYLvPO4PKdejxHOchiE3Nza5+2f3uHyhbN0GhlJJJCiwHiHwSC0QPgYoWOckMgoIZYWHUsazRRnHQsllKVnb2+PpaUl+r0eaRKTJAn1ep1GvUY909QbDVR1XUrp6ZgiQEqB1gccXwFT+CDAAH2SuCRNFDOdBZ584ixf63uW1zb5+O49PllZ58FWn9J4hFYY48BUY+Id1liMHU+Lpqbc9gp3VipoAus8QkLpYHOnz85/+glvvHWDudk5rl69wleff5LTpxaQSjPoj5GupF6rkyRpNa5HY36BRTtHVuQo+4BabUzDDxmaDFFmqCylROKsY5AXUEZoaRHCopKUtJEx25jjwsnTPPfNb/PmW2/w+g9/yPq9W7iyQHqDKR3GgHMlUocoTghPFAm8MAhviWxMqjUb7LC8tMrNGx/x3e/8Gi997UXanUZI4Cp/BJaYsGjKsqTf65GkKWmSHLBKxAEGGxCWA755Z6bDN195mYWFRd5//yY72/sIBTLW02dsjMW7Q3iuEEgdkvaTCuO33r6BMwX/xe/+Np1WQpbpKbYcxzFKeWbkHgbw4jz7YqFiyRwk9/8+qPkRJov4LONASFL+nNi8ACQHukABNu8hygEUfdY+2cIJycLcPDtby4xGBToJhWdCx4zHA1Khq7UjgwPjIYoyjLEUhSPNJBsbG4xMIHZIKR8maB5R7P855Auj3A9P9IlMlPokYbq0tDTZmuFwiDGG5NACGA6HNBoN0jSl2+1Oqzy3traAA+UlECRegZBIDNJLpAAdN5G6jiVh4fRlvvP9fwl2zHiwy/72Og+WPubO7Y94sLbM7u4GIh9U+N2EoxuDEshIYoWv0m4H97a90+f//Lc/YH62zeJchwtnF7l44QTz87MoHREbhZcSEaUMckPhQPpJVWWAQxKpSRI9vcfZ2RnSOGYwGLC3t8fGxgZxrJmZmaHeaFCvZWRJTJImAZf3gDBHJpiv/j4ZH1sWKCyqYg8J7+jUIlpPnOTJS4t0h2Pev32PW7c+YWtzj53tPkQaKVQwos5SWEFpyqlRDQnlQ5PaCyCGydh5gXNQjjzD1V3WNl/nnZ++y5UrT3DlymXOnFlkrp4wHJaMR6aqrhUIeVCVbITCijgsfmeouW0Ss87Axpiyjdcn2JVnsSpDpm1ykzAu+4FCKRw6jpDUkVoz02zwje8tcu3ql3n7jb/mz//4B2yvL2PGOb50WFcgpCXSMjx/LQg8cIvwhjGeeqqZnZtHS8E7b7/N3v4mL774LCdPzdFsZsRJciQ57q1FSUmj0SAvCvqDAWmahm28D3TUQ2tECEEURYEpFsEzT1+g3cj46Ts3+GRtG5dHU+M6HueUhT0CY04YWcGgS5yNeOe9jzGm5J//zm9zYrZJmqaMRiMAVFSnJhwn/IDCjCi9ZSTUVGl9WquLh9f4w/JprJbPUt3iMcf5rPOoquIYKievyOluLrO2chM73EU5y3g4pEwLsANMKRhYi/OGSCuiLEWKMPZKKYw1FHlJljaQyjMcdrFOYEVOozNPo97AWoeK5ZF7ETKwAEXFopHwKYbqF5NUhi+Icg/epao864MEi6is3UTplI4Zqx0AACAASURBVGU5Tbaurq5OJ/bkYU48DGvt1HomScLO7i6HMy4CEF6GHxQIjxQKLVKUSIlkhrFQKkmka9STlM78ac49/RxfsQXDUZfl5SXWlu6wev8+q/eX6e3uYcsRUhic9DhBwNIOPSvnFd2BZzzeZ3Vlm3feuUmzFjM312F+YYYrzzzNwqkFinGBIMKhwOuHJsCBERRCkMQxMzMzdDodjDGMxiN6gwHdwZC1rW1MWTLbbDI/P0+jEdg3OqpCxGqc8XJKbTPGIHygEgoIoaW3ODfC+oJYKOZamldeuMBXr55nb3fEzZtLfPKgz+bWDr1+DyEkpa+RF3YKl4XknsVUZfOBNFedQ0jkobBZCkFhBJs7hr03bvL2e7c5dWqBF546wzNPXqbdbmPtCIQlSTRxHBPHMTKRCK8q1ktOyJ8YIt9HDwfM6B3aqk8vTdFphyhZQEctsEPy0V6AeFQU2EneoXXEwunz/Ob3T3LlylX+p//hv2f54w9ITE6sPFmqmZ1tce3yaRYX52i2agjpkSJG4aknmnY9wwDD0QDnC27ducGND8ZcunSJ5557jjRNMcZMk6mT30kSCvZGwyFKKZIkPrpgOGCcQPBM00hy8fwijVpK7f27fHhvDWNMgOxUxFAcUCXxYI09mAOA8EEdvPf+x5RFzr/8Z7/F6cUF0jRlPB4jVI7UghjDCb9JaRPuqRNH5uaRSOowq+sz5XEK7jFKXDz2m+neh/W78D547h7A0d3bJhYlvZ01lj5+j1gqlBLsbNxkc30NazyFFBRlSZKAUoKqJVSIhLynXm9wanaRW7c/Io5SyqLECMiLnL39fSKtQ+L5oVv57MpZz6N35h/JVXxe+UIo9yzLuHb1Krdu30YAtjRhMomQSQ5UNIn3Fmd9oI55jzUl/W43KH95iDqmNGlVKFSv10NLgTxnPOwG/FuGkD7gexJPhK+y4alyRDJH+IBvFlIivcIiMESgU3SryYWri1y++jLeWrr7++xsbbG28jEr9z5iY/kWw/1NvBAIKUAqvNChaEKqQC2sqs/6hWGwts8n67u8d3OJZiuh1cg4Oddh0B1y6dwZmo0GURSFCEA6LAYnPDJSlK4EYUOyVIaCmEajDcBgOOT2rVvkhWVjc5uV1XWSJKFWj6llGbV6nSSOp5RGa21gPyiNl6IynJLQaE0GT0M4hAjRRKwhbkfMvPAEX//GPDu7e9y6fZvV1VUebA/oDQ3WSIpSYKpCrlFuGRcWayzS5yBESHgriURVSbkJ7VBiDYwGcOf2Jmv3lnjjrXe5dPEi5y9c4Oypk8RxSjG2UBbERqMThadECYswAnwMzuLFkIyc1BriEva7KfXoPLm4hJCCWLUZ5Y68yLHC0ExijKgxEjm1GNrtBs1axMmFFnVhaWSCa0+d4xsvP8+VJ08xM9MiijVShiZokyZU4LBOhJxJPsZ7y2g4ZmNzh7fffpcTi6c5ffocsVcINXEGPDiBRhNnMcPhkIEJ9EQdRWEtuANacMgrycAK8575+QavvHyd2fkWr7/5Dv1hgdYCNNheaHznfSivPyxCxgQzYblxe4noj/6Mf/Uv/hnNekaWZuz3d6m16mitaNHF2DW2TIN9UUf4nEImKP/pza4eK9ONH8XSQ4WmeARC/1nHf5hw6KXEoJHOsr6yyniwTyYG2NEOke0y6o3wzoQGdLu7mBysVBSlRUsV8m5KoEUVhQLWDLm/cpfBsEeSRGgVgXQYHK25edAp6jATyIOQ/kjk8eh9HGr+dihZLMREwR8M1eeRL4Ryl1JSq9VoNZvMzc1RjMbTnjFFUbCzu4eOIsbjcQXR+IpI4LE2QAzBCwdnLc5ajLUMh0N2d3fJ8xyVNTDluGLmag6o4yHpYV2BxxNpUDL0XHGVCxBwQFHByhMTrAKKJyGdyzg9d5LzTz+JNK/SW/+Ef/u//I8MB3vhPNWCB4/S0aEH7HE+JBiFdBgh2OuV9Hpj1h9scfPmXdpZxuLJEzzxRJVoXGjTmWkiREScNPFInAiVlV4QeJB4tNa0mimtZsb8/Ena7VBA0+/32d7eYnt7b+ohzs7OMTMzG6hdeBAKoQSOCb1SIIQ+uG4fGAdCeIQISbpa6ulcXOTShZP0BwM2Nrt8dPM2y8vL7O7sMjSOsZVEsUeNLcY4pHOY0mKdx1lwU1J0tRiq6E1ULIWRjbi33GXp/tvMvHuP06dmuHblCa499QQn5tqUhWc0HqEjQao1Go1yPhgsLQN/nxERlmw8InMRHT2gkDWUO4WIZ5FSk9sRvhjhRU456vE3b73OjTdeQ9oh507N0E4jnnniDN/++nPMtGKSxCMwIV8mFEpaPB4vQ45HCEhiRaQzAGZaHc6cPc/Gdpf7y2u8/8ESz19/jjNnTiKkx3tbAc+BTxEnKV54hqMRkbEkWXZEgQUPXlVuokdHgrrUPHvlEp1WkzffeZ+VtW2EipEiYSDHFHmJdzkcomgKQlUuaFAJH368xH/44z/n+//oezSzmHq9xXg0RtfrSCwN1WXWbdH1Mc4rhOXnFlFp9Op2OZyCnK62n+GpP3rMAwkxapUX8xZTjBn1u+i0xIwHiDLH2xzvS7CBclq6MuSJncdbFyJBpZAE4z8cjZidbXPv/Y/DObwKrTkizcLpc1x84mmkjsGG7LI4dF0i7PCp0FLgGhxuTXEwNhND/vPIF0K5F0XB3t7etABjfWOdxZOLxBXk0Gi22K26P2ZZhneBVzrZXkpJHMeUZRmaiFW4nTGm6h4ZfEJng+ILcji0ZQpLyIpDHyh+DtTEnww9Z4CpynOTzpKE0mnlJDqK6MycQcVtXK+HqMq1AXSccPr8JUbDIb1+H1MW1aRWCOmqh6iwWJy3mLFlNByxsfsJNz5eJYljmq2Uk4vznDt7ljhOOHO6Q2fWk2iJFQ6vQmkJhFBfKqaVpRM+9cxMhzwfMxwOWVtb48aN95mdnSVJEuI4Jktj2q0WOFVNrqrgBF9h5UDVJlhKiVaKJBIIgoFs1jSdc6d45txZut0uvV6P1Z0dlh5s8PGd+2xsdymtxltFPjZYA6UxWDuetrIN2KuqsEmFFAJJBiJFAN0eDMbr3F95wDvvvs+TF89z/eoVzpw5TekcbuyQZUmsIoTQSGERUuAo0A4axpKwT8QuI12jsGOsHKCVxscNfDpDPhjw3puv8Vd/+v+iipx6BJ1azMsvXuOZpy7QqCuksEgZIMWDKlYxzfeADy2go2g635WSCKmp12vMzAhef+1H/Pgv3+C7v/nrvPzyV2g0EoQo8LigmGSoH2i225RFwXg4DAyWQ4yaScJ1UvAmq+Kn82cWSbMar735LkvLO2hStMsDRFPsYV2AyQIE6oI5kRJEBEh+8s4HaBXzT//xbxFFMUmiGI/HJElKrErm/TJbtkbXzRE7j/15+or9DJl6uP8/YWjpKzPpLXOdBjurdxnkI3o7e4yHY4y1SOmnLT0CTBYaybmKhGCMYTAYsrOzw/b2NkoJGs0MgcA6SxQJ0mab5559mZMnzlUFTEEr/zxK+RdVyAVfGOVecuvWrSrxM6Ysy4r6NwagVqvTHwwQQrCwsEAcRaw9WA3l28DW1hYnT57k3r17RFFEUZTYCUWrmiBKqZCgrXBNF/hWh9qlqtCPvMI8jQnelyRsLwFVVZxNxl+Jif9UsRmqB+m1QsbxtLJzIlmzw+/9V/8NWZZx//59lu7eYXX5E7a21tjvbjMuxiiv0MLjMEhhA7zgQVjBeCTojYasrd/lnZ/eRWvNwkKDixcXOHPmFKdPLTLbadFKo1AtKiRCxJgqsXnAzAhFVJNqSaVWOHHiJP1+nwcPHlDkBfUso90OhVRpmhBHqupWGeCxiXJPkoQkSdCiDIhr5Z4oGaKp2ZmEmU7MmXNzPP/c02zv9bnx4R1uLy3zYGOX7r7DGokxmlEpp5i8MYbSVq0ilCBSEV4q5LSxGTgiTGG4u7LD6toOr7/1HufOnuL556/zxFMXmWk36PUG6CiUiYfkToQSceB7C4MhxpOTlfeI/QNmdZvczbEfLWJkE5Nb0nIY+gYlkpdefJoXvnSZWDmyFLRiWhUdRVHg+asQOVk7SWAGqHAKoUQSY6te6ki6/Zw79zZY/j/+gDd+8h6/9Vuv8tTTJ4njqCpw8WgHokq4qiShLEtKY0jiuILjOFCqVdSjgEQJTsy2+LVvfpXsJx9w88MlXCJJowZ7gzGj8SjkQESADQ5wX4nzEoPj9Tffo1Fv8xuvvkKaxkBOXgzJ0pg5t885NvnY1bAi5lHf+/PJ45KkE8/975tnnLhjynswBdvrK5hRj821ewz29xn2+oytQ6oSCE5dURQI4dE6MGacdXgZOsYuLp6i2WgQxxodhfUeQDhLvdbk4vmrSJEG2LVKhE+LyH7GTUw89F+UfCGUOxxUrPV6PRSC1dVVhBDs7u3RaDQxziIIvN18PCaKImq1GvV6nX6/j1KKEydO0G63WVlZpdvvAyBVVURzEBMBhyYNE/ofKK0r2qAI5dvOE+mg9IWj8irFgUFwlsMPzEqPEwVSW5KaCi5DVSXqASs0Pu3QOHmSqycvcO2r3wRT0u3tsbp6j7t3b7N6Z4mt9VX6+7vk+RAd0qpVXx2QLvD3w014er0t7i09QKl3qDdSzp9e5NrlC1y8eJGZmRlMEVNIy+MXXVAyrWYztDvIMsaDMft7XTbWtxkM7gf2TafJzOwsrVZ4iYnz4ZhhvELCSnDw8gevDroUCjzKOmpeEM83ib/0JNevPcFuf8BHH93l9sf32d4ZoIsapbHhRSimZFQEfrbxHu8seFs1tJIVDCWBCIQi95ZiUDC4eZc79z7hxMlZvnTtCk+cP8PZs6dQWlHkBc4JdBRgjlKNSL0CLF4WQIlGIos+qlwnVqc41/bcVI5ECa49fYkrT5yjFjl0BJH2KC2Jo3j6EhOlFAiH1iHRGyi+TBuCBXwcvAhjVZSWza1dSqsoBiVvvPkeN299zLdfvc63vvVNTp8+XRWqVfNWhtqLibMyHA4Do6bq9XM4JScIL+pQkSJKFa984zqtRsIH791lMCzxuhVyQoSX0ICbUmeDPpKh0M6V/PhvXifLUl75+otoLbG2oCgVNSSLfoM1Pce2beF9NMlmTa/k87BkHrfdlAfxGKX3eTxdAUjvwVs211d5sHKPex++R3/9HsqXiCTG2II4rt4/UDURA09pSmKnaXfaXL/6JJefuESt2aZWyyjyUWChAVGkaLc7pEmDSGfYCtQ8ct1H7uHv76GLCi34WfKFUe4IUYVAoQmrrxRufzRkMB4xoWz1Br3grQhBs90KibhIh+2loN5sktYz+uNhddig1GQMKgYlKy5t1UcdMXkbUSgekiqiKAXD4YAUQypCAYlXHj1dOdUeR8a3SpYQCoIkAmGDj+mEwekIlbSQcZ0iSkEINAIXe9KsyeUTZ3nyxW8SFSN63T3W1x6wdPcuy3c+YPXeEt3dfVxRoojx3oAIFYs6jZHSgHDkpWPpkw3u371Plr5Jq92m0ahx9tIiRtVotVrUahmxtHhvsdZhvcNTYu0YUCRRjG4qms06zsKDBxsMRn36ec6tNz9E6YgLF09Rrwfj2iQmy1K8LPG4wCMWAikC1XTyNqhJwy1RGlJpmG02OD3T4MqpBTa/9AT37j/gxt11llc32evCqNAIE2GsDw2ckBgzCpW9XqKUnr7owUuF8QJUUJp54di9u8GtexvMdep86eoVvnTtaZ64dB4daUbDIVp60tBwh1honBMoEQwIgCz3qEc5z52wvBN72q05njp/lkiFXFCWZmitECLUX0yqTZVSOB/mk1RxRT+dYOhMqbjOG0pv2RuO2R2MGROMpbAw3B3xhz/4W/7uzVt846Uv8+1Xvs65U+FlO0J5rDcoIVBaksiYoswpS0OWZlVraFEVPIWXgkjpiD20E8VLzz7FxZPz/OTtG3y4MsaLDHDk47LqjMkBtBNSh3glGTvLn/7oR0jtePmlF0l0jTIvsYmmJocsuhVGLmM45d9XfXUeg6l8lnr6BTqv4T58+Il0TD2psbayQre3Rb/IaaQRjSSuIpgcpRXNVky3N0DJMBbOG4rhiJmZNl/+2lf4D3/yp7zx5htkaQ2lFMPBkEimxEnMVnedBtfCfTx0748YqJ8TbxLIitFk8Ri0+uztvzjKnaOW+8CzPviDxx+ETcCt4ZA4jg/R7SxFWVCv19ne2Tmyr7WGoiyYREh+mqQJ1lXI0Ed92tLW2WqK+ikMMEVhqoTfwTHCSUT1f4eYVJpXhqTC3lzoWePlpONfheNXx7V4SGPS9ATnFxY4e/065L9J3u+zubrGzQ8+5MHSHdZX79Mb7IGwOF8G9owtqhawBk/K/tCzPtxHqS63Vld5/e0PqdfD25POL7aYm+3Q6XQARWkhNwVKxaFvjQxei5KKrKZIai06s/N09z/izp0lZmabDAah9UCWZSwuLjI30yRNA0SjhIbqFXmB0lqNl/cYU1ZdHj1C5MgYTi7UmJ97guvXnmG3O+Sj25/wwc27rOz26Q/GFLnDe0lpg7dalpbSCpQtQ4K8gtq0ElgEKFV1ZLTs9HL+7q13ef+Dj3jqiYtc+9IzXL54lkhK8nGBsZ40SwIkIQKnX1RtlDWWmVZGq5HSatSIlMC7EiHSaYJrCnkLMYVnQhWwrvItFWPCHTTIwospr39jc4fBuAh91wndNT0K4zXLK33+8MFf8tbrH/JPv/8qL730As1WBtjQfRFQUiMihbOewWBImiSBUTNZP+IgOacViFhy+tQCcfIi5ds3uLu0jMSTJ4bxuJjSjSf5vAnk6JCM8oI//rO/oJZlvPDsVbSWFKYMCVzRp26GjIjCQjkaQjwiB19/SmLx8N8e+vphT/3zFARNoFQP6CiiKAtKW3LwqhxHq9UivIkszKc4z3HOEcXBiZEEI56kSfXWsgC9jUajaY+Z7Z0d1jfXOO/9pIHN0Xs5cr0HuZKfR4TwgRrt3TQX+Dj5Qij3SZb4s7c5FOL50MvCFgVFEZpibW5u4r2f4vSHG4pBeNepNcEzEVI+AlJIKaF6vR0iPLzpYqycoSPTSjzG8FZ0LiElpTE4N65aKoB9pFPfI7tSVIo/dGOU6FpGmnW4MH+WS9e/gshH9Pe3WX3wCfc+ucPSrZtsLH/CsLvH2OQIJ9CirJKdIbB2hWJsLTv9PneX36WmLWka02m3WVhYoJk5krRJu92G/4+69/6xJMvu/D733jDPpfeZlZXlq7qrveX0+BnODIcccqgFSUnUL/pF+y9ofxQESNCvggQstAJWXEIQwNWCK3E59EPOTA+bbaddeZuVlVnp3fMRcY1+uBEvX2aZbtEAzdvoymfixYuId+Lcc8/5nu8XiXSCUDrAIKQB6zljSqUS5XKZmekZhobLbG1tsbW1RavVYn9nnzAMqVVrDI8MU62WKJVy9SGRGyW2h4KS0uHyPH0YC5S2RC6jOlpiYvg8z5yfZ6NpuLu4zO1b99jc3GFfQ5IJQi0wWqC7LUzujLy8XIgLFEp6aKdUDiRop6l3DB9eusEn168zNz3JxQtnuHjhLENDAWmrhQwcYSSJZeD5eXJYYakUMTY+gk41adbFU+eaXkd1f02niN79d6uerQp3IE/nf1vPpe+s4sHKBo1GBy3Cns05JzBGIa2mHEq67TX+7e/9Ab/46BK//J1vcv78aQZKCmdNzvfvO46VUmRp6ifdWplC0bAgLhPSeWlAHBOTo3ztK68SKrh75z7dbkZDKdptT2lb0Gt7VyuQwsORM5Pxp3/xU0ZHJjh+fAaXddFZm2qpw5BpsGOHMK5YoUgcnw2h6W9g/MeO2oudWumRR3ML8wwMD7Gz7qG+SZJQihRheMAdVfTHFKmvUqmEKNhicyK8IDhQfMuyDCUNWZoyMTHho/2+6PQfqyPVL8gsxiR0G/sMVI6KgRweXwjnDp/PufeiIA4c6NFtfDFEPLQGEn3biL6//UPmN2igFMZYsjRDKenTRRzZZR90Cw5HFEEY5LJqnlOl2MZae8Dz8dDSpNjoQANTAFbm32V9NKAqNQYqVc7OzHDmpZdAa1r7dZbv3Wbp9k2WF2+yt36fer1ON0l8LSFxvoHIpBjticsEXbhfR3CfWKT8/M1PmJwcZ+HEPKePz3BsepRqpQQuxhiPR19YWKBWHaBSrRCGQY/ZcGFhAZt5fv1ms8nK8jJpllCplD3aqVajXArxrAMF7YFEuCjvjPXRslQe+mito1aLGBgucW5hhs5rz3Pr1iIf37zJ8uomrUaGzqAbVkjToMcpZIBMezpnqXLZQBfgUDhhcDrFWcftpW0W763xt299yKmTx3n55Rc4Nj+DcaBtQhwGPQGVIJBMTIxw5/ZKLnEX5E1YB8X4IoorJjKZBw+uMJocRSOlzCN40KkjSTQrK2t0uppOnhLJLdEX8Z2DTHskV93x9rufcvXGHV5//RV+7TtfZnZ6yqt5OXB40ZOiic9TGIQ+7y8lCAtOg1SoPHgZHoj5pVefpxKH3L2zhBMFr5NnPnSuoKco+OwDnJPUWxl/8hc/4bf+sx8yMVwjS9tEpsOAbBEIP3EV90r/XXa0u/ZRj/8BqehD41AHNnhnqxTD42PElbLvcDcatKGt8EyupVJPx9hactUsP8rlck7CJuh2On4S1ppyuUyn0yFJExwwMDDQQ+D9o1ZHKTIAlkuffMje+ionj809cfsvjHN/UkT7JMd/GBfKoQvqegDRvKj5pOKL84x7QRTmS2vnl2nGgsxTOUeXWIeOo/g+/44xRfFV4YyHmllj0FnGYbM72JNwgsAdnkGsMHmix39H6vJneVNUGMeUxsqcGxvn6ZdeI0s6NBstNjY3uXTpEjeuXmb//m22N9ZJkwSTZQQuQ+bnJ4UkNJr1lTo3bq7zzntXqYVw/tQMJ08uMDBUY3J6lJFxQ6VapVKtoZRBKjDWkqQp1jlKpYByeYCRkQpCCJbvr9Nqddg2e2xu7BCGUK3GSKUYGRkhjEqIwvx6WNH8+uWMmaXQEQSGShQz+uIFLj57ko2tPe7eXubu3WWWVvfYbXRyeUBIMkNiLNqYXEzBp2oUlgCHMwLrPHe/coJmc58Hq59y+eo9zpw5wUsvvcCZExMILGnWJVCeIndyapxrV2+T6RTnfANcoTf7CIt89EsH3g5nHdpoWokl0RFxZbCYu3vc7g5QBIQFjYXwHDw7ex3+9C9+wpWP3+fb3/gqX3rjS0xOjKOQkEfJQRBQDStok/XEN/xuC1ebp6BwDA1WePmlZ4nCgE+vPcDzJjmyNKLbTjwpXI7VdkJi8R3Tt++u8Gd/8RN+54c/oBRXSbIWZdkilgbj4l7jTpHZKb798xQCnzQeRVPyqOH6HnktWS8zUi2XOX36LDd+8RYuSwkweX+HT6XFcYksS9HaT9ieciBA64xmc592u9nrhA+rPueepglSKhZOnGBicspH2DhMERQWOq3O9ib9R3o0d/Spy9PR/pkUAp1l3L19m2sfvs/asdknXoMvjHP/PKM/NdMPMSwio0PbGosRYIRfXUZak7RbOc3uwQqgGBIDUmMCi3MGZWzuVA2Fmow4okRwtBPOSp23vgcoFaOdxVqFzSICp5FGY7MsN3HxyF/4qPEL66PcIj9o8xy+EAIpIqwDJxSGkMw5iEuU4hGOjx9j7vzzfCdLMfVtVm7f5P03f8r7b/2M/a0NtO6CTglEQKYz38EoNMiA6mgNEQiu3rpNM9GUKyVOXL7J9PQM88fnGR0dpiJDUisxIiAxoKzL1aEsQliCQDI3N8vw0DCdTodGu8HO3i7bW1uUK5vMzcwwmPPU+w7ZomsXlLMoawmVQwqPYkFBRQqOzwxybOoCr718ks19y6Wrd7l69QZbmzsE0hJq343YzSyZNuis5Z29zesgRa+CDEGFWJOxudNi873LfHJlkYX5cV575VnOnZ5nsBoShDA0PIxQgjR1WBtjXYR1+YpOWJwIkUpinM3ppXP7kB7aKYTACddrEbDOktGEsbP84L/5H5j/8E1+/vZP2FxZRllDIB0a7SclJ1FO4ZRDc5A/X1pr8gf/71/y9kdX+PYvf52vvfYaFRV6pkqnva5AXMpJ9NpEUUwclcEehuZJCeVKzNMXz5K5kJs3FxFOkwYR1jpMt+tXgOT5fSTOhVgk7354ieNzU3zljReRVlJjgwm3wpI9jZBdlCujc/nMnj0/JvEiDmaAg3vhMU78Mx18fm59e0JJCIXCGUspqjBQGiZrJz5tiiQMy3S6Kd1uF+cgDAO6SYoQkk43IYwFp07NMD5eIwqVry+VQ5I0RQWO6sAAZ566iIpqSKGRmSVpdlBxRFgpe9vO03ni0KwnDh/2oaP2ExJ5h6rLSe/q+/tINLevffr4a8A/I+d+lOjqoeJr/yiqJ3khs5i906JY1Ntn3/YU2/tHntHQHOynV/4pjPBwjrA/UkBI4ijCi00ffNbleX9rDCJQPHTkj7T7o8Wlvo633mTXt0rpu0ZSSmQUURoa4NxTZyDZ4cHiJ5Rtg4Vjc5w8Ps7QQJmNzS2GR4YJwwBrDQOVMlIK9hsdtveb7NXrLK3v8OmNu6RJwtjYKMfnZxgcGmJoeJCpWYGKHdIayBEnmozUdkhdTKkWUBoYp1QpE4YhzWaTdrvN3u6Oh2EODlKtVqlUSh6KZkCJwEeRfQ45yNMezklCpSjHipnR5/nqy0+xsvyAT6/c5u7KNjv7LRIDibZ0E0XSSTCpL27JvITW64PAd3cqpWi2E27evc/y6jKzU2OcP73ASy+9wPDIFOXqMN3UUwkcLb4opZBBgM1ZSJ+00nTgqYN1QBpXCGem+KWpX+OZ11/nw3f/jg/efovt9RWU7fioUhuUwxeK8z4LgcS6Kp2O4/qNNVYe/BHXP73Fr373m5xYOEYQRkhsjhCRgCLtJmC6RDnSDIfvGRAepjlYrfLyC89Sictcu3qTeqNDojOSmkX8DgAAIABJREFULD18/EVdAMi05c/+4sccm5/ixPwYVmdUwoRYpqQcsCI+ynE/ycyL7znqxA/l5p+0mnf9zYq+3hMJgek2qe+ssnzvCpneR4jUcwFJSRRHdNrai/QI3+dirc17OwSV0RGiKKToOBVC0Gw10Vp7FFptkO2dPRacxBiH04adnU1qw4PE1Qgh6HEsRVH02GvR708EOYUy3t1IHFEUcuHcaZYuf0DSajzhgv4zcu6fZxwyBHLsfL7kKtSeHjlE0YFJXowTOGcPFcw+T6WncP9SeM1Maw3Wet6bYv8uLzb5EtdjF2gPnY8/THGQy33E+496zRjNR++/zXs/+XO2Fq8QpHW+960Xef2Vp5ifGwSTcO3WPY4tnCSKYjY3N7m7+IC1jT2295u0U42NKpTKQ3RcSHN3l7truyyt76GkJC6VeOeDG5ycHebcuZNMT49TqURoIUmNoZMmaB9zHJKZm52ZQacJzWaTnZ0dlpeXKZcrfY6+4lkTi6Kws3lR1kc7wlqESQgFDA4GTD+zwMVzJ9nYS7h87RaLy6ssr26yudeiIyFRFqMtOkuwOUslFOLf3kZEEGCUo6U1dx9ss7K6xYef3uL82Yt0UkE7tWjr7cpZD+9USvpmIyGgwPgfcUxFJ3WPAM9qMlclk1WSKMDaQarjVb78vRmee+2rXProPd772Z/T2FrB2AIGzKE734mS7243GXv1jJ+99SFXr97kK1/7Et/85leZmhgkyoXUpQipViOSbodOt3ugNZwXh/0qUFItBTx38RxxoPj0k6ukxtLJc9AFisYjfXyKMwhj6u0WP33zbeb+818HJ4mzfcpRA20G/ar5oWTm5xv9q/T+YO7zpGWUcEh3eLWQNZo0tpf4+L0fs7F6BRE0iEoGIWLGRyfotHZ65xWGIe2csC0MvcbxyPAI5XIZqVSvBlcEUEmS4GSDzY1lsqROqCLq+7t0uk2ma1P4NRekadIjciuO7LHnnx+3yHkZiog/llCJAlr1XUTaeeJ1+Gfr3J/0IwtgbHwMlGR9ZytfBrneTXL0UwIfVUspfVNCnvbxMnLCU9UKgXvM3NC/o97PJQQubzwqpLK8Dqk5OPbDWMrPe+YHkfrnGDrL+PDtt3lw9yZDMuFbX3uZV185x+T4AJGEpBXQ6Vpu3L7P9vYeD1bW2G92SKwkswItAoyFNO1gXEhYG4MoJeukpMaQdqG5tMXSvVX+7v3rDAyUmZ4ZZ3a8xomFORaO16iJEOkMaY5kKqB2URwzFseMjY2xtHSfpKvRGdxbXCYIAkZGPYJneHgoF0nweHEpBcL5fgikxGKxzhKGAbOjZcZef5ZXXnyazZ1drt9Z4tKlG6xv1mm2U9Kk4OD28NnUOJzx1NGhjMhEIZTuc6bpdpvtnY8YqA0S1+ZI9QG8MVABKvD5d8RBc1v/KBxGEfEKIeg4TYcq2sZII7E6pKsEIoiIx2Ne+sYE559/iUvvvsnHf/dz9lbXCF3Tc+3kOHlXQGhzyt2usdzf2Oc//uiv+OjqdX7rN77HC0+fzgv7fpVYME12u13CICAMIlSPj1P4yTNwnD9zApOmND+5wcDAQA8Z1Em6OOPTTb54rLAi4tLVm1y5fpuL509S1k0iGkg3TFaEnY9LxXzOguOTVu0PD4d0DtWHThIGbJaRNLbp1FeIwyYDQ44gUHTbjhMnTrKzFdJsNtjZ2aFSqdBut3vIGZGjZJTygu0mj+rTLCXOu4XjOEW5fbbWrjMykdDY20bKMkJaECYP7A6oNQ5s5XAG4tCZOS+uA54SWwmQZNR3tzBph8g9GYn0hXHuj6ukP+7545ZtxZJxcmqSiakpNt78WY4LpTfj+m2LB/lFlRIpVA/CZp0Fl3ddCpHnvw7Fzflx987gINUncv4QIX0np/QSa846krTgk+nbyxMd9UHRtVewPQjdj5xMXrQq1nFYSlHIibljNO99zMunT/P68xeYnKgQRoo0kSyv7XLl9gZrW7t4MIAi1QHG+WqDFZ6qQTuJcSFWBMgwAt0lzTt2nc6wxHS7mnqny+rGPa6oDtXyx0xNT3Ps2DGOz44zMzVKHIYoGYAIcU77oq4SBHFIqTLM0ECNiYnxXF90j2tX7zAw6HVkx0d9E1Ypzidg4Zk7VY5IUc4iZUYEyBJU50aYnqzx8jNnuL+yyaeXb3D7zib79TbaxHS6KZ3MkGYGbSzGae8YFAjlse4Oi84stBNarZQ0s3m9xBdbhcihkdYW+bFDTv6AllfkyCBH5hxdUQUbILt1VDhBKgSG0PMVqYjqdIU3fm2WZ175Gh/8/E2uvfeX7G+vEwhfgXF4dI2SuU2IAGRAYiy3Flf51//77/HGK8/wve9+13e5SuEx/FIQxyE60zkXk2eZ9Mfq60xRCM88fYZGpvnFx1fBVcE6jIbEZX4lXBiwCuhkGX/5N29yfH6OUqyIbAevT+nF4R9ZQ30oL/75hjf/I/7gyCsF7Fbkl6bbbiJ1k83V2+xvLhNJw/BgBYUBDSvLSwQqA6ynFQgkYajodjOc08QlRcHVhIM0SxDSEQSCOA7IsoROu8nGyi1G714mliH371xheuFFMAZh/MooVAGhEgTiABF39BqII489kZx/InF0GnXu37tDIKBajp94rb4wzv3oOAqV+qzZu3DyUkqCIKDV6TAbhn7paUFbyNIDGKI8chUF3ldGUeQdPBAFAmE1VhczgDpwpId+E78ykM4nWpwQRFGMkgEmp+QFr+CUpRmH6uVHcuWHh3v4cZ7WefKEUCAnLCpwHD+xwNaNCs9dOMZANURZSXM/49L1+3z46S1WdlokRvkoxXm9SFdEXDmqxkeeDmskDsnMiTlEZYy7d5dJ6zs0bYpSlsBmBFaTuoCkZVm98oCPr6xSLknGR6rMz07x1NnTOGIGBmPiUkAYSRJrCGLPgR1GkqoMiePjqDBmcLhKvb7L7aX7KBUwPDSUI24UYS4n6KzA5J17Tubt5kApcIRVydDpKc4dn2D5wR43bi9x594q91c2KGeCbqbIrCLRYLIuVltSKRFSoaTXSc2MJulmmMxhrFfdMgWrg9YYa3tIl4d+RSfwTV0egoiVoIYYGxqhke3TdA0CSiROYGXebY3AiCrlmdN89V8s8Pxrr/Dez/+GKx+9S9LYJTZdCikI4XzywwmvGWBSx3aa8Fdvvsulm/f4zne+wxuvv8xoxXs7IRxRJUZn0Em7RHHsoZyAFNbj4WPBMxcXSHWbTz++gauUMUaQmJavV7g85SIlQgSsrG1z+cYiLz77HGGW4qIMSZQb7GNs1S+ZHxHXHxWF7qtoCRAipN8NBkf2IfIJTxpP15E0d4llHWn3cbpLGIQI5QgHI1zSIus2EbEAZxioVRBYKpUKzVadJG2iTYQMyoRhgLOSbreNswlhqDx4QHlq8rSbsHbnFmPVaaTdYbiqEWlKGEmMFaTtFgO1QSLhgwNxtAfgKLXvkeumBOw39lhfXUZIQ/BItNbB+EI4914e+hEO69HUmPmcdyR6L4isLl68yN7uLkJ4pZrBwUG2m41e3vDgkhRF0WJh6lMFB4XW/u0+wwEfiajDICBQAZkRvVyllBL+HtHKQ994NBd/6GjEkecwPT/FqXMnGRobJtGanZUmH316lVv3NmilAuPkoe2LvR76HtG/T8ELX/oqz7/xbZqthNV7t7h+7TL379xmY3kJnbTRnRSnDd1UoI1mt9lmfafOnfsbfPDxNSbHhpmaGuHkyXlOnjqOwFCKMnC5khIWQwKBYWi0ytBYiaw7Rb3eYHFxka3tbaQUjI2NMTIy3Msju0Nn4SAvrkVRiAosp06Ocez4GK91nmFpZZ0b1+5z994DtvfbdFPQWdmnLjKLNmDQvjQoodlskiRpT1lKG9Pj6+//bfpJ2owxGH0gIam1xiQWJTKSxh4DA1XSVJNmLaIgRjvff9qLIYSAIGD0+Hm++9vHeenL3+Ktn/2YpU/+lqy1h7QH3aqFJTsHUoTgLA8ebPN///s/4vInl/kXP/gmJ07MowIvVBMGvkaQpr7/IYoPcN3gqJZDnn/2PCbRXL+2iHbQSlO6SeIl6IREycDL1mUd3n3nQy6cuQCVwiEfNAH+Yw0BecG077k47NyllDlxncVpw97eLvWtWyRJl6SbIsOAru4yPjbK0MAkK8trGJsSRRGdTodSqcTw8GheLzPozPoUnApwwrPYqsBrpOrM4KwiCGJsCt1Wk6XFy6iyRXcbuKxFc6+BjMqEQhOILNctUA9lrJ4Y5+HTmVvbW9T3vYZFeqTYfXR8IZw7HETnRV6yP1o/+hceX03XWjMyPMzY6CilUonnn3+eZqvFzo0b6H6lpyNLOed812u57MmDdOJpaJ17kmPvPyZPPFCMOC4Rl2Kc1Bgrybp+GV3g3/++Fv+k9NUjt8cxMDzM2OQ0Bsf65j6fXL7J0so6iYtIrIfwAY8vOOMjE6l8nUAFJcZnTuLiIeJYcmZ0jHMvvkLSbrKzusLa/UXuXr3C3ds32d7eJmu1yToKJyELBQ1t6Gx0WN3tcvnOOuon71GrRjx1eo7zp08yPzPJ5Pioz/kjcSpEG4tQHlkzOjrK5OQkrVabVqvDxvotlFJMT08wWPO/X1GbsNpfbyEhkCB0hpCCIAqp1ua4eOY0u3stPvj4Kldv3GF/t0k3lbRTS0c7kiQD6wvySZLQ6XTIMs9aabQkCv3S2BpDmDNBPqkeYoyB1DIwqMlCQ6fVolIuE1hJNzOIoIxSUR7TiZyiRaCJkFHAxMJ5fvBfLFD/yhu887O/4srHv8CkHUKXodxBPUCIEGdzVasufPLxDbbWVvnBr3+Pl195nhhHLD0aRClFt9Oh2+kSx1Ev1STIqMTw/DNn0d2U7O4DWlkZ6zKyTOeFYpUjbyKW72+ytPSAoQuzvs4lPIz4yeFMf1RUPD98X7m+3LIAAg4zvnoNgwNOnFwP3v8uzqFNxrXrl4nEHp1uQoAkywz7e3UClWKdRusMIUKCIPBork7dNy5pS9ukJEnmmxu1Ly4LYGJ8im5Xs5e1sMYRoGjs7iJUwujkJKbbIG3vQSDI0jZrm1s89dQzBEIiCDEyPHzmjwhw+59LKdje3qbVaiGFb6Z60vhCOHelVK+zruBpODr6UuS9OPpQvtodcLhfuXqVX3r9dYaGhqhUKrz55psY42FIvX0f8dkFN0iYk0Dp1PW6SY+ErY8ch8oiQhDnotUqBGNDOs0WSarJsrQv33h4ifC4m+Dh26P/CvSlaA6quYc+HZcHGRyeYnvnLjcu3WR5dZvMRhgV4bWCNEW1+HEF54O/gnK5wvDINI4YIxQgQTmCwYjZ2iBzJ0/z8utfI+l2efBgldu3b3Pv1i3Wlu/R2N3GJC0y452D1j6dtb/V4f7mJf72oxuMDlaZn5ni2OQ0x+dnGZ2YQ6mQONcoLRgYJ8anEELRaja5f/8+62tb3Os0GRgYYGpq0lMRByrXWfVnKq2kgMdaLJFKOTY1yMS3v8SXv/QK9+4t8uEnl7hzf51m5ugmFUwCNrU5FXUbY3LnbhTGGqQRh7RijwYnxQUsUDM+ZmsgZRsRDtDubBOVhiAISWxGoEKfovHZHBzk0bkP7YKoxPiJ5/jl8ROcvniJj979W9aufwC61dNklU5Cb0UmccKytlnnD/7DH3H15g2++91vcGpuCplP6OVKhaTb7kWuOIdEozAM1CKeOn+SrXqLjhNkuotuJBhtUELhnGfjS5IO167d4NVzL9Mj5ntoHXlgVeLQ6+KhqLVn4f1vuIKBFF9sFGCxeQpDHtpHmqbIvADfbNZJGg/ASer7TVAGa5oombK/XyeKvNJZIYDtDDQbbRDa12By/2CtIU29RkQQhJw+dYornVu0GvtUgtBzWCXbNPcDsk6dWFla3TYb23ts7W2TnJhjbbvN1NQ8YWUoP5NeueYJ6We/Jj916hSjY6PQDWjsJDxpfCGce6lc4uVXXiQIArI0o9noUK/XabUadLotdJaCEz0+B5PrqJIjEKQA44xvGBKwurnKJ5c/5YUXXuDatWusrD3AYTE6RdgM5TQIhReh8EYkhSCQkiBQKClQEnTilz1eFu/h1KGUh40OwJITkYUBIvACHtKWkKoDIsPobp5rU8UiOv9on4MXBxPQ0SzuIUff97nCKxcgS/JiJ/mNVx2e4YNfvM/Sg20yQkToG6DA9LoJ+51RX8rdY6EtWCcxMiAcGSMYGsUIQPgCo9+FwiKQQeCFvsuCuZFZ5i6+TKANje1NNtZXWLxzk7X7i6zcusbO9jqRFBgJKdLngXcy1neWufTpCuXyp0z99D3m52d4/unjHJuZQKPwrieXeBWCoaEhxsdGyXRCvV7n9p17WGsYm5hgdGSYUjlCyly0I59cJQ4hrVdnCgJGBiWDFxc4fWqGtY1drt64w63by2xuNGhLL0TS7hiyDLLM68EmiYUwRGud3/y2R3vcu545jNPLRGoCJYnI6CYbCJkyGlepJwkDcYSxXRILyJrXHXAH9Fb+d/HlQy0D5MA4Z176MsfOP8ut9/6aT9/+a7bXHyBM5ptfsJ7CWOB1CVSZVivjrbc+YnFxmd/8lW/x8ssvUiqFGK0JggiJJEvSPJ8eIKwjlJLpqRHOnJyl3unQ7VRotlOcS8HZ3EYEhCE3Fu9xsdOFEhRpmZ4bd4ftV+A5dwpq3GISk+7A8TtxuFwqhMjV1HztoDD9Yioo0CXCSSIVsre3xvbmCi7TtBoJyoA2BqMzTFtTq0IQCtqtDkplHqqqNZn1im6h8p2rURD26COEcwzWaoRSkqZdnDCUqj69lug2US2k3aizs7lNpXSPZtLBWMvUSJnmzjqLd1aoVaqM1iqeQdQJcA/TgHmJS7DOYK3GCUdcLjMzd4y9tXvs7zx55f6FcO7dbocrVy75tulqlZGBCWampiiXj5GkLbY21whDyfjYOA5Hfb9Fo9Gh3WrTaDTIdALOkRrhqVmc5tbdWzRaDba3t9FWI0KJcxqcQTmDxUdwB//RV2jNRfG0b8/3FMSFosvjh8sdtXNeIFtI3yhiczSCkhat83ylDPKOSf/J4vOPit8f/Z1Htitm+dzhF1G2cwKLwsXDrO4bElnBkuP5nUNJTxDlejdKHi8VC4I8iStwCCswQjE2fxwbRwhpepNP8X02lx90qv9VASKgNjlLbXKS0889hzIp7e0Nlu7c5P7iHe7cvsb9tUVa9brnPDQOYyxJV7B/b4ub99d5572PGB2qMDU6zAvPPc2Zc8eZmBjucWerQBGEJSqlCIlfGSTOcG/5Ps5ZatUqExODVCoVbK7ghSpmbZ0nyyzlWHByfowTc+NsP/c0d5dWuXzNp7LSLKHbSb08oNGkmSPMnXrBOdNfAwLPMOocvuPTevbPwGVEoo62GWiLC0dod/cZrA6xlyUgSigRkOG8DKHwBdl+o7DOgQwJB0d59Zu/xvnzF3nv53/Dx++9Rdbdx4kMJTz6RyKweK0ChGRtbY8//MM/ZXl5na9/48tMTI4hbIaUAQ6o1/cJo5AgyDV2MVw4e4KNnX3aHc1eIyNLAO0DICuAQLHdaLJXbyJGio6EwzZ7INRUdHm4IpFOQb6t+ty5n6IKyoecalv0RfkFxPnQLeGdfBiViENJo75NfWePTsvr4QZRANrR7XoxayEsUnpqDI8nB0OKsSkD1SFiFXhNB+cFeuI4IraQJgk3b12n2cmoVYfY291HBQabxThpeXD/vufbH6jRzVKkC9FJxkClQrmkQGQIJIKg72br+4lFIRTk2NzdodNusnx3kfmFeVq7qwj16AJ+Mb4Qzh38krXb7WKtpRqWCYSmmRmCAOZmBzl16hjT0zPU63Xq+y2sCdBG0213SLKMje19bt/dpN5uYUSC07C+vn4oj9zjlznkVA9H3wc247DOIyD0kcLr0dHLc7ret/T9Jf+s3y5Nk15DQvHvP9Xw0EmJISKsDCMqw2ThLsr4XF3BP16sFA4xYfbto78BCxzH5uc978bnPZAivZBDFoWSEFaoTh3n2ak5Xvilr5IlTXb217hx+QpXPr3EvVt3aO3VewLgOE3azah3Giw92Objq7cZH4+5+NQpTh47ztjQCAODw1RKAdI6XKYZHKohK2WM0ayurrGzu0m7Vc9Fwn1jiioX6kHF0Ajp6z5OwtR0mfHJM5x/ap5bi0tcu3yDdqtNmlbQ2kvqFcphRUrmQPHKF1q180yPWjsSDUmaQSlBBT7tIbRDKoerVOgmjulohK2sjooHscYXWB81xfe+SwgyFzMwfoJv/8Z/xYXnfok3/+Y/sXz7IzKjCZXEd0urfJnqg5B6M+WnP3+PlbUNvvnNr/H0hTM5bbIlKpdJOh7rHccxQgiqlTLPXDjHxladvXqHbquD1oeOiHZXs7NTZ2xB9lSz+g74IIWYn5HsiyKK54eje88NBI+//x43jLFevlH7iTxJU0IpiCNyxbYG1hpGRgZJOppA+KY05wQutRjru5qLJi8hvYiPh5GmxFUvS1mTEY1GgzRNqERh3tjUxe2uk5kOw3aU1BigRFypURuMaTT32W61mZ5b+MyzEgKGhoZoNevs7m3SajfYr+8A+omf+0I4dym87Fuapp6VUVoUGVjNQLXGmTMznDl93LMqduokgfbiG0riBipYJJOzU4yMzWKkIrMJrXqb3Z3dnB2xS2IzOu2Ob//34TUH0z95pOpyZRr/WGtNweMiijCkL2XfS8VYv2wWzmLx0lxKHkQBRSOEdbpHUWzz5Z3rV/z4nLb7/4tC1PlrWS1FTE1OsPNg0QtpFBOWyJtw8JSxBS9+/3cpT+cIwm8zMT7+6OR8/zHmtYdiH04IrHQIJXESNAoZxuAMzmlkEDBWq/DGzAle/8ov09jd58H9Za5c+ZTbd26wsbFC0txB2BQhAtpZl8Z6l5XNjygFlxis1Jg/NsvJhVkunFpA2oSgPEAp8gXggdoIgwOjlOKQ3d1d1lY3qe+3GBz0dZlKxTNdQogUFs9UacGmSCcYqCouXjhJp9HAtBxxXGJoaJBqLSaQyuf3w/DQNbA53YTRmU9pacteo8uDtW0WTs5TLhmqJYmzTURmKbka5XAUnWwypMoYYhJCUqEOotPH2IIJBE4ppIuYvvAsP5yf49rHb/POT/+a5uY6gTO+CUZIwiD2WHknyIzhzr1Vtv/wR7zxpVf40pdeJS5JnFIEUYTL6whKKQIJM5NjzM9Msr3dpBEq2kn/PSSxRDRbCRNO+QazImUDvdVl/3koQc+B+vN56LZEWNN7D+DxZf/Dw1pDt9ulVqsipMg1laEcxxjnazdpaiiVY5qNJkEgKFeqXiYxUcTVGpVKBWEdpVJMGIYY6yeMJEnpZHtkBoSKMcYriEWxZ4WV1lErB3Q7e2TdgFK1QqlSxboMFTh29zbJZIWxyVniqIB2Phy5+54bnzau1aqoQLC1tUan2+SzFMm/EM5dCO8ASyUvfqywKAzVWomnz51m4cQspSik2+0QytgbhOj6ZWrOM40T3F28yW6jQ1QKGawNMzU1xcmTJz2Bk7DIqIJzzjecyH7/fhDBF3muopPPWuct8GjGxNHjaaf4dC/9LYji6BHO7wiOlf5VxOGXH/c0z9I//lr2/dtpt9nb3aYmO4RkTA1VuE32UPFAHHzk4LX8RZU3dZkc+TCQo1X6l80UzVW9/RWx10Hh1wlfIBR9NixyUimLRKDQIkJKCMolRkuDjE7N8/RLr5BmbVZXl1m7d4+7N69z6+pltjdW6SQNEuvoppJ6x3B/e5H3ri4yUn2f0WrEqfkZnj57hrNnzpCmAeVyiTgOmZwsEyiP7e52u9T3N4jjmIHBQcqlirfDQsfWZQjnsE4gnWR6YpqsqpmcnGBkpEKlGmF17vzivKkkDx6stRgpIY5wIiAuKVYe7PNnf/kWx47N8cbrFzk2OUYUBwSyw0DXkYUCGVWJw4imS5D9SYqjQIP8NxICrLQYiiAD5NA4F7/0fRZOv8Av3vxrLr//d7QbG4DBdbzISclKhLBEsWS/0WL9//kR12/e4Pu/+i2mp0c9uCAMSfLak1KCSAWcXJjnzuIqu6WYLMm8NjFF81yEdR7qV+S/+wv+xc/v77O8sarH85Nv02+LzvVYaopp4VHI+EemM4VgoFZDa0O9XidNEmyaMjRYAuOo1Wq02yKHOZLzzPh2xcGhQYTz+fdQ+Ly7F2Dx56S1xkqfFhNCUIpjkh5njNfsTZIGTsDm+n0mZmYYGZsls45Gc5/deouJY+eOdDU/yhn42t7+/j7lconx8RHe39tGSke5HD20bf/4Qjh35yzSJARhgEAQx5JSOWRycpJyucba6hbtTpP9vX06nQ7WGsrliHI5QioQgW8PtrqN7rTIOop2O2G/UafTaeOAIKwQxSXS7r/j+MIJxqePMzI+ycjwCEPDQ6hIgU18Q4TpoOhC1kVkGRAhpUC5DGM1ToQ4wl7usDBeLT1ntnQOG0BiUsrCR/Aqt9iuTjyNb4EyKNqkxaNylA9HKZ8Zs4u8CGsSbl3+gAdLdxgsK7rNFlm7gQpDdGZ6diRkERM6hHB54S/HMCsf0Qvn2Xi7EianpxmqDnpdUw5SEOZQIH84b+oPy/rjcn5j30XoC3e2QDrgG3F6i80AhAhQcZlj50ZZOH2RV7/6XZqNOsv3l7h59SNuXb3Eyr0ldJIinCNDs9XssttscW/9Gm9/dIPx0RHmpid4+qlznDp5jNGRIVAlBgYHKJUyOt2EnZ0dOt0dQrlPtVqhUqsQliJPFoVGW4uQEEbgtEQoATLwTUPkCj7G+hVgvvIp0DECh1AhKqyh1QA7+wkPHlxi6d4q3/j6G1w4d4LBiiVQGmX3UDLFklJTKXuy7IuhTiOOTPN9FY2ccVLgnG9tskJigoDa5Dzf+o3f5sxTT/F//G//C1urSwjdRmRdIu2QwhFGXte1GmpsVmewAm98+VXmpueISxWi0LNbBtJLEU6LnS/9AAAgAElEQVSM1BgfqrGxtsuuE2iLRyQ5LzZTGpuBXBRE5Tz1PdSiKEAM9sDZq8IwCud/yLv3tiuMNhS9d3zt4ZHhTobRCZsba2yub2CtIMsMQjl/36FwNqEaK0rKUYoDVOCvB1YTRhU6rS4u05QHBimVSoTK4/mF8aufTGjSpE0UQ1lZVFkS1yKUcqTW4kgZHBxge3sLqQepKMvGXgcRODpJwJkLYwQqzs/a+AmRg/tIOEsgBNYZXJayubcBWrO7sU6EphI/2Rt8pnMXQvxb4AfAhnPumfy1UeAPgBPAIvA7zrld4ae1/xn4VaAN/NfOuV981ndYY2k0mjkjG7Tre0gc167f9D9TluKcFxCQUiJFQCAV1VqJkdEByrUyYW3Ay8JFMZkGpCQMA9JEeiHhtEmr2eLdrbf48P13EWGZMCpRrlSYnp7m5PE5ZifHqK+tMBpJzp6do8Iu2/c/gbBEaWAIUSqjggBNiKOEDQ4i1OKvcPRY6Q7oAg5aiw6i/b6lwNHVQ98Wh17gcXP74eHwN9Hx4/NUS5JarcTNq9d56dWX+ZP/tEW63UHmxRohhJ9fjvDIA33vOywSK2B8coZSqUKrr3DbM8a+XRzNEgvheiwmeQ6sh3Y4yLiKXvRWHE7vsJynahZhSHV0lDPDw5x9+iwm+TV2NjZZvHmb+4uL3L11g/2ddZL2HsZptHG0N+s82Nzjwys3GKiVWTg2y0Al4pUXn2NyYpharUZFa6IoxKZdtve22W5sMTI+QrkUI4AgJ5HyK4/DyzhjvRiKzCM70dfApI1XshLOQKTYb7RIUoO2ggfr2/zRn/yY+8tP8fUvvcTw8CBRHCKyxCNvdBtkhpAu55U5/Cv3JuVe0ianzSCvt+S/n5SChYXjjI+P8eDudcpSE0lHqDLKpZDJ6XFGR2qcmBvnzJkFyuWAD99/n86FJufOnvP3pXSkWZLDUCXTU5Ncv/WAzBgyY5DOY1iCcpnK8GivoqRUAVY4atUHSJd+Zy54uD/1KEayz4p6d94RtKS/LsIyMjzE9PQMl/PUT5ZmdLsJYexhi9gMZ0wP31+rVUizhFS3ECIjyzrgQkKVEocabEI50ESBwUhHOZAopbGRQpUGCZVB2Q7lrEVJpgyIiKZwaGNptxooUcWYlFplkOGhUQrqh8IHCPqiJOGRMt1Om1q1wvWrH7N46womS4jLikrpcBrw6Pg8kfvvAf8r8Pt9r/0r4MfOuf9JCPGv8uf/LfB94Gz+/+vAv87/PnEY69hrpL0CUUkdRMNBoFAq9KooKibVGcIpMgI6acZefQcROkQoUCpGiog4jnBKYNOMSAUEQhJaL4flrAOToTNN2qzT3hXQbZBsrbASCqqB4KWzJ3j66dMEtYz1O2+z30oQYYmwOkxleISBkWkGx+ZxagCKrlMhUM4hrRfOVoWGqD2Syzn09x9vHPQG+HWtFZKRyRnGp6bIbIf1nQZjc8epjU/S2NmAXi1B5Nw5OS9P30QE5ILPFo3DScXkzByGJ1fp/6mGy6M0kxOP2SBEqkEmTwwxd+IcVhvq+zusLi1y784Nbt+4zoOl+7Qbe0ibgda065bNy0soDB9eusvsdJXjx+eYmZnhqQtPMThaw4UKKS3dpM3O9jbluMLAwABRhOeZKa4z/roXDKIHDItA3qlqjL9azjmiOPbC3oHCJAJjBXuNhDf/9n2219f53q/8MsfmpwmRoFNKZUUsBMp6TpgeIqpvPPJ5DzllEU6zs/6AxWsf0tlZY2qoTFkG1MIy50/N8MILz3Ds2BSDQzWGamWi0AtUJEmXzKRsba0zMjLi+1CsT0dIETI5OUEYhj1BCyEE2jpqo+OMjI4hcwqIIu1w0FVu+2z1aJTOw45cHM5Gfe6ial/NbHRslDAvdIoc2aSkRElJtVIliiJia3rMsT4dpRGRwoQRA1UBtoUUjnIU882vvMrNxSXaWlMph0SRIiqVWV7b5Pbd+3QTSRDUCGpgbMDwwBAi6fJgZYnBkdMk2nLyzCyVagX6CvBFlrZfcc5Zb0d7Ozs0dnfZXLtPqByNvT1k+cnu+zOdu3PuZ0KIE0de/iHwjfzxvwN+gnfuPwR+3/mje1sIMSyEmHHOrT7pO0qlCpMzJ3x7d7eLtRnOeoxKljpwGshyA/dRqcyJsUwvDeF1JMPAgbNgHASKUEpQiihwuPCgkKgzk+cKHZXAUQo0I8M1zi8c4+TMBOWKIlMJoeyibAuXdEj1Pq36MstLN0mpMXXsaaZn54jj2OdpgwBhfR5RIQikQoqDZhKXB3y9PPUTOhn/IcO5HD4mhcf/uwBUjCVkcnqe1ZuXezlh7wNsz0FZa3uhUVFI08ainSQsVRmbmv17OfejN2UPclk8/oxL4R07eVRf4OojL6eSw8BdEFGemOPM2DTnnn+Vr7c77Gxseajljass3r7B7vYG3U6LEEvWNdTv7HD9zhZRdJXp6cucmp9henKYi0+fYWR4BOwmaafL3m6DKIpIuhmBOrhtehOi6O+uLhhAba/YroTH/0dxjFAKjTdTB2gr+OTybbrG8uu/8T2mZyaoxgoXJqjQoDA4IQ+ivCdd575eBeEcJu3y53/8H7l7+T2CtM54LWS8VuG5C6d5/uIJqtWYMLKUVUooJYFwSBmiojKaKh9+/AnHjs1x4cIFhMg8rltKKpVKrx4jpcztRnDs+Akq1SpaSF+Y9gf18PE9zi4ePqNewf/Q+0+wF0/wlTtI53j6qad4e3qajdW7REFMu92mUi3jgFq15usr7gBQUCqVyNIOQaA4feosF86eIw4t/9e//xNCQs4vzPBb3/s+m6vLNBubzEwNMzkxzAfvvYteabNpoOsCGt2MmqjhUt8j0tESgh0yJwlD6WU4jxx7j0UUvypx+KAgCkJGBgeJA8dAOcTYmFj9A9MyjxlTfQ57DZjKH88B9/u2W85fe6Jzn56Z5b/77/9HWq0W6xsbrK/co76/z9raGhsb6zTqe9Trez3nY50hEBahiiKSwmiLNSk61TlHtyTQqpfLdsJ4GT0VgIBYeWUWISAOHINDZc4+fYoTczPEwmBchpUBIohxdBBA6DogFN0s452332F396+pVgcpV8qMjY4xPj/P7OwJZicnaO7XPa6ZooElT9XkOvfF5ASAEwfOzh2kIg4M+Wh089k/UNHH59MEAcL5HP/s7HEuBzE26xYb+l2Kvq7KwsDy6MdYhyWgMjDE8NgEro/VrpeOom+u6h1fkWY5lHTpHeFnUbe6vuMr9ltk8p1w4BQ42ZsnjRQYKZAoJI6gVmGyNMrM8TO89sbXaeztsrG2zM2rl7l17QqrK0voVgDWYFPB3aUGy0vblGPBO+98zPmzC8xMDLGwMEu1UmZ3b49Go8XI8DB9pcHe4fn8ukAGhwmdrHM4rTF5US+IQpwQHh5XaLxayc3b9/ijH/0Z3//+t1mYGSOWHaTKkMLmFYs8gVWkrI5W2ultkv+mjkA6SqEA3aWsHAuz07x88Syn5ieplSGKJFCkaRyh8opLSkicDrlyeZF337nM7/7uOPPHxrG2g8Pg7IFspRcEl8gg4pnnX8yLpR495jx/wuHf+jFpluJxL0v52K2ObuAOnuaRvs5Sut0O1hju3FnE6iwnR/OpIo8A8x/LMkOaZASBQ2ceNum0IwpjXrj4MhMjI/z+//n7LK1s4TLBRAV+5ze/ye7OPR4s3eBbX30V2xrm5HSJ//L7F1jdWsa4hMwGbLcFH1xv0RUjZDpjb3cbwhLaZPl9dvj8etcUev01AkkcxXzy0Ydsri+j8HTFVv8Tc8s455wQ4gnz6KOHEOJfAv8SYGp2jsrUBFUxwcSpBZ4Vr1EQMGVpSndvj/WVZX70xz+iXq/jjCZQXq0+SVp00i67ezuoZAeMJJODdJzqWb9H4xikcJ4XXAqCMCQIFZVKxNBgianxUTqNDpeu3KLVbNBsNOh226RJilSKocEhhmoDhGXfpSaTDJEkGDRJFnFj+Q4f/uItgiiiFJdQSpBmqReSLsUkWRMhNd3mKtn2ImEYkWQGF5QolaqEQQnnFE5JstA3m4dWoHK1nN51O3r9ecj/0R8PCIeHYWLRCIYnjxHVxmjtbvgmppwjvegBEEKghPQoIcBq/751KaPj04TVIbQS5Bi3PJ/qo8SDwpfo5dz786OfNT7LiISTh6cHl6/SyDuIhSTI01JOQuYcIvRRk5IlBiZmGJyc4fTzL9LuNNnaWufOJ59y69KnLN+9Tdps0DWCLDU0H9S5e/8DIuWYmhzl5Kl55manGKiVsSLEyMj/FRKBznOlIsdCH1YCQnhumsb+HmmaUSqVCcMOSZKA8522QpXQOG4vbvDHf/YzfvOHv8JsWROJJoGqkDqVT/IPzaD+/Cmi5OLtAGEgCCzTkwPcDQzHJyd4/ZVnmR6vEJYMIjA4KSmVS6ggQIQBRgqs0zjpybWWNnbY2uvwH//4x/zub/+Q4ZEqkJFmHTrdDlI4UqHQqsxzr73B9OmnEUIQuAzggC2+KOB/hi0c2POjpWweshFX2HshPSnpZBlWWIxLSJN93v35j9lcuYvIMoJS7OsXGtKuZnevC87Rajqq1QgpJEb447VxSPPBJu/9hz+mdW+V4biMCSWN7S3e/OnfUZrM0BLaHRithMA2s9NNzi1UEAQ4J3jQGuD6Skq33iISA2iRMjg0y8TknMfTG4/HL/pGDlEOCId1YGzk2VitRllNGDiCICYU/zRomfUi3SKEmAE28tdXgPm+7Y7lrz00nHP/Bvg3ABeefd4VHBoI4WlE8emXSCmqcczE5Dinzp3z0aTz/OgqcCRpm93dbT55/x027lxic2WTejvABWX29+skSRed+cKJ74bTPYcUBJIwEuzs7XHl8lWcOVhKe4SLr14bYwjDgIFahYHhGlG1RqYz4jhAqZzWKZIYITFO02nt+30IgSvHVGPJ2MgAk+NDHB90NP8/6t7sSZIsO+/73cW3WDMrK7P2qq7eZnow+wxmQGBmQBAgOKBEQiZRMMOTTKZH6X/Qq8xkJj3pP5CZKJrMJJKCKMkkgSK24WAwM70vVV1de+4ZkbH5dhc9XPeIyOzsBQIla7pZVWZEeHpcd79+7jnf+c53Hr1FFEfM5jlH4xlJ0qXXGTLYuEyyc4do6xIIvfTYz0/ws6oca9BGeOPc34THRTQGJkkz+oMhp0d7aCXOHLxNoK4/gnVdY02NEIqrV68hpMR4H8QT1r7yIu/rvGE/67CdPavPU8z1cbz1/MmeS/C2574+nua/OM24eus2167f5Dd++7c5Pdznw/fe4f23XufRhx8wHR0hvMAieXpU8XD3LZLoHV596Qb/3t//Yfi2NmfRwBMtROFdjT/nVgtAekc3UXRiQS+LwHmKKoibKaWQMiTQHj9+xk9+8pf87u9ucOlKzLhy1B+/ZGevzbmbIJxFegu24srOFoN+xksvXafXU6ioDjRPFaO0WmLnrQ62J0y9w+NjxtMZeQVvvP0uN65t8+Mf/5A00xS1YVHWWDQ1kt7mNt/9Wz9C6ni1yJ+5patK0s+70F+033qA0m6y0UaSUjE6OWbv6IhBL4NyyvRknzKfYG1FFKnlfoHJ5Fgs8mXUXJZFoygp0ViMKTh+8D7f73T5+s0XmMQRRz7niZjQSWKqAoRWCO2IE0siPVosGltT4b0lFjGursDFKOUQGjY2LqFVB5zD2lBp3+l2KMuaOElDflGERUsACsXpaATOghckcUYkHMVi9qnX8P+tcf9nwH8E/BfNz3+69v5/JoT4x4RE6uln4e3tdpEiYYthWgVeKJKNPt5DLNpGDYZI9OhubYIrORhGzF6YIZNtvvX932QynXJ0dMSzp0/Z233O4cEBo9GI6WxGMZtT2Zp8XnIymSOcJY3jppBFoaTCOdl4tRHOgJ9VTKtjdDojTvt0e2mjJ+KRKib2cSi0qmuqqqJ2Fo1h0I25cWWbr3/5LleHGbEokK5C+pxpeUw9O+b0JOL4kWbry5Lbly5hCeJP7q8fFJ3bxPJaeh86uvf7feIoJo4EGotHrVoBeh8UIJsiLGstlakpXMTl7csXQikC0RiGtXZon4Whn8s3+AveWxv+xZ99RuguhEfYGlPVFHlBL+sglMLicTLkJZyOQUUMbt3hV2/d4ns/+k1G+3s8uPcuH37wLu+98yaL2SleRBhXM7lAia/tu7kUnvI1gazS6rcH6mckPde3h/z6d77KX/z05whn0VpSVa4p1HG0DJh33n6P1155lWuvfBflgh7Op11S4YM8xnJMwuFNQaLg2tUrpGlCXedEsUDpoHPTjq/9hydUDzcJjgcPHzPNS2qfsigNf/JnP+Fr3/wyt+5c4eB4zKw0WJ2Rdvv83u//IwaXr2LPVyCdhxQ/Y2vx5vb3T9pn/acShrKcURQF9z74gKKsmGpFPTvh8OkD6sWEXj+lrvOg6CgEZVmSpineexaLOVd3tsnzPDSOkZKO0ujY0HUTXomgiGvGsmIrsVx+eYM737rFT9+5j2HA6RS8uISWx7i6yfd5AkvKSiKVIFDUpiLtajY3L5EmHXAVwhlMPWN6OiPJekgZY12FsxYVhSpf6QWDbspiOmE0muJNB41hdnr6qdfy81Ah/ztC8vSyEOIp8J8TjPo/EUL8J8Aj4A+a3f8XAg3yPoEK+R9/1vEv2lqj3raj8kpSC98YWhBenA3RhcNJgdcSncbIJKF3eZuNq9e59fKrfFuAcKFCbT6fM53OON7d42h/j6OTfXb3nvD8yWOm4wlFUWBMjRRtuNfyckOlmMDhbI6sAjWz1+kQxTHdbkKaZEgRFAKd9VhhUQq+9PJdXn3xDpcHXSJvmibdQU3RNXik9yFCmBwdQlUj4ohWEeRvIlGwlrcMrxENrU+TxBotQhswqcKDHhZZsRyTEAIVaWKV0e12m2Oc/47gqolzrvxf67G+wNj/dbaLFh1nLc8++pDnjx8zH0/oZh3iLOWVr/wKvUuXcELhfYRA47ylxiBjxcbNu3zv9h2+84MfMR4d8uij93nvjV/y6N77yPjjIwv1AHL5u3crzN05h/AK6UBhSFXNt7/+Mp1OzL/8059xPDHkhaUuKqytwqUTkro0/PKvfsGv/tY/JJVdZmGifOL5LxsyNd9pTEEqaxIt6GZp0w5OIEiWHYpaeYR12Qkpgh5RbQz3HzyksgKvNF4KJouCX775DlduXOHxsz2siOhtXuE3/s6PuX73VZxKWGrFtDdxFS79tbbPa9jB413B6HiXhw8fcv/+fa5duYaOFB+9/yajvcfMJyNqU2NticciZUSe52xubpLnOUpKolgymRYkTTRTO0Nkcm5du4Q/fE6kKqJFybVBj6984xabX77Ez94yWBMxnTvmRYd8YXDCIBxINJHXnBiYzA3z2oDMEKiwoNcLtIK6KrHlFBBsXtoABdPphLIsqU1JVVRc7m5R5zOuX73C7pMPqKtgE7T6G8Iy3vs//ISPfvuCfT3wn37WMT/he4CVoVj3AptGNgFL9cGj1b5NMCm8dygkWkiMVNTOhWSilGdwSJVm9NKM/qXL3LlzF+lDuzJjK4rFnNlkwuuvv86f/Mm/oiqrULShFGVZMp/PKOdT8jp0PHdFzWw8acJpGQymVEQ6Io5j+v0eg2GfbtYhiSL29/fYfVJT5gVVXVHWNfP5Aq9jOlmPLO0SxR3mRwfU+YI4ysCLRtDrbLPfddMXXp19feZRavjkcun6CaSKgrSyM0HZsSkoabVmAiWsuR9yRdUKneCDGVvnbYj1hNZyjBfe5Avv+efZLjrihehzm11tsEsdR6goIi9LpFRs7uygdIQX4SyklwQiqAc0VniccDjhkEmf/k6Hr23f5Kvf/AGz4yN23/0JQlYhaUigkSqlEQ1BQ0mJjzTOSayxgenR2LuQ4jYIX3P3znWs+z7/+q/e4Xg0Z+qhLFy49gashN39I6qiYmOjy+lChFJ3/HLBd0I1DodHehtKYLzHVhVZJEgllPMZ9z64j7WSulbUtcQ7iZQ24OzNfVinwFrnmM4Knu0eAUFrxSApjOX9+w/49vjb7B2N2b5+m+/+7d/j5t1XsCrCIxHivN6JO3OHLsLMP/HGtvfzYx83KpIu3Ddb5cwmRzx++AGmXHC09wxpDQfPnlBMjqmqnMoaPI5et4MUitl0hmyaXnghWCzmdLopQsggNIjE1TU7L1zlxa/vsBidsDWt6QwHDF7ZZiwVeZ4yqQ2FXfDBRw+42u1w/+GEsjIoGaOdZteMGOcCEyvq0iMXNd5Z8sUxiQ+J6TSOGAw3qE3FfDalroLa5jtv/Jw6L7iycZVUBqe2rh2mqkIC/BM6f7XbF6JC9fy2rGZrShqVb2u32h2CbkXrLUrn6fmI3GtQnsIFVUanRNArF6AkocCj8cKNaPFZDV6h04xLl7b40e07fP/v/i6mNmiCca+qitPTMSdHz9jd3eX5s2ccHBywmEyYnE44PT1lXhbYco4SOjzUPCeOIoSHv0z/KrTFEsHDCmMLPTilCMmsNEm4cvUaV26/zMnBLjf7Q2ohQBjOG/PzSthnr50/A4kIHEoEDXHpBFiJk3FAUVyFi2IEcklrCz1om4Khxv0WgJcOL33Tw/MTHtLWaVutI6txfMZT/Ylmfo1N8LGpvLbwtDv7NePugcu377Jz8wUOd/fpph26GxvLYUkAGYyRY5Ug9kisD4k1ox2CGqczsrTPrfIARg+QMkILiRZhAZSRXoZJiihUKfuKKEmwxq/wbGGx3iCk4u7tK/jK8NOfv4m1GuE7FHkF1iGVgrSPF5pMKLrCUdlQ+ClFiZOGQmTkTqF8jaJEOoctKzaSFF+XuHrBz37yEx4/uI+OukwWOUWR4zsRSgZedxRphJToOCJSEV54jBccjkpOxlXTqKTGoLFC8/x4yntPj0kv3eKHP/qHJNtXGwfdI0R9wQ00Z231uXu9KsBqdwjLoDj3N6v9faMN5RqBuJrJ8QGT411GB0+Yz0vKqSGfTcinR8TKkCQxxXRBXYcipDRNMQZm84J+v081X+ClIElTrDUsFjmVk+gkot66weAbd1DTgsmjEwwx3Y3L1NMxI6cZ1QXZ8BVMvMHMVnzwoOJ0lmO9Z1GX7FmFTTexsaJ2HoFidnpEtdilnI1BZXSGl/FCYm1FOR+RRYrp+JjZ3oeYYsHDgyfYSvHuex+S1wZvSogVWb970ROz3L6Qxh3WI9AwdT5RkZGVsZMiJENsbc+u+GK54+p1+yXeLx/KFifNOp1lQkgIQeI7dDeGXH3hBV5DYI3BO4uscxbzGScnJ5ycnDA6PmZ8NOajDz/kcP8JB892sdYzmdVAHbSqhYBIEWUpWidImaDTDJV1mBnPwFqcFE0DAsvHZYDFx87tbDODTw+CW2x4Xe3x/BaqgMWyQAcEvsklrI7zKV/i16z82ZGvD/NTt09KvH1yM4OLcHyxPNhSy+SCwS8jn+Xuq8Ri+773gdJ4eHjINd2O5YIBLle1ACkmSYKPfYDprAUk1hmsqxAy4s4L28zKFyneeoitF9R1M5cjQZJkZFkaDJSZEEmItaLbzVj4mGruUDg0NcpVYA29RFPnU2aTMW/84mfsP3+CNxV1VZC7/GwBkVjNh9UFCFdtb28vtNNrG34IqJViIhLirTv84BuvotOQG2qprp90H87OV3FmH8EF7TU/47XwDluV1FWJ8pYnDx/y9NFjJuMxRV4zP61YTCcoDNi6yTeEPJgxBtm0RXQuQKvOORbzElO7ZngS6R0q6hJv3OXnHy74o3/+v/P2u4+wSL75jZf5vd/6VXSk0cbR6/W4vL3J0B3xD37rRWI3wVaGiYP7k4w/+tmceZXgbY21c0x1SrU4oigdUdLHMsMyJUq7lMUYm3tsXZAlEc8PTxkdP8M5xdHxfijMshZrYWNj44IJuNq+MMb9s5QOz34eHv1W28T7pkGCFMRxDIVr3oe27kOKTwQLznzHEtNrkrbhIB6kxBDKfZ0OxVAyVvQ6HXrbO9wWAkGEsIpyNuX5k3v8N//Vf83JwTHO5DgfgumtrSv8zo9/l80r2ww3N4nTPlkW8GwpJSKO6Q03AjxwvoEun2DAPgekuV5kE0WhWk8JiW0rKpt9lgqWrPp+tivjunH/m27nUgEXvlbnnnH314dugXYR+Hww0HnY6/x2cHDAtevZ2jX3WBc63Autl4utEHJNFMqB96EHpxMtexMpIE4kL7/yAk8OJuR5TV4s8Di8sgw3LtHvdZGRYafnKcoJp5NTHj4tOK5iLt16JRTK1SURFb0sYX56ytHT57zxxuuMRscIW3Hw/Cmjwz2ub29QFCVCBIGz9bmw3t/Xec/TJ09Dda1UeCHxQmCkwugenc27iGyLGvmZ5Wyfds1bWe3lYuo9nxAXnjmO9B5rDdPxEYmCg+fPON4/ZHI8YjbNKQtwriZRjqQTL/tEOOeIoogsy5jPFkBgg2kdUVUW7wS1qUPVahThfcRf/PnbPL13n8f3H7BwggLDX/30lyT1hOl0Tmd4mSzL2B72GJTPSYYCoed4aZAioTOO+afvzJkuDJeGXUy1YDbe59H9RehGJjTZ8DJ3Xv4yw/g6kSzZf7pLLBWzyYjDwwP2945wTgSnwFoiJYnjhOoznscvoHG/+Mauy9AunVkRCoTavw/SoRccewkvfPzDVRJwdZz28C4QuHEueAuy5e764O3VUuDPNEdOkUoTDyIGW1dQaYaINUpFTaCpuHr7Jf7Bf/CHJN0ORoDxQfQsdJFvDk7Lkvl4v1XPeVt+/p0WRzl3HZvoRAixLBk/8zfr12PtWvnmXJ2zZ+/Bp2yfD0k/e68+dl7+481RlnTZf8PbWcfBX+hNCiDSmuvXryPF+MLjrATDmtqBdlHBgQgOx8HBEb98422+/q1v0+mk4AWdjmZ7e5OnT/YIrTxqUJJvfes7dLIMoWrm0wWv/+KPeevdd/jSq7/Oza/8LSrh8bYmFtmshn4AACAASURBVIZBIinmJ9x7+03ef+sdqtrgTc3B3jMO957hTUVZpEwmp3g/QIiwWMdxvEysWhsqlYu85vnz5yipQvNvoUJzmUjx8ktfYtDdARcFSOtTnLKL+oG6lplz7rqvJ0g/dfOtvIcj0QLpSxKtMWVFMV+Qz3JqExrzWCo2+kO01svzbKvJtdbL8wdPkec4a5pcU1jQXG05ebrLjk6RG1d4uligtUPPj9h/8hStU5SSHB4dkd/uc7VbAyVeGpxwgEH6klKErq/CgzeW0cEBe48fohNIel065ZT+MEVqSbkIMtDPHj1lMh6R53mQ90USxxnWxdgqpygLTscXz8N2+4IYd4+lXJbDd2MJzjKdLsiyHs4FxqdovYim2rMt1pEenNJYFdqSiYb8jw9JiDYBuF5r5fxZFor0qzA/TLkg8eoJmjEQJlQL3wTTq1imocQKK2ylB1RTISiFasJ6jXUSJwI32AiBFaLRu26hgfbbg2GXa55M+z3rkqfCe+Sa0fWsH68xKngQDicMXtQgQtLMSYd3QZXRrYXLEoWQMkgnANZ7cGCrAulNwAfXkjnCrzD1UFXX8ODPsVvFJ7642HM/bzaEl2d2CtDZ2j0lYLHhM7/6v8VoJXhhPn0g7bsNLhd+hF713lnSwSY+nyCowaUBymvdgYZ14uxaUtGvFgvrLYuy5k//4nU+fDzmR7/5A65fu4YXkp3tHeLkXpPMhe989zv8nd/5HZRSjMZ7/A//+L9lNH7K7Tt3+f73vsthrbC2JIsVkfPMxiPe+Plf8cG772GqQP88OTrm8GAvkA3ijKquWOQhMpBKIJBBAqFZRL0L9d6T0znHRyN0FIy6R4LUZIMhP/zBb5JECuOb7lJrEFbwTfxSOA3AirWnTBC0lpZwWlOD4Ns7tTZ/vW8E+KB9rsI4BeBwzjCbjFhMjjkdH3I6OgbrAmRR1yGaV7DIK/qDPkkaoRR0uxlxrInihKKoWSzqpn9zoAkrDUJGOCmgrnHFlCvDPnqzx1gUVHlBlka88NJd9u49Y6BiVFXQ7wqczCG3ULqgAhvHsEhQtSP2gjwviaOIo5Mp89mMjct9fORgOmV6vI81Fp1uolSX0emY2fQUZwzCh2jPVjWa0HEsjqIgu/Ep2xfEuDuQxVIBrZicMjkZ8dGDZ2xu7ODRGK949bWvknQHDYMkNFfDhWJJESe4SON8iZAh3MP7tjE6aq3tG35loNttOVEbnD1g12eVo71cqT16BEoEQmNb+ONFaOEXUYdu9FiUdKHVnpMIrxAuqGBafPh3gQPdGoPw0rLycs8nDZqek2viZB6BFbqJagJu7zx4CRaLEw6hROONB76zFx675q0KETB3JVRgJngZfq9KFCZQCOUaC8KDtMtHNTytzn0Maz1Deb4wwjr7wp6L+b2HT+NNi2ZBW10lt2bsHaErmV1SPJeDPzeGdY9S+iCjq5xBCouPO3gjwVdgLbaJMLwLEAa+1ZpZH3hjnISlRlBaxf2Hu0yLP+bHv/c7XNm5Qq/bA+mpXMVgc4Nf+erXmEzGbAyHnJ5OeXD/A27sbGLnFeOTI1znEv0kRbqK4+M9fvanf8746JiyLJhMJowOjnB5RabjxvA64lSzubVJmiUkqWdzc7O5NSGaU8rjheL0NGD/Kk6xMkLIhLQ74O/+3u9z5+4rWCxeBKPvWKlQBkN9Hh//+E0Oa3ToDeu8Dl2OlvdPNC0Fw3ySuJBPaL9DgFcVeXHCm2/+lMNnj1CixLlqVYTnTOOYKOZFTVXXQdwr0QjhOJ2MQ1OgKKGsGto1DbuuRQlcgGUXsqbqeIZW0M+nWDxf//LX+N5vfINfPPwX+EVNbEpOd5+CmjAbLZBWk+iYKO0yGqfE3hCroJsPmmluMC7mNFcUrmZHw+GzZ3CwTzS8xs0Xvs7ReERR5NjKIpwKlazGI0Vg29RCIuXfUDjs/4/Ne4fNF1RlgQQ6aYLcGDIcTinLkk4WMej10CKUy7sGkoEAGUgBSmuUavRUAOVtU1rfPmXqjLERTp0p+nCCpuFzk2ASDs5g3sG98X6Fcau27LmZmta3YaoF4ZDSo3WbFxBYJxuv2Ybv9h/HlVf+S5jey7ZjbeNfsWqFAc3DcyZJJlFegnfLyRrobDqE0j5G65hWQ+bz36Ozr+Wa1yAA7WiiBPBW4aRrQtNmf86F4e0ffsp2/mPv3af+jWi9xuVCGBR2Qr+VpqmyCzr8sk0mi7odzfJLPxEnXnvbe4+xFmUCdt7mK9Y/X41LnPlzITU4yfHxmP/5n/8Lfvzj3yVJM+bzKZ1uzA9++GtsXd4kX+R4D1rHvPjSS0iTE8cJSZLghMMsZjx79JB33niT2cmI2WTM7rPnTKdTfG1IdUzA+0EqQZZlDDc20DqCUPO6xNyzLIOmsMtLDSqCOMPJiM5gk1/9jb/NN7/za+AjQIOPmijVhKCliYPNua66yp6VmbOSZk8LAiQl0lZNy3OLaHv+IqmdoyxKRL0IzoEPkX1nEGGKfY4OPuRg/xG9bo8oChRkrUukDBXVrUJnVdd0sm5ozlIWWOuJ4owoklhrsLZaVqZrrUOBk4woYk/lJS7OuPWVm1SXI66++jV+41vfZ7T7ELzCmNDR6enzA6qNlD//yRG2hm42JE2n7BUlC99BZQbnQovFAMMqFosCUyvyTspiMqbb75HKGYd7z9nZusxk7xDwGFNjjUBJ3bDZQAiH9/8fa8v8m9hMVXH89AlFWbJ9eQsivUwA1rVl6/qQy5d3QDhMuQj4dJsoBVrP1rXyAdYivUF6s4oXvQAhV96pNx/zKpZQdRs2Nh78yhMLyny+8QZDMLCyNla6kAzzBiEsUjqU9gih0FpRVQ2h0zf/rRn3lg/tWqPQDE4ilsQwz5JRt9qW2K5o9vd4VyMaWQTnDV4JEDXOB1plmzi21iKUWp7CJyW1wzVbw0o5uygJPD3tSWKDoAKvsNIvm243d2j12K/lNT5tO5+sc94ur/3FAw2c7HY+FLVhXoFQMVpKpADp6+DhtecrBKvqaLEUa/rY1syFdt+VwBogJGcjgWaJbu9lM4eWJ+wlUsTgHaPRlDfffJsXX3qBus75wY++zw9++D066YA07iKkYjjc5MUXX2T30X0Ggz6DwYDF6JQ3f/5L7r/7PqY0TMfH7O89ZzafEwrVBEIGV0dHmjiOUFo1zdkbfr4M0gNKKbTWOKWwTpJXjqPxBPqCnRfu8vd//9/n9ktfRioJjUpRwM41yuvlWYd7tl5b4lfdoZpcjnbBG/eEZvHPPvgpzPZ48dY1IhHYPxZF6RRPn+3z3v2HFFXo4+qso6xKXnj5Ftev7qB9xWJ2GtpnWiircrlQKRVRVQVRpCjLgqqKwjGcQ4jQJzfNAu4exRJ8cHjKskQrTSEFOpJ841e+yde++xp//PQ9nu9Z3nv9DXQUcaeXEsUKKTw7VzbI0glV4mCQshjPmOQnuIVh5HosVIb3hswFxaU0TXBWUBY1lXdMJznYgqzTxxQVzx4/pJoYxsdjZosiRPoWtIrwBMEzUwdl20/bvhDGPV8sePuXv8B5x8MogthhjGM2q1AyJZ9MUVKF5KNQ6CQmTiKkCLKZHokt55Qnz3DGMKs0b77+C6IoZMrjJCHuDRFqlUiUriBICYctUppI6aWxlAqCsmvoqxpFEUqGxEwo2waDIIripbFyNqY2Bm8rpPTEiaLTTbh27RpXdq6wtz+hMimLxYJeNw2Rgm1hhDYcWGHfAF6oM+CQl4p16VePW0JQR8fHVGWBwtHtpWxvbxJFnoKC4YYkTg1SV6uox8N5O9ZK1l5cFbtq+i3bBYqAs/aTmq2BQat5E02oMweXS7yU1YJ4BrpYN/btIiLOfCiEhU9ZEhwKJ3RjTB2vv/uA46LD9pVbSBxSOHBTjo6OMcYSxwnDwXYjHxCUDDkL6wcqYHOuLbvDWHOmxeLqJGXIC9i1+3fuGgtASo2QMUI6tFA8f76H0oJbt2/wve99m+l0xGJestFPAwZtYWO4QbG5yWAwYDI55c//7/+Lo/1DqtmM44MTjo/3KIp50IjxoYdvHAfj3f5TUmGdQwpBlMToSDfJ1GaRQ+KEorKevKy5ciXi3/mdX+e1l67hdBV4/xiU1AgZvH/VRJShoAi0X1v88AhZsfSwBMRWBQ9dVizcKVM143i0S7Sdol2JEh6pNaczw/6zh+TT0yZaCLUaoirYffiY0/0DRodjbOXJqynGeOra4Vxw4AILRqO1Agzj8XjJlJnN5wjh0FrSH3RZzAuyNMEYg2k6fkskUdohunmV//L9P+XDGztkRMx+9pfsPnqXP/jB94hjSGJJb6C5lCmGacl/+PdexFUznHdUXvHes4T3/88xIsrwIiw+cZxQloYkyajritl0wcYgY9gdorKMxXiBqz2xjrF2jlQSYT3WOoTXy7n4WdsXwriXRcEH770NhN6l1oMQzaRDhcjShT6WPsRyAU+mfYgESoKiboLxiPsffAiESaaUwquoSTQ2sA0lkhrVJOFiFZFFEd2OoteNkMrRGXRRSYyOY6I4ptMZ4p2gKGrqypH0hvT6A5RSTKdTDo6nHOwfYKqSusjRvuA7X3uJ69evUtcWm+dYq/iXf/RPiLopVoiGkUMQn0I03tHK00t0gmy61gsESoumNV6wkU2sgHOek5MT6rqkrnP6vS63b98MnXCEwVnY/eAD6tpztPs0dKBBNMnQlfRoy2iQDXbsG89L4hgd7XLv7V8iogRnA5zRJqCPtGF7IyJLTcg1+MBeChW84Ty9WMsdNBCK9D5MXAcLE6Ie6wrAIXQEHuqyRHpBGiekScTWZp800wgRoo+o7V0aTGcwps6RuAmZqOjpHmpYoSJL7SwnxXPefOM9nOoz7F0miZsybk8o1moLuADrgoekpEBriV8c8dplEdx7b5DEjXaMAlTDHbd4b5dVvhaPdwrlJXGny86tS1gb+OpChHzQYLPPj779Q4a9IVVdYUqQ5BhzQmlPuXp9k07nLkmacXL4gMjNubKZMnI5vXSDK1c0UaxI0xSATqfLYLCBlJIoiokjRaI921t9Btt9IuVxKsGIQO81FYzynJPxFIvg3/39HzO81CcWFc/uv8m1Gzfp9a9QN3Q8EbAo1pspiib3Ei6/a+DEnLoqmM5mpEmCTjs4V2NNznR0gJ08p9dxeFHgVUjsl3XN/sEx02mFkHEQzKJuGGWOMs95+vgJB/tHzGYFtXUYC9YIrHFYF+altQ4vDMNhhzTR1HURRAclOCebhc0RxYqqLsF7rA0Rrnclcan4Z3uPePidrzLvdpnenSG7v87+6Ue8uf8BRkg2ewNEqukP+vR9QapyRDTDaYVQGae5wpsKpTJUpIjiBO/KQG3EoFXQytoYpAwHfWqn6ImI42JKWRqci0JeDIs9k8f6t8S4e/ySYeCdBasaowJtAsyvKyQ2zSXCr424mBBrCPkK57POYTxBEGkJb/hGuMughSUSsLG5we0rm2xf7pMlnrIq6G1eQsQpqIT94xPeufcLdncPKHOLMR4n4zNJmPZh1lojpeSrX36ZqztDlIaFNWhfgYPDp4dUTel/09cd2eAtS42PFr914aFRuvUuWXK2fcgMNg9Z692GYHj83PH03vuAx7kKhKKT9SiKEi0snSRBKoWSzYIiW/hJNJr5rrl+FmsDbvvg3rt8+PARabdPpP1y4RRA5GoiFXR3FrMTMOZMnaGTK1leKSUIhRIa4TzChARWjUJQISjw1FS2RhNkfLWQDIdbDIc9di736XVjdKQwLlx30zRpDtdS0uv18DKhO7yJOdiFusZrRZzAtazkaVJxNBtzWpgGamvhsGCUVguoIi9K8NDtdohFDiImgEYOgWy885XIcavL08614PiLkFBXimu3t7iyvcONmzfIsow0CZTIKIpItCSWCV4atFwwOn7A4ckR5XweFtIapC945cWrgXnyyrWlnlEo0AmNM7wFb5tITIY225qKupgxm4WJ9WxeMugP6HQ6VHXNaWmZLkpkHPOl114B6YniiFhJytkEZQSLeUlVLej1U6RSWLESTfMeauuX5+2sDXCgs9h8QW1j8irGViWRcPSko7s1wJmIWLjlMaracjI6aVrcNbRELxFCI3VEVeUh+hSQFwW1FQH/rj3OSZwLKQMlg3TIYlGGeyQFxgao19VBy15KFc6/zLHWkqQhopGJQHZjui/f5uZrX+P9jx5z9Ye/RrU4ZufdmMHeEQ/fe8qNa9dIBptsXOuS2ASmI4SrkaIGI/E2UEaNqNFCYa1DKUm/30FXFYv5lG4nIUtCzkC5mI1+wmxUoXVMmmrG4/GSkvx5jHq7fSGM+/rmfRDQ8i1OycVY8JK2eBFO7P2SIb7MoPsmO9+sfkJ4Yq0YZCm3rl7mxeuX2RqkdDoZWkumswXGKGbTnCfPn/Ho2S4nxSneC7wLxtux6rgTGAerzjSdTta0J9OhGrHhzAtp0VEIzT0C1SBDrczxCu0I567iFrMMSSjvQmcnoZqKSxGKjoIRb7RJrMB4s1QrrHW4zbYssVWJjlXTuCDgrkpLrF+jU3q3XKhW8IJFC0GcKCIdOvwIKUl08D4jERao0nim8xxTVs1iE+6R84FXsVQhFAqnIrQXKAdYj68cSlZIFmSJoN9PuH3jKpu9AVpIZJQgFETSYYoZvg7RjjV1wzEPxtUAk3xOFHdQpaOIY4ypwzqYBI2RFwaCZ/fv8XQksE2yPSSqVtNINOdUN9S6KIrY3swQt74UrokIGH3bRk800dbHxK2aG+ucoyxLjg6fgpvw6NFb9Ho9vvzqVxBe0e8POD3ZpZOmAd7QCs8mH77/DvfuPWIymS6hBWvngCPLMqJIE0cdiqKm3+9TFAWHu/tcGmzQ7/dColFpXFWxudEnjyST2Snz2vDh0yekacLu7h4i6VEUhtPxHJqeqJ7Q1nD78mW+8tLLKGuwriSfCHr9DgJFVVcNlCkpTBWqsIXgyeMnpL2MnSs7nJ6OKcuKRCd4Y0i1IJYQS8Gw3wtN2JWitHA8mTHJJ8jEEzmFqcIck0IQJzFJR5KmEXm+YDQak5cF3lmMEXinEFJR14bNS5eYTmsWi4Cjp1nUQC+evFks4yZq6/X6nJ6OSZKELMuIY0U66BMdnTA8OuLrl4csfvqXXH/2Id8rRty+dIV/vfiAyhkqEuZ1D2u7iFyTGnCuwtBjXmoUhsLVRE5iqoK6DvdJQNP0JNRGzBcllRWYGmZFwcl4hJdZozy7ElP8vJpMXwjjrqSk3+8DwbCXhTtj3L03a55848X7lbfVdplvfw8yq27tQniEq8G75oFVoILH2ssirm4NuXypS5ZopE6orOD54SEfPXrOeFYwmddU1lELFxIaLhgCp1p5gHasDTddSqqqbry31SIkJHgbSs9bqSrRZJ9EU34pm2KmJRbajL91BWXD5GmPab3BVk2SbI0d01bjaa0bfLxp16ED7ioIC8qyCTZrhTtCLOUHAJQK49ASXDnn4GCPuiiW4w2bCWwZ4XDCooVCIpd88WXU1JyKUKAJsIw3FlvVJFrSTeHq1iVeeekGmxsDsjhB+4adIzW+qdwVygcKnQ+GpEF4l8bVOw/CIcUx1IJIhM9dHfIUfa1Q1ZTFOMf6VSHaCg4MchZCOpwPUETuPMNoe5VvEE0Vc5uUNSFh7c5QU1eQV5BSlqRxzNXLW3Q6HXZ3n/Pe278kUjEvvvgie3t73Ll1m43BJUxV8O699/nLn/+c2axmNgsGqdvtIGXQJ0+ShKquiKOMNM148qTm+PgYHSn2jp/S7fXI0jRIgdewc2WLql5wcLhP6SHPi+Wz0x1cZj4vqSuPNVCb0Ng5imIePdnlzTffIYsUztfEseTll16g3+1ycnxMUZR0ullQRWwSk/P5nCSLeb7/lIODgzDPlGRj2EcT9OZ3Lm1DcjP0m3WW2dxy7/Eek/yUbJjgrQseu3QIZTBViRSQdVJeevkup6dTZvO9pYKNWM4FWMwXRHFEvsjJF0HqN4obfr/3FEWxNO5LJ6QhZESVpusEf2/7Fs8Ppzy+Jbl6q8OvTCt+9ZXr/OzhDCFg/+SA58dzyongRi9h/92S1EUgU051xNv7Fi1TlFxx3NqcAKJlK4kgUyFCn+hIabavXmXv8JSj49myglg0EZpzbq2q+JO3L4Rxl0rR6/Wa0NKSpdGKqYLHmBLnVqW23ktwEtdyUhvjEeBcsaS64YPXAR7tSoSANElCKTKecj7h6vaQ2zeucvXaFpVzjE8rnu2f8HB/ztHCUaGho1HOI22Q4ZUiwnuJVRU0uG+LkXtYqkSuKHnLkTcoSiho8UIEqWLfVjWyos3VYdmQXgcsXgavFFGyDPYbOp5pMNBIh2pYISJqQtGOUJJERgg81lWApXKBvhUJhVYaqUUoUrItSLQK/lqWQ8DuPUJCLJt2gZ5lkwgvgyfrG1E3IfTq3IVoaIos4Q7va3w5BevQjV71latbvHz3Cje2M7LYI2SMQiJdk5B1TccopTA+fL9sPLq26KpNNrdJYYtprnmQbK4JjBuvEvq9Lp2oCv1m2/NtpJghXI+qKijyKYs8x3vH5X7UFOqAXyvQCdCcWS7oq1seIrYQUXuqsqSYFzy8/wgAYw1KQVnMefrkAc558sWcTmebex89YP/4kNo5TGUoirIpXvF0On1msxmLxZyiKECM8d6ymOdEsebm3dvITDGanfL45CmD/ia2Ujx6fxchLNbVjTqkbIyax8ymGAN5XlHkNVUNde3I8zwsWDJCKI3zNVoL7u2dIJwhjmO89yRJTDdTCAn5Ig/vu3rpsACQwuFiRBZJvKk4mI75cO8Z8/kiaLXIHt5n5HWBKxZEiQapGU+CguNwuEEUZ9RVzvHJhNPTcbi/tBChwroAycznC7JuhPewWOQYW9HvZ1gb+qXWdYWQQSuqrv3SaHrvmdsaFjOy0YSvxZe48ycPuaZqbl7ZYu9kj1+8+QThHWVV8HxvxLGreL865vU//whvQGUDbMeyO40QqgO2prJlECRsImnfQH2RonH2FFKneOfpb2zw0iuvUFb3l8ng9Ur99ajjk7YvhnFvBuqcI07A2hUyIaRAVwLc6kRar36FdQfa4aoWVRJJcNYs+0FuZl22L/e4ees6se6zdzLi8bPHZMMBhe7y4WHF3uGIvb19FnmBsR7Z75F4T9RGEEuPLKzwOuosQyXgTAGHavBsISTC61DMFHpWM+j3kTJGSBXgkGZyBhnXlYeulA6cdYIXiYCqytGRxrf0vWYBacgJTdJFslgEFbxIaYQMVZnSC6SVCGuRCOIkQeugLy3q9ZCv8YPXMAqHC9CMkkjtSDqBWrf0HppFYDUmuTynYPEswns0IhhqGyCewTDl2rUtbt3Y5salHv1+l0iF728XNNksCK6pbgyxjMdZt2I/NXLF7XeuQyKtUJSHcH1NRVlr7t66yoODCbZRBg3k1rCvsZaj3WMmkxOsKbHW4ZylmOfYwlJ7h3WWuFEXtdY2sFxI1rXQIgC1x1uJ84LZ6ZT7Dx+zc22LJJUga5x1yCjl2f4h1sLz+T2EesbJyUkTkYWk9GD7MlJKamBc1RzPplRVxWAwQOvwDF2/HvYx2uJsBZFHJoLKV+ioR6RiJrMxcaTwxqCj0HAGIaitRcUSZS29VLMoPPNRSS0E1itsBVKGxjRl5ZgvFkhlgUXDxsnppZ44CQyyjiCwySJBkNYuUcaglcKomMl0QmENZnQcWG02RssSU0j29w9IEs3W5SGxdszGI5Kkw0IKanGAs5q8qIlij9Ii2IxlB3WLMw6pNOXCBUiKMiTwhSMvFzgX+irM52O0jnAiIeukCFfj6zlKaWQF//3/+D9x+85t7gwzXvrmC8Rbdyinltdei/ny1zOkzFjMF0xmU/7Vz36B10NK7/GVRliJUGqp8V+bMjzrWmJ86PSkdYaMOljVo7IqOKWE3NTOzhUmp6EordvtEscxh4eHy2fys7SevhDGHVa4+ro+jF9jY7BG/zuPi1ovg6YvEilCr0nXaGd3I8e1rR6vvnCd69c3ieOIfCFYVB063T7jyZz5vQfktWdROmpjQWjQbZHSKsm5jnVZ2+qbrw2mwRyklE3F39lOou2ugVccI9eM+1LfYw1+0loHqllzzPAVhjgO2GFbWbjUvfeBLiWlwlmLqQN7SOnWE20zrg3cohVK6+Dhr+UvQv3t6rzOYHzN26Fls1y+bhO6zYtlSb5o+DxKGJSwRARYYpB1uHlji5u3dtjY7KClI6otpiyg8WyEXBnuM4voGo7fdtaJ4zgYfL+C49bzMm3yuYVPnIUsS3HehjLutQTobDbn4OCA6XSK/VjUuDp2e85t8ctZdcXmpwchFQKNq4PBkSplOrdk3T7WBxleFSuiZANqz3hSUdsRVVU3SWuL0hZ7WhPFgdZYlTUIRZxmICRSxRhrWOQ11lm8sESpCswWoSkLQ2EKsizCu0bTXYcZnsTNXCwMSmvm82D4kBFJogLF0Ps2nmkYKSHalC7MQ2NKBJ4yBylrkiSmqjVJBmmqSLMInWhcPcc0lzNO+hjjqao68OYJkhjzeclkVpHUDqnnXNqEtBMhhedkdABxTaczZLCRcWk7JS8t1tbN47eqInc2eOK2KTQKCKGg0+mGSBlC4t2HnqSJFigswtUMOr0lxFsbS61idP8SJ/kI0d/hSmcb6xWmUmxe7rBZeLae7LN7dEDc6zAvc7x1pBGB/WQMcaKCAyIlRVGQxBopFFUFzsU4pzDVip7tnWRjY4M0TSmKgqIoiKJoGWH8W2LcV4ZFtOjD0iCGkJsz3dbFmX9Yh5MaKVZeTifN6EcZNzZjXr29w+WtIXES7Nr0dMHpdMpknjObzdGRxhJhWJLbEQRdGO/9CsdvtvVxLDFrIZbCUe0+rhW4ufCMG7y6NZbNAtKWc7fGIsBLq+ujVJggxpjl72fK5YULQmctk6UZXxtuyga+OJusDzSszQAAIABJREFUPmsE12GZ5XjF2v1ojhdF0ep717r6AGgkSoDH4LFoX9NLJFvDATevXOPqziYbGzFRJJAqyAQo2RaZhAVeR+FetAvbmQKi5mfLSz4/vovmSju5JBGuYXkYUzOaTHFGEEUpZWXY29unLMvm2JYzDcdZyU+vG/pV/1nOjDk4HwVCOJytkWlNkoH3JVJ5lIiIk4i8qrGuwLoA7yRxH+8kSZogpaAoJxRFYFCY2hFFoddmnufkeR2SxypmtiioyhKlBX4ReNVpklLkhuP9Q4YbfZI0wRhPFElkFGNRjMYT6sqEStVmi2JFpydI0gRr4PR0TlVaaht0yb2zWOzSMUFIShPqAownJPXzGiEtWRYhlSeLFUJ4Iu2w1hDpsNBYI9AyobKKorRUtQ7uw6zG+gnG1PT7A+bFAlvkRFFCmkZcvTYgyQaMjnOePD6kLmuaAG9JdpAC4kijZJAk8M5jjSOOIyKdUhYlr732JZSwUJf0Oglf+tKX+LM/+zOePXtGXddMRyPu33+buj7Ga0mUZiiVEOk+wqdEcYekP6Q8OGAxy4mTGKk9nU4ndHtSEutq0jRaJkjTNMUZgbWKd976iOPRW5g16VMhFN4Jqqpa5gM+lqz/lO0LYdzXnV9YYb5t0sCHVPmZE1p/mJWAyAX8u5OmbG4OuHtjm0vDDlpUbGz18TpiUs7Z3Tvk6aNTHh8tmFc1Tmi8jMAFT10QeOQsf36cnrj+3evJDgfgVxTB85tvMXmxSq5IIfEEDND7FYa7zIwjlqFaa0Ta7k8XjkdI6qZyraUprrNW2sWgbTmYpimRlqu8QXMHzt+T8wuac+7sOQpxZhyRJRSrKE+axVy7NOCFa1tc3bpMP+0Qa4UQNUJ6VMMcclKtGfezxro9n/WtHfNFjKmLkk1Lb7txGPI8xxjD6XhMkRukDGJMxpil0XbOoOTaYqtkA0G1sr6rGKe9Z+2iS7M4l94AEi9r0JLeoI8xjvksR2mJigKdMopi8J5SQpFb0rQXaKYqxioR+Nm1QsURAkccZQg0VVUB8VJIKi9Ksk5EGmcMh0OSNOXk8BRva6anJVJ0UVoEpocXeF9TVQ7d3Ns4jplOp2FhcpYojkmyiKKETqfDdNrw8GUMcjV3hBQh8pShCrW0Bm/CNS/qCusq0iiwtOI4CNplkURiAUM+DyyqReEpjcYJBYXAWKgqh8NifcDMRyc5vZ4iyzK0ruh0JUkqMLVFIIPI4HLxFTgbmq+MTiaAa54DjUCTpZosTnj80X1euXuL3/rRD3nxlZeQUjIej+n3+/Q6PfodgU5KRrMZveEmcZSQpRssZpb/9X/7P6iNQcUdqrJgOsuJ4iBGFiK70NzF2qCuulgsiFSCt5r5tGYyqihyF0T6lvMpdPJq5/d6z9uPPX8XbF8I4w4tHNOwNETLfFixRAI0seLTtgqSAofGMIwFGxt9rl/f4erVLS51g7Lb3EhGpePo+SG7z59ydDRmMZcUVoTiEtEoTuIQLZumARWcb7nOYunKCiGCB6BDNWtraMNy0Db8aIp3RKs6s/ImvW+8/RaLFqJJmDaG2tmV1+xXaJRnbTEREq00Wkcr7160Rl5gbEgku3Ore8uKAZawTtLw3RGNUmSboD6DxCwlwZbXQ2oVKuea0YVo2KN0mIwxnn6nw9ZWj+s3t7m1k7GZhVaEGAvOIlXUlMGH6103EUQb0ayreK5742fO5wLDHqZLuxi1yar2jyTGGk7GYx4/fYiSgjjSzOcl1gqsC5S+Rq8tLKY63AfnLM5bQmJZoaO4qXoOIJV3hCizmbp5XpAXBZN5xe7uMfcfPOGt+w84OXR4BzpSSCXo9Dqk3ZReZwufKXodx2gyJUszLl26RBTHQEJVharLIPo2AekROqYyIUyfTSfEkWOrs0VVTrGupKqCEqLzC3au3qIscsAjtcD5HGMgiuNAEyzm1HWFUoo4SVBxRJy4Bs6qubw9YDKu0Ap8JHFVHebRWjwjowCvWBzeWbyLwavm2kmEFTjryPMQVRTC0k0DzDjH4KWmspp8URNFDpkH2EjrjKPjim4nRQKTxRxnS9KkT5qF4ynlAqLnQu7ENdG+8EHOuFWClULgJcxnOVW5RyfLqIqCr//KV/jDP/hHbF8aUpiab3/rm8zmcyanp0RRzO1b20Sx4ae/fJ1ef4iSiiTKguYLkllZUjtPp9enrkqQJcX/Q92b/dq25fddn9HMdq211+5Of87t763GVbFNYly2QWCMhJIH4kgQCYkEIySDlDwgQCLiD4C8gOQnhKU8JAglRkkkQEKCEGILu8pVLldfdevW7c69p93n7G61sxsND2PMudba99St4u0ypaW992rmXnPMMX7j9/v+vr/vr64HByTJgrPVtC1JkuK8oK4tZ+drjFVk6SgUWdHn+FyowPdu2ED7a/ppTs328Zkw7t55vDEhleU92oPpgpiP3WK8KKWQSuG9wnSgREemLNOR5o3bR9y+dY1ynFCWIJTkYr7i/YcnnM5WXFwsado2ZJs9eALzBQ/O2A1EEo19oNOFia2ExIlQ0g0xMy8knWlj7jBYYONDo2mpVBTNCtrTUgickGip6Wy4QUJFRUEX2rNtmCpi4E2rAIaHjca5YASdwJgAvQS4JuL6fSjqPZZgm4xzaII8AM6jotpjf4S8QBxTAVLrIMpmTa8QEu6Pj9o6MkRJqVasHDjh0LjQv9MHkaZMK8oy4950wr2b1zi+dsBolJMkDhGZK8RxlCqwdXr8XBoT2DTDV3zx5N32ZPpw9eprYQGEBS10StU5VJKhlOTDh4/55ne/zaPnT8Boykxy4Q2tDeMXoECBUDF6Ux7nLUiPUx1GGawqcfkBlRQ4J1gvKy4vzpkvFjx4+ITz80sePz7h6dNntI3HGBfKx4Uk0xM6D9ZqLuZrlg3s2RypAj6cJAUHB3dRWuNFSmsEtfB4GTo6CQHeZQgrUF2CdBprBZm4hUsdNvGMDgSFCj19lVIc31qDMawXKxpT4WRF3l0iJchEY5wD7ZCqJs3Bk6JSCTIwaZwLP62vEMoyzktmly3CGKSIYmJW9u14g2fsgmw2CLxROCMwUXHOiZCP6bzHdh4hdFgDQiBVaFVpDWitQoGq6PDesFo1pNqhtaAzgtW6QYiWRAuKLKNeNqFaPWLSzvuYu9nAus6DN7FIz7aUZQnSgPZcLE45Ph6jtUdrj5KG8TilbitOLp4hhGRv74im6vDOsLAXLOYtUioaEwrvcl0gnKQcTaibGmctrbWsZzU6SfDeBW12NKvK0toUkTgkFmWJhU6KvMh5+e5NFvM5xlienJzQucgM/CnowPbx2TDu+AE7BfDWbbXE6lNzG6/OW4f0MC5KjvZH3DwuuXakSEcKJwWnl0sePn7I/YdPOJ2tcTLFbe1+QDQiEXuOGNhVnHYb6902IBCMR9t2KGUHapMQIbrwNqojXvlMOLZEu1zUnd/ySnerOt2uMKXWWGsghmXb32kI17ynadtQeAOb/xV/3x7nvj7AO4v0DukdMbAONb7hQyBl3FQlXiXIJMfNF6HCVYDynkx3TKcjbtw44tbta9w7nFAkCp1IoItMosjE2YJcghfqXuiNbNO+4JOw2IuO7UIPj6BqLRenz7n/4DFpMcZ5wQ/e/hHLpmK2almsGqQqkVLR1R2hyXPftUpFOVoXv7bEipT7p3PO6o+Zfe0HLJZr5pcrzs5Omc/nrNdr6rplPN4jTTO0PiQtFVmfCwCscTSLFSpJmM9PMZdLOiuHjlxHR9f4xV/6Vd5++20ePnyIVJJWGgaekIfGO6at4fMy5SVjUc7SIml9RuszGilo5YYTLfKCZ6rgvHGYpKDUjlF+gJDQOYtratJkRV5auq7CeodpNFKHXgRhHnSkeYOQklGR4FxOs7bUdYfpWkDh1VZiPnrMRHabNRZDoLX0G7MUgsbaIFHhPMY2pFmIho0xNI2l6yRgUdrTtgayMBeb2nFxsWJ/WpJnJUXeMZdt9H5V2LjkBgB4EUwtpaRpWlZ1x/d/+DZPHj0MsMzrr/MHf/APWa0Xoc4DQ9M1OCShi6fFWYeSOWU+ZXp8PNSW9HZrXVVD4na7G9RqtYrwjKFpw8bQN1gPc1iSpJrXX3+FV+/d5r133+P8/HwQDfx5vHb4jBh3/JVOS27DOOkXRG/zpJSMyoSb1w65c/MaiXBoaRCZ4+nZjJOnpywWNRcLQ9M5vCiCJo0LVMm+zF9sabj4K8ZyG5/u/+5x5v5vY3YbV0M0zNbjbWAbOGN3ztcnCntcOc2SQcys67pgtLfhECGQWsaNJMh72j4MFpsWaf3Y9ca97jqapiFJkiCPYC1d7BfZNIHW13djcs6BM+AMwhmUt+ANSkXtjZAkABXCQ4kh1eCrFdYbiiJhOsp57d517ty5wdHxlCSVZCo0I3Y+wGjC7yZyYTMO29e7nTDqr6vPE/SvWWuHpFT/+4tyEMY6FuuWP/7aN3n45DnGC2arirPZJXtHh8FrvmxIkkBjbJo1UqahVkCqKL4VRbd0SMhdzhz/2//5dZwVtK2jqTt8F+ozvHe0TYtEslhWIGqsMTRtg4nyGj0EJ3rvsm0RSjG7XLJaVlyczzk7m7FctTx8+JCLy8uQgCwlPToo8CTOI73kx0bxoANjWzpv8Soyx5xEdRu81gnJMi14nisq5SiJBWQxj5OmGq1rijKoKHqnsD5ILwgC3JTknslehrOaLE1ZLZfk+yPs2YymbvAolM6YTCZDQloKPaytzTwV8RHkuonVv4Hl1dK27daaDMlbKX1fKsDaWJLW451h5BSVEiSJZDo9wDnF+ekSa3tdpLAy8WJw7vrx314zST7m3sERmfL86J33eXRyjkxKrt+akiSCvZEmzzNOLxdcLCqkVggPddWxXrc8PnnMfDbDe896vSZAX92wdnt7tlqt6LoubiprEAqlPToRNK1BKIFSntE45+bNY5SCg4MJjx5/hHMdgmQLgv304zNh3Ku65u2339480d/FeEilEBHHHo/HXDucsJcLHrdrumqNc45l03J+PgsaycZjpcYLiUrSCJhus0MIodpPSRr27+sfQV1ODwZm29Pceb9zCNwA/VxtXNC/r2/3JZWi6SxCiEHwScVZZ4wJ0YbbaIbIwQ3ZfF/vfczGB9GoVOvB2e93+N7o9UUPVVXvfP/QYSmo7iWJIs81h5M8yiekeCHovOD56WlY1OsKWWdcP9rn7s3r3L5xzN0bBxR5gk4ECIsQhhClCPAyaNhsjff2/++9uH4z7aOLq4ns7eiqvx9pmg6PjccdHIInT5/x9T/7DifPL5mvGkSS0SlJpyS1dxzcvMkoD4ms1naMJ2OkKCjyETrRCCHJs4JEZ8itzc60hq5pUEIwzj1etcM4t02L9oFtL6TDaskiHWO92iTqhaPrKrq6orNQLZecnS8280nA93/4wwFjDWqVu85E4SVKpRiRYIQOHe+cBdnhpQGR4CnoJ7mkIxUNNi1wqtduCjBdphPyRDMZ1ezv71MWR1gDTiRU1XlUmhQ0a4NKM0blmDTNyIsE4RXjSRmZYRqVjSjLcjDM0sWeYn30JTRSyZgPCni4sw4nY9cxL3c2doEAr/DegY/y115gOoftBE3tWViDcy1pJjk63sd7weXFCmMCzKOkIk2KIEpo7cBA6uecThLefud9skTRrOZUyzn7+4c43zGepPzKv/yL/PIv/SI3rl/n7/2P/5B/8f98jXI8jg4SgX4tNW1nqOtQ8TselygVIu1B78f7QaGzqio6E2owkkQwmWbUtaBtwnVPJsVQyb5/MGE8KTi/uITYx9laS9M0n7Av28dnwribznD2/Cz8ETHu7Y0peO6Be332/JQHH8J3nQBrgmSuVJieC91/JmzWQRZYKazZEtoChIz9V8VGqVCIgI+HRGHkzNOzdsSO5x4MUU+BC18ykS5ooMhQGDTdy9ibFsTmc0DA98/PzrHG4rwfhJb66/QuGPuev+0FTCZjptNpKKyQEqV09KYCtDTZ24vGJYS2Skq6GIdKIYLoGAQlOisRQiGC1iTaOzJpGeeKyWTMweE+1/cL9ktFkeehWbaHdWt56VrB8bVjnj17zhsvH3L7xnWODqdkSqF9SG7L0JoKjwodsoicfhHK+HvdGr+1SW04/BvDHZ8YkBw8dC4SRUVQWhQIVJKgkgQRvXclA0tlXbe8e/8Bs+UKlaZM9vdoPQiVQ6qp2oYsSyFLqJuaokjJy0OUGpHno82GIxXeBu9OIIJ2eGuRLlRO101FKi1JolFaslcmZDIJyUQEITDKadGB5ogDbxAmaBMhVYjeTEyY+Y18xm4uIV54PBZoROcQrIOqZxzz0DQ+qB0iV5tFJBxCWHQV+gs0GLxyoeTPCxIlaacpoz3L7duHOF8FRo5WrFZLvE1I8xJrQi9hSUKRl3ing25LliFVymLd0dQVztqQSxugpECAQIBUGqwdXgu9DETcyDaQXbh+ERPrvZMiiMgOXRdyNdW6QuJJW0XddKyrBo9Hp0ksNlRkuUa0HlfbIJ6nA2W661q0aVksF3RNjfCWLNFcLpZoLUiyoBN1dLjPtWvXGE+mpHmBF5I0z8FD07TsjUvq8wvWqyV5kWOtCYykpqYocrI0RB9dW2OswXQGQYoxNaaF0ohIubWkWUJnVvzox99BeEvbtFRVg84kTd3GzU6gk/8fVKgCW9LqHid3MaXQLs4PiUXnJdZLvA+Gx1uPv9JQsE8IhglhsFtFUOGcoVvS5onB7hPIJyqU/g+er0dtFdUYsZX4i6yYDouWkkQlNN5Sr9aEDJMND+fBwunT5yyXy7jY++bEYQBUsokUkiQhK0KfTmMCLSorcrIsG8qRd68pRBZJGih1dV1H7n/83iLo2yidooQkF56JNLxyfcKrL9/g+No+Wa5JlA+sHe8JEryAcOSJ4miagsm5c2PCeDwevG1vusiW0QihB+/L+yDX6nzkUPhef14MUgu9l7YDq3iGeyro9fJDe0KUpPMO4XyodozYrrcOLx3LuuX+wyd86533WNUVq7aiMWEOGFeQpWO0KqCRZOWUw1TRtTYkDX1gMRhjsaYD25EliuViAcDZ2RmzGH7HacNekXJwsIfSCutaOoIEsXdBIykRKwRgNLQ2CFeFyNQhlESnWdCJNya07rMmXvsWG0i8IHkmoG/ewDY/mrjR7mjcgPcyZlQcitCOUUiJ0posLThf1SzfO0MmBQfTHNvM2StThKk5P3U0qyp0FDIt3ngECXmWYZUl0Yp11YBrcbFHgRKxSlkG+V0I1FJrw712NjYTiZXO3nt0ojFuqwctDk8LCKyN/RZicVLdGHzth8g2bVKSRCNVEguNJQiHSiX5KHRpk1ZiakPbeJQCqTxtu0J4he0Cg8xpRWsacAnSSnIZJMWlSlAqoxiNMNbQNh1ZlJter1c40zIe5VEny5MXCYkSSOVJE8Go1Ez3jtBaM5vNWC4alOhIJjmCDplKvLCkqcez5sbtw1AFvI7FXW2FTBzWKCCjV8r9acdnxrj3x3ZI1h9CbnVzgdgoWW4wY7Y7Z4Zj17gHzHHbFAaN5E1iJ5x/i0JHEMoavoMAwe73cm6D4YFAWgW2RbiOVCd0r9sQQgznUATuqhhCtFSnCBFL50VYw1qrQfRLJXqAXpxzAwacDRrmfMLbUcrQtgG7dM4Go761WSrlSRPPdJzw5iu3+NJbdznYH5GkgfPsbBskUYRARFles1VI1RcwDQlaH/IMbN0P2NAtgSD45e1wz8QQO3zSuPcL3ZggB9Xz9V1PCQpnxHiPsyCToHndtC1vv/MTfvjOu8xWNfP5iq7uWKxqGufxImG+XJFmGVJIrG1p6ueEwpYQIjcNLNc1+LBgBZ5bN2/w4MGDAKVtcY3jRGG+WrJuKrI0QSeaPMnC/5AqGjmPFoF9ZduappMIWQz3TCuNKtTAgQ4aNW5nbLaPTxTW9dThnTep3YyQCD1x+//pCRFSluWMRgFCoFmxXp9xdrricHqLNLFI6bl9+zbCr/jowVPu3brGZK/g8mLBxfk5WRaSwCHabGOz6RZrDc75cG2qr5SN+RchBqYWliEXY6Ox3867DPNla/5Kz1DYs82Watvwf5WSwRmMa6xtW+bzNWUxxhmFaetI91SUZUpVrfCdYNmu8B7a1iBti1cOY/L4XXWItLtA/0SGa+illq21ZFkWZSg0WZbiMSRJGsghCjrTkmbl0KR7f78NWlpFGRy4zvL05CmHR2OU9iRaUuYjzrF0tWFvnOBcwnLRUVcW73cd1qvHZ864X00UhBsdjG+PIbsYtg6THHYSkeFEu38a3GAXNosjaGVsCobc8FEBtLRb38cPxn0Dz4RXegVHZUu0aMiTFTev3aDMNGGILVoG76hpK/I8p21blErwKtmcQ/Rw0cZY99/XWhuLbkKS5ujoaMCd+/L3fnx26YDhivoEJIDzDcdHx/xLv/Aar9zY53BSkCoQPpAorQC7dZ4wPnb4Hn2RziCZYG2Es9RQZNWr1+0symgYh7qBK2Siq0VRXoUCL1RkTJnQ3g3vqZuW04sLPGJYVB8/eMB3vv99ZssKK3Wg6OmUcpKjneTBo2esFhJj6kg1K1jP1yFXkSYkWqHzBBdpuIu2QicJTdNsEpNXrgk8Xko652mrDqoO3GqniConJL47IracFSRZmM/9xmWtGMbU+yRGPFv/5UqUts3sAnB9sV9/OIbIKA4ofV9RHQusiH0HhNI4IM8mtHXD5XnL5X6LoibNapJUkaYCa1uEDN2V0izc59lsNtzb+WxBku1FyC1cn7Gh1WVfzLM9DzZzyw9dkHzshtXnmPqCsj6p3ueztsdgmxIbErdBciNJEg4ODnDW8OzkhNWiQ4gE23ksLW1ncS4BXJAqUPH7RfLAQHxIUlSMNJq2wTnL0dE12qphsVgMydL+ugKkGoy5EBLnFM5ozk9XnJ2ugOfhXmvL3t6I8XgUbJLtOD4o2ZsktO0a7w1FlnLjeEyuBctVDeSYfcmjR2esFhWfdnw2jHufQH1BBngbKtm8mYgtbvQ92CpU6YuJthkWXdf3fey9gpDs25juHv8LZxAidB/q8fWAcYf39l52b1z74qFclExGHb/5m5/jl774Re4/8DgfqGQnJ894+vQZaV4MnrcP7tPO9SqlgwcfL1jJXq89HG0bJF2dc0wmE4qiGAxmj3G2bRtgn2iItZbD4lBKkWh4682XuHvrgFIZpG+RJAgfMD/HNiwWAe8XJZDje7I0JVX5IHfgt5JHgy4PocPR4PU66DOsfQ/PbePuvOPJo2c8ePiQw6MjXnv1VaQJuuudc3z4wQd883vfw3rPwcEBo/GIjx884HIxR2clnfUx6lGhSw9BnS9JJev1nCQBZyWXZ5cY06Ek5GVKUUzJ85Iyzzg+mJIVI5Is5+bNmzRNg3NBQ32xWITkmXNB4TPioES+v7AMc7NzTaBAKoHQisJ7NsINfMIR6cf30/6+8mLgo29Bj1JrlEh23zc0dw8/vdTYHggHpMwoiylNPWcxdxztF2jVBb18DDduHpNlCiEN40nByZMFk8l4KLFv2gZLPUQNwRkwEXrR8V5vojKIG5Db1W/qGUXbBn0jJRBK6vr5Z63d2UgDrTroBXnvWS6XlEVBmhTUtcF0wZDjO6R0tC0kiSAvFfuH+ygp0TphtV6jvKLIi5AAdg5jLFmakSQpTV0jRdh80jQly7LIkiEmTVOsMyiVUq9blnVL28biSGOxrkVKw/zSspgHIkZbzQOHXxxTlBlJGuJVUo0WBXvjDJ2WFNmUySjn6ZMzPr6/ERK7enwmjLsSnpyG1mVYKQMeDsPNFDYITwlhEELRWY1xPY2p99yjBxMNvkNQuBTpW6z0tMbhhUZKj6QBL5EyQStNluehMbcPnea1liSpxLkgpdt1HWenpyRSUpTlMOmE3vKwhWB/74D9vY5XX3uVe3fv8eHDRzx6ckFTV9z/6CPquuP23Sl37r4aeOgx+Vm3hqoJgk9KdkgZcwvWkWhNlgcIpm1bijwLyTsB1XxGs16hk/Cccx4nYNl2rE1LqiUqAS2CIJIUBqEabkwz7h1M0M7glccLhxeWQb8XBgMdvBG38caciw+LEqH5cqITtoEB32+q4pONsDdsJfo9I4yzCJojsTaLxbriez96l3ffe4+9vRHFeMzhdIzzjvc/+pivf+fbPD+bIYSkaS3dw8cIJSlHJSIJnWzWK4MEzi9WLOYdxirmlzXOG0ZFQZEl3HnpFVarFefnZ1zMKmYLw7XriodPnjKdTrn10nW+/e3vDe0Pe3rkZG+fvamI9NKaum5oGxMhEh/tZ0j4IaNUsXOBAGA7pItVvUrjCKX1wwAGPeGfsWqiUR6qeOWOc+QB66Nuf2/Albzy+Z0bg1SaJCtwpuVyVpGkGYUySCPJiozrpcK4GoliVOYcHZUcH+/juo6ndQPeYpwh0RrTGIx3OBsqkK13EdvsmU+BOiqiI2G9w29dQ+81B6NqNiwoKdEiJo6dJyh+7upOBdlrcMYzv5xTrWqs6cfJhVybFzgvaVpHZ6DXD9rb2wvFY96BCLahKMogrWBt6OJkBY8fP+fwYD/mTCTGtVTNOmx0TZAqdkiEV6xqQ9carN04m84FMkRnLev1HKkEko4ih3zekhYTVrMFUjuKfBRorsLTrC8xdc10olHsfeoM+UwY9+OjPb78hQN+8O6cpRGh5Z4P1L0izxFt4GC/9dbL7I2n/Mk3fkRlfZzAfpinwVA4YpExiXfcPPDI3PDoXLNsZeCRihYpFOVowptvfY6/+Jf+Eqt1xY/ffhfvNd4bFotTvOkYjUqm+wcURcmkHDGZTAYIxQpP2zbBiHkP2uPp+OqffcQ3/vTHPD01zOtswL/H41HYiFVKngQVM0nAv8uxIssLDqc5QkS82YWKxjTL8N5T1xXjUc7tWzfouo7vfe/71E0b2S/RU0JgPKgkQauA9eatzaJaAAAgAElEQVSiI9cwLjSj60ccjcaMVSha8lLhpMcJNxiB3kHf1rfGu52HIHhhcshXbBUPeR83WYZk8ycNfL8Z+OER2EvghGDV1jx5fsH5fEVtWr76jW/wla/8MpezGX/yzW9wtlyFlskGVuuWtqsRIgkshrJjNCmYP1nz5NEZz04uSfQIfELbBh0TtS9D6C5TmvmS1gkMmlSneKmpWkPWWdZVy3y5GvIMARbww99KKRKl0CrF6RjFZBuD03Vd0AtvO5RzCC+QNvaORSJkgsAi1BZG5Tf34MXHxtumH0MhdwrgYJPw71lX4X2xSUz8u78HEfhEJUloUL1YsXc4otAl+Tjn4vlTrl8rEXIcGm0bR5lr9vcU7bricD9jXY+ZtyCUx9rgQAVml8falr4fr1IKlUTnyYMgUiOjwRPyk13WdoTZ+gikh/eu7FNSqpj/CU5iU3ebN4lQIN1vJMFnCVG8c5q6dmjtaLsOnSYhihAqwiuO5XLNbLaiqluWq9BuMMk0VbVGKGIjGWi6lrqxVOsLqsqEHKHs14qMdTah/3HXOVKhUUmBw1J3ktOLCikCy+/SLVmuVuxNRpRa0tkVRZlxcPRpc+QzYtxHo4x/49/8Fe4//SqLs46u7YKX4y2TcU6mHKMs4bf/7X+LL771Ft7/ff7vP/0unVPEfj7xTBGDQ5ACZdbw7/z2X+SVl8b83v/wVT48aTEixSEYjUveeOtNXn3jDfLJlA9PzujSEiE0xrSsrSS18PjxM9r2IV3XkaYpZVkyHo8pipxrN65xebng5OQkbipBRVB6S6pgvXKs15ayLLl+/TrXjg/RwtPVq4j5B+rgYt3wwYPHLJZrijynyAvSNEHrJMihOocUMvY17difThBC8OzZM46vHXL77q0N3CFAO0vqHamHQsH1seLO9UNuXT9kOilpGoOxXTC8QtA3pwie0ouhl+2jZxDthMPRGr1IzGs78ffpx6C3iPeexXrFbLmgsRrx5DHmaw2dCX0+nZBczGfU64Zbt25gfNhodKKwSKxLeXZ2xkeP5khZYEyCtwIlE7IsR6U5H370EGNChWLPuMiyjNVqNXzn56fPd+R8+43KORdhmSBC07/edC26DhTUHn7L0hSfhw17oLgOiUWFt4LtUmTvQxXrp47UVl4ljNtus+orw9p/aOezO/coXlOiFUVR0qyXVHVFtoJmYkkSxcXFnNu3bwWminNIlYTGFzphspdwS4xIZhXz+Rp8i1YpisC4UhrSTJPnI5arNd4HKrDwGucC19vZQEzopZ6v5hmGokIvdryFno68U4EuNjpUwJDQDc6K2Nj66JS0TejsVa0NStmt3g1b4x3h0iRJkI2McGKCjlBTv9kHsoPEGkNVGbo25PdUEhrYbLSTwkMpj/cSZx2dFyxmLYtZC15gDTR1g9IaU0M3hus3D1jVz7lz99Ot+2fCuAtEaK3nEkSQvsNaz3rVsF49RnZrJnkNtmJSOP7d3/5NZOL5469/n+Xa0iuhBI8ktuxwsOwMf/S1P6e+vMtB4niuOuZOko3HvPm5tyhGBSLR5NMpz88vOT2/5NVX32QyKWmrJWcf32e1XAfPu+sQuuZysUKdngOgf/IebdvStW1kowW9DOEtwjucDboZt2/fHpTgRJ6xSQ5DKpOIAcLF5YxTs6CfeUL0WKQfWuIpGTzcXkDs7HJGMco5Pj4GQHpH6msS7TjcG/HaS7f43L09bh0fUaYa6T3PZgtW6yA5K5VHXtVz95vkdSic2k1eDT0dY1FVv3Bg12D0VYkhIHix6dnmuPvo7TvvePb8GZfLBfk44OR11/Lg8Qmdsaxby2xVodMCu7Ys646iyEC1dNR4c8CzE+jaO7z6udcQQtHUHetVhbQVr7x6l4ePPmK+rhEu0PV6qK0oCmazGUII8qJgFbpJ74zJNoNDyqA26IjMFudJrSPPsoFd0zbtjuEZWF4+NGQxrtsZNwF4ucuEeNEmu5NX6jt17Zxl+3MvNuw7541zQCeaLMtYLhYcHd6gWs85mJYs547LyyUCKPJ9EpXTtkH4q6obkkRx6/oBWnjq5RKtPKat2ZuUvPTyTdIsNLqez1OePHmKsx7jJc4KtE4wxKItYpMX2DLIG1lljd6peO294T7S7D+3qcLe5OD613rF1wF6xFDX6/A+CVkeouZtqKvXhy/LkroNFeBlWTKfnyFEUPXsGU9d17FYVHSdwzsZCit9YANJFWyeViI0yoEwFk6Q5gXWKC4vZiEaNAHCSVPNGsFqteLp6SmoJUm2kWd+0fGZMO4gyPMCRI31KzyhiMhDCIFFwH1tu0TYFa/fO+A/+Z2/hnCO/+uPvgte0UUjFJgYHisVrdjjBw8qTh4+5u5Ucvd4j/cvO67duk0xKgddjcvlMiyQtkN2hqcffcwHP3mXZjbfYH7e0zW91GYIyV3b0htq78FGqVPlOqTvIIbdT58+5fGTJ1v4Z5wz3qMFIJMIC0isj0mwaOiENEOqOE1TrBMbgy8soqr51p9/i6/82lc4OjrEmZokMexNC9545S5vvfYye1mQCZDeIxykiaIK7ZAIXvuLmRi7d4hBZTJ4Om1sGaa2jBUxughHLytsnWUyHg1NQ/rzwUZnR0aRNIfn9PKCBw8ekI8KWtdxOb9k/2CCF5CXY2pfI5TFeQU64eNHTzg83Gc0Lqlb6EzD6TncvP0lZFGyrmoae07rW4RxVE3HbLECqaOKFIMx6aWXtU5Ik4TLi9kLZ+xWfDO025M6eJa5Vlw7Ohrooj4Pi7DrOqqqCvoogNIarUJPW4SOUEKv5y93oJnde9Ibqy0Dz44d+sS33TH7Yvfn5n5s3lAUOXXb0LUK70KJvJSaumpJk5Rq3aBUgP6qKhj8NFU0bcvx4RSFZLWskGjG44zxSDEap5xfzNifpqwWKV3nsD5nuQbvApQSWG27UUvPBgsNNwQ6ioz166A/BjlsIXbGpocNh2vu2TbxM1mWUGQBZ6/r5hPj1b/bRurzaDQKtRZdzfn5GVqH/7NarRiNRiRJwnIxp+ss1gT4DR8cPUSo2hUiiMB51xM0wuZ0ebHGOYHWBc6GGhWl+1aBDU5YVJIixB5v/+jFc7M/fqZxF0LcA/4BcCOOx+97739PCHEI/AHwCnAf+Ove+wsRRvH3gL8CrIHf8d5/69P/iSdNHVp3CNGgERwclnzxF77IN/70uwif8a/++pc4Oi4RrFFOsZeWvH73gG+MHJcV2KiV3Cc3LR7hDV2XsUozOm2ZFJKpSyjSDFNbDJ5qVTM7Pcc3LZmCdn3J4wcf4m1HY1YQKy0REkUQBlNSMhmNWDgTcL2YK1IRAPRS43zoDCVEVImP3t2OxyRCpaWP6Zzw6TixY+So8IgoVZopicUHBT8A71FCUa8sH73zPnd+RTGdOG4dH3H3zm2Ojw4pCkGqVAiDIwIjlQdv8FaB20AxvU6NjXkLQUx0iTABnfdR2wYSrSD2bhXEFnh4iEbeAA+ePuab3/kzltWSm9dv8PKdlxiPxxweHpKqBOklTV2zWq+Z7E1Q0rNqKr753e/y8OSEql1zen7KYrFgeniAk+G8TWNZL1bIxDA5LDm4PqJrBfcfrDg6fp0P7t9HSsvsg+9R24wmaurgLHu6wzQVXVUjnYwJvWBA8iwP+CsqbFrsauN7HxRKtyMaJzxIi5cCqTXWW2xref4swDlpmlKkBUWRIzMFQtI2C4RXJFKjpAANzkuk9aiYQnI98tDnLq7utz3S1f/cRSqGzap/VvQZbHzsoNV3OfNIFU2ZImxUUuPSgsYsmDcLbua3qZfP6Opz9sYTiqJkPl9gnUELRZ6n1N0apUSAYfBMRhmJFAjvOTicsJxdkCZTxtOExXKJxTI9mJIkGTxdsVpYcArnwIqtSnCpwlwRChWzCtIbguzy9u7n0FGmREb4JGjfhSIqE/NYWgXJAG8dSmucdeRaMhplcWOYh/oM1Y8boedAolhXLZfzSywN40nC7LLCe8iyEW1XobQhiQ6YJ8E6u9GwkW5wgEK6Q0TefC8xIdBColWOl5so2hKqiIXsNZo0dAJkSmc/Hef8eTx3A/zn3vtvCSEmwJ8LIf4Z8DvAP/fe/10hxN8B/g7wXwJ/GXgzPn4V+O/jz59++FAtVhYlgiUKxxuv3+Zv/s2/xqOPHjC7rPjKr/0q0+kIgUVHw/BX/vJvsvKKf/RPv4l1W+F9pGEIQsLGesesbim9QOkRRR6qPuumpa07lrMF+JAgbNua+eySpqnJ8gQpU5raYEy4MVIEEbL57HKzgMTOD0DieynePh9wxV3qk41hufnA7/UeIdzmTJGh0ifQ1usqMB5iJ3WlNInWTMcTusbx+NEjfuO3/zXuXDvg8PAApQRt10RPpk9wwiCH4P0mn8kWjrt1XT1MYr3DOse6rvHOkWVZmHD9LYweUfTtqdqWn3zwPh8/eUTrW85nF9z/6GPSNOPzX/g8n3/z89A53n//febzOV/68pdxeL72Z99gUa+pojzz4dEh165fw3tYrVqq2gXmUlKgs4z54pxylKPEiCwdcX66ZjXv0Npj7JrWEtqsiSDPkCUZq8UcE8WcVHQKvPNkaUZdtzE0VxjjBqivH59NPiFisAgEOfvTfY5uXMd4h64d1lhWyyWz+YLFckamF+RFFjw26THKU5QaUQSBL91ppAgbOT5KDPdkAe+H6dNPiytoy4vyilcXGdC3T/QblF70PQviVBXghUAlKZlMaVdr2lpQWcvB/gTlIc81goLTi1M6kYOziCx41aoL0eCyWiG8IMsLVtUaax3n53P2jjIm4wytCH1fC6jMAkMWJI1dgDTZ2nhSkYII6zMY97BG+iK4/riaI9qmKUsREruTyRglZWjIrRTrqkKp0NB7NCqD2J4gFNxtnTts7iayoyqkLgBPURRB8kOl7O8XgObycsl8tsb7Dawj5EY3qrfwcluC2/eRcbg/ff7MC7BDRzePsEGC40XNaK4eP9O4e++fAE/i7wshxNvAHeCvAv96fNvfB/6QYNz/KvAPfIgj/1QIsS+EuBXP8+L/AeR5zt17d/ng8QWua9k/HFOOEiZ7I2aXVVSKK+NAeYS37B9MODqeolOJ6tQQ4osYKvdYsXWWRW1wSNLxCJ2koRgoSnReXPZcZ8n5+Tl1XaOVoigmrFcNSZLifRf1uLex6V1J2m1cNsBDYuf9n0hQxoUEvaywZ4cCJwRe6GGSBaxQDhNDK41KElqvuVzV/Oidj3l+vuLNl+8BAutim7gtjZyrXPWt+7wpztrCPHtK4+ViznvvvUdeFOxNJnzhc5/HSRE3gMCOQYTsvxehmcLJ2fMoAavonKcRgXb5o/ffxQCHkwMenT7j8vKSW4uX0EnC45OnrNqazsdS9FjxZ61jsWjY21PcuHkd02VYrynKMY6WPJ1Qj+HBR0sSIVEIpHCBCusD19pbeP31L/D06cnOGPTXniQJq0UQ8OoLsa5CVH01cgj7PSDJ031u3niNtMjpvEMXQegqmxrKw4r5yQPWl2e0XY3GkYiCItXsqzG5Kll2krWv8d6FYiTvQ1S0tZn8tPs2POcF/spzVwudrn7u6us+hgj99adZSdPOmNcNWZkgEkWpJXmu2N8/pOpWpOMyyFyowAkXzSJg8omk7RxnZ884PDpEScX52SU62ePG7SllLsi0ZzROKfdSWmNwnUdKh7Ae77b57jrmmPrv/eIuWy9K4m/njrTWIXGeJozHOVVdI6XHuY62q8nzDCEEWZbTmSY2y9ji20d6sLGG2WxGkZWMRqOg6CpCdOSdxLRgTB8tbe7Z1ermcE2beh0fDfyGgCAG3zA85xF2t5Dy047/T5i7EOIV4JeBrwM3tgz2UwJsA8HwP9j62MP43I5xF0L8LvC7AHduHjAZT/j85z/Pt374IY1dcXg4YTxJQxecHicbDKHF0UXtFxehjF0DJnzEtWLxQZsqUi9I0hyEDIp1kc5m5itOT0/JlObJkyc0TcNolNO1dZzoMsoBbMrjdaJp24a+ki18L7dj3AVhYvbVa9uyws5FLGcrsWPNJ0uKvQwZdqKH1UsJKBk6IXk0rddkqmRRLfg//tmf8BfefIU01aHiTjiE0J+Y+FcX+qDtPniL4XlrLdY7nl2c8c4H7wc2QFSK/IUvfpEkDXigc471ek2eZxhref+j+5zPZ6zbFkOL9RDkpiwplu/8+Ifcu3WPk9k5y9WSP/zaH+ON43wxQ2Upi/WKJBkNzYEvLi6RKse6Fq1HnD9vePZ8ifE1e/sZt2/dZL2uApUvbgr9RUgVdNmtdyyXS05PT4eEqHEmYuxhjEwX8PAsy3YaEA+66KKnz21anR0dHZHG+WAFYVP1YEyHVJLx/gHedihTk7oueKZujTNZ6MjkDVQt1js673Cib1CzNQ9eYNx37+GL/fafZdi9j3LSInRIgo0SKkrj0TSXNcdpAuuGKu2Y7pcIabl2tE9nHHkWnK5RljM5lHSNxdQto1yTaMukDHOkWQi6dY1yY+7cOMQ7TZII7hzv0c3OqFuDJySZuy7IFwgvUQoUesjTWBHyEVeN5fZmta0k2ud0pIwNPJwBuqjRlLNYLBFCDOqq4/GIqvJ0tUFpRZKkCBH0n+o6yDiX4xJn3MCsStOc9bqlbRx1ZREkDPLioq8Q9zuSHFePfunt3CO/+4arm9anHT+3cRdCjIF/Avyn3vv5FWPhxXZPtJ/j8N7/PvD7AL/4xZe81pr96YSD6QSTpOxNRkjpyfOU0agIRtTGsCWAVoMcbvBQX3wpnugMqxSvFSpJYncWgRIJRVnSPX3O+dkpRZrRtU30/j3rqkKrWElK3/XIB5Eq38VxkVHLgk2QGD0gIQJVSoot7nh/w7biahEhg153evtwMnC3RbzuoOfYRwYQlAB14O9nE370zn2+/Z0f8Fu/9WvBsLOp4rXODRNkuFlX4vsB3/URjrGOzhpWdc26rRFCUHctX//zP+Nsds6rr7zK3t4e5+fnvPPOO7z+xht47/nW975D1bWgZOiqBzTeAjpEPMbyzvvvcXFxwXQ6pVkuaFdrdJaSj8qwKRhLlgVK6PHxMQ6J0h04HTRCmhHWt3TrhMcf1zTGsahWtM6ChLaxCB/oaV4IvLN0xiClIM8DxpolKXUVGhorJSNmGyQNvvzlL/OTn/yE+/fvI4SkKHK6zgxzMEkSsrwgKxTrek5nQ14jcUEUzceEn3GWLC+RjeMwSfmSTnlJKXy3or1Y82Hj+H4j8VmKSqNEgIAte74x0gMcdGWeRxjQD3Mv5HtCAVoPc2hetEilCPPXybCGXGw1mHnFnui412i+0KYkN2/wTJ5hbYNxnkme8eiDR2RpRlEWJJ2jMR0CT5mnNE3H0X5BqgEUdq9g//gI0ywZlzlZNkFrQe4SxMLz/GSOdYokL1mtVlEbyZHoIBMytI30G5rj5ue2DMG2Eeyj4q1G3hEGFjjyImcxn6OTnNVqTZ+cVjoUSikZ8i9Ka5I0YTwa4c5OcMbRVh3L1ZLJZMLscklnFOtVjRC97IiP4+mGHFs4/xWjvX0fr/zs//DDGo10YfFTTzEcP5dxF0IkBMP+P3nv/2l8+qSHW4QQt4Bn8flHwL2tj9+Nz/308yMQrWUva/hXfuV1EpXz5quvQ5fx1ut3ePPl2ywvZpydZty+sY90CucFJw/nXJwL/JXLkEKihMKYho4WnUCrEpKsYJJKrKswUiPLkso5ZmfPkGZJYxaYriERQSHOOQFKE2qqZDSTYiuE6vF0Aowm5Q75RAfZuZDI9D1dzm8tTonwwasJDSc8TnRs3zaBiEmzfmJvXhVCoI1npC37ewlClpw9y/lnX/8RX/7Kl7hxoMF4TGzOIURoMmKGqCEKNnmB9AJrQnJHeoV0NuilWMvbP/kJT0+eYnxoi7Y3KVk0K7799vd47+GHvPrKq+gk5aOTR6x9S1VXPJ4/BxW8Hp1OWM4XtHUDVkGuwnUJyWhvGjXtHZOjMUUxwjuFsCHEXa8C9j6bzxhPJ4ySnHXlmS2WCDlGeEnd1KzrGVlW0K1bXn/rDV567RXeeec9nnx0Qp5n3Lx5i7peYe0cR4NMZEgk6pbJQRYYEEJwu7gOQqNkxnsffEzdOq5dv4McorcEofXmfguPidNA6j6pr3DGDY1YRKYobIoTMGou+LXikK+MCo4ywcVowp90Dc+fnnBxeEAqBUYEw2CN2ao72HiiAEoM6Gx8NYjp9bCKRMf2kAbvQhGPFekQdfTZAmstQkb1xjjLZOyDm3hHkRf8eir5ZZ/w/mqKHYMvoXUWv+64trfHslqjJCxXc5I8pWkqWluxblcUIiNNQ0J/NNYIs2KUFzS+w7PAe8F0L6d45Yjb4ylVbbDK8+TJjC4JY9g6gVYSrRWL5To0xEHjnARUID3ZTVPzYOC3I2kIqcNgZLM04ebxIVlW8vzZGdoLcl0gSZkvl3jZ4ZQjlRonE3SaBHhROVy3IreWQ1mQTg4xxRFVVXHeLKmtI1Ea78C6FiX7qDZE/8ScweYeimHc44IOToHwW8XifTIrblZqI+3ws46fhy0jgL8HvO29/++2Xvpfgf8A+Lvx5/+y9fzfFkL8I0IidfZpeHt/SO8ZZRnX9/cx1pIoj20rtHJxXtuh4gwh8c7z8ccP+JM//hrbEewQUWxfu/ChTDhJEVJgTIeRgqO9Peq25fT0FGsMSapJtMJZSdd2Wxi12HpswiGldxM67kqC1TkfW35t/u6PjfMeQmHfN59keGHrOvzW77uvCel4+ZUb/Gf/xe+S5fBP/vE/5qt/9M/53g/f5Td//S8gt7y83nMJSaONp+6cw2xx1cPzgXHUWstsseB8donOMpwztM7Qtyis2o4PHzygaVqaruXhk0fUTYBHfFTi1CplbzJlNrsMC9B5Li9neOspo6Lg7HKB8DlZOsIax8nTZ1gS0jR09rl75y7rdo3SgGzp3AyExjsVhb06kiQbmpa8/MorHB1f5/Gdx1xeXKK1ZjIpWcwtN2/eRieaUTkiSVRg/hA2vsZYOhN0Z7yTTCZ7A3c5TD05sBtcbxjjuGqpwEvaphvC70DhkzRthcSiTUfWLDi6PmF6a5/i5k0+v6q5J2Hvxh2k7Vgbj+vMoDYIYMyWLk90MUyvUIgHFzpo9Z6dFx2diEqdPsNZhTFVgPbkpuW5iAVAaZIOEOGmBZ5llGdMjeOas5y0ISKYL+bkRU6SZ5RZjtAqrFHnSKSizHOE9ygRSBKjUYlAsFwuyJJgcvIsx2twrsH5NZPRmMxKlosQra5yycGtmyilWcxqRqMR5WjEyfPnPHg+o3MdrY/ixULihdnIfscK8j4nElaNjLCHINGaw+mILM2Qbo9Uw2zVcefWdeoP15i6xSmFVMHICh82PwlMypK3XnsdaRWpzBBx3hx3LfPYAW21WgVpCZfQdl1oRuIdRoERDFIIKqCt+H5XFT5o88MQYfeRfk8x9uLncNnj8fN47r8B/A3g+0KI78Tn/iuCUf+fhRD/EfAR8Nfja/87gQb5HoEK+R/+7H8RmO2TvOClG7dobMUol1SLMz73xr3Qkd5rDo9GEMUFmqajKEpefvllHl28D1doQT5CL8IJ9vf3IS56iF2OYk/D+SIUKamIvzd1WJSdMYPx7kPdnVhYfBLHHPaVLexvN3u/iw2CQ/RNQFxs+iF3z7lhy/QjFYs8+my6t9y6dcwvfOlzJKnlb/3t/5h6veRf/OG3+MIbb3DrqERih++yrejYY5Pn5+eUZRkaBRO2m84anp2f8vTsOSfnp7TWILOETKVY71kt1qRJim0MlhA+d85TVwvyMiXowEuqquXy4oRxWeJ9MFghsWVZL2tWq4YbN65TFmPmsxlSFBwfXmc8nrKuu0H6uG1b5rMlSTalLEtu384osld4+PCEuoE0HSGEYn8/46WXXqJrO+bzGWdnJ1xezoY2hW0d2pxprVnMK1IdIrKBDhr1wL1TCHTgu8cuT13XYawdNkRrgzo6NiUvYlWxcoyLDRZc1zXGd+hxRp5qxs0FN8cZeSHojsZc5pa9pCTTglRBIlVorZgonEsHY266MO+ddTRNw/NnT6nrup+KDLIQIhjmUVZy43DEm28ccvL8Oe/ef0bbpkPuwPmg/KmToFleFMnAfgolBx60D81Pli2pl0xUzsn5KfWo4l5WINME13QUZRBSS4Qi0xrlYXxQsFwsMW2LazoODw/JkxSExXiDU5Jltcb7FiTkSlKmktF+aApeiGOKPMcazxiD1pZU1Yxv7DGZ5JxeLlhUDfN1AyiyRNC1ce16h42RlXchoS9jb1yEoFq3PHvyjNu3bzEuC4o8g9MLblw/wFvD7HKBEYATpCokzzUC5aBMMlZtjRLgXVCNTROJkIpRrvHjHDMdxZxAQdsF4kbTtrg0UDX7VpdYhqrloUGNAK00nekGLn+f+2maNkib+5iT+RlG/udhy/wxu2Zt+/itF7zfA3/rZ513+wiJk9Bo+aAcY7XGS4Np59y8todFgE+ZTKJOsw0Lazqd8sqrr/DV73ywcz5BqECTaIpJwd27dzl5forWmqZpaNuOdJSTpimz2ZOQGIuqhNaFwgN84LSHJJOkL2v4pEHvPWA/4KHbSY8r+ZArYxUbVwgiZ1dEvG7rSiI744XZcR/YKT/+8dt8+OEHvPb6bSbTCf/ev/83+G//6/+G7//gPge/8QUm6W5/0e3Eb9uGXp1pmnLz5k3Oz8/RacL+0SHf+t53+ejxw8DFTxRCKjrjsc7jZcKibtBKcz5fcu3aNdKRwicWpR1lMWK+qMAnFHkwKKvVkuvXr/+/1L1pj2RZet/3O8tdY82lKrOquqvXYfcMx+MZCpJJU6ZpWIIE+Gv4jaAPoC/gr2D7jeF3tgEBMuQFsCERoEmJMocz5JDdnOnpZXqp6srKyiUiY7nr2fzi3IjM6uY0Ddgw2gddXZWZcSPjnnvvc57zPP+F1WrFqByT6Qmr1esLn/AAACAASURBVIrlYsN0OsY5wc1yzfHhQ7K0pOk3eO8HOzHBemVwruZg/pB33vpN1hvH8/NLfvDODzg7e8bieoFOcy4uLji/vsRay2ick6SSvh9YoPaYpmluG6IS+r6j6/r9vdP3PV1r6buIYNoR2ZxzWOdj9GNo6CHIxnNeeWVM8BIhJNYZ2qbZB9+i1OhcoxPPcVlSKoksx/DuO6THE4ovLhllCb+6eM5rpydxB7IDDwyLiRtgmd45epPy+ME9rq6uKMsyMiL7Gj9IShR5zu/+9g/5D35wxKMHPS+uL/iTn17yz//Hz+i6KFjnCYg8QycJWZ6jlYqBiJj4gsAL6IVjKx1NAiQK4RQ3qxUTNMevPmY+n9HWDTeLJVmSEIyjrxvy2ZQyy2mdJ1OarqqRSqHzSDKqjaWuW7SUFFmOkopRqshkCkhKVaCVxvqAkj0h+MHrNmE8n3HvsOTJi0tYdCA0Uira1lOWJbqc8PnTF5guGvsQZCyrDs+fCYIXlw3L1RPG45Lje0eMijE4wysPDjk5LPFSYK1gPsvRysVgbhxhgNUqJemNoTNxdySA0LtoCUpc/GNZSSCHhVsOrhNFPvQ2rEeK6OcwKksQgd72g7rrrlQbkwqlVLx2Il6n3cn8+fuffT0uDONbwVCNhg9DjRoJMiIGdmvKjigzsN3x2OjIozOEzvAiVsOl3ClsxJ2AkJI333idcTkiHMN2uyXLMpp+SxhF0kO1XuO9I1UK6zzeR9cjqSLhxPnogMRAJBJ31rnbwsZQO9tjyW9FtHYZ4W0T5VakKctS8jzdKw4CBKGGLnk8LirJxU59GKCS+y76UNa5vLrkv/ov/2v+yT/9z3nzjTc5Pj6hGB/yk5/9gh/+4E1m9yfxfILHuuji3ruedtvxl+/9FS/Oz9EqofjkY+q64fTRA5LLc569OGe12YJWpCKnLHLaruZyseB4dp9ESpaLm4H4dEU5yuj6DZNZTpZDmee4RNHUPeNxwf37cwCUFqSZJkszmqZhtVphTI+3Hc621HVLCDAeTVitV3S9Y35wn/F4zOmDRzRV4P33n1I1FWmRcL28wTiPDWD7nqZumR7OydOcYAxKKMp8FNmhNpaUnLP0vaGutjG7tjZ+z/XRNcrHoLCTfWBXq96hmhBIqQhCMh58MaVU1HWNErH05GzU8Ol6Q57GoCatjRDP6ZT07bcwqWXz8RfMpiNeKyf4psHZbm+lGMstYWDShkgSc5bgDBLH088/HVQTO0KwpDphNJ7wlz97n4/e2/CdNyc8ebbk3/30jLOLFiUlqUpwMpCFCVKOmE0OWK03cQGUEu+jtIXQAYOnm48Yvf0mSbumXKVM9RjnLOcvnsP8iOAcPjikiLLRWgnW6zUMaDFrPdtqTQiB0TwDLRFSo2WCkimBKOjmkhARXs4ySiXgUdJTjBVN0yC0IC0y8jQlyQQ2TMkKSW8jYbCuFVmak05GtO2cp19e7+vW++cwRBKXI6M10N40tHbBwcEE6wblVGkQQqKVQNCSJpExbK3BOxvZ2b7FcQuV1UJTZMWAerN4D53tUFoNzlQC4aIccZJE9Fyi5QCOAG87rO+RSuAZgA9OoHUOA+mwzDVSakwivlYV+JvGtyK46yQhGaVDlmxRUsYGERqCQuBQKsKifEhwsuPFZsUf/tsnvP/JBSHRBOc5Pp5TJBLT1qxv1qRJyjtvvs6Tp0+h7Uh9oBCCTW9IgNA1dOsVaaogCLabBqVySAXshLpkJJQIIYigg7uTOpBBhnK8Y/CfGBolIh6+h2bu3dhFXA6ODg9J04T1er13dBHCD4vZUNcX0VghSW7r/s4HjI21vCAcDs9P/+Kv+Wf/7L/g3XfeIQT47PkVhe756FcXPLx3FCuPIrDarrhYXHJxfcm2bfhi8RwrLYmEm02HlAlmeYVdGGRRMFMpfWfo6pqNWVPMC06ODmI9dTan3tbUVY83EtPCbHpK39TUpmY2n6LLDJ0K6mqLVJK6rmmaFUJ4Ml2S5XCoZygpwUmmkznT6ZSLF1e0dUuajiimDxhNX8MlglUHy82Sy25J0zaE2qNXFfPZjPn9U8q8YDabDc0EgbOSF2cLmqamqrYY28RmYZKQ5znj8Qhre5wLzGZTqraj7aKDlbW7Ps+ubhuXZRkCyAS0JklzylFGb5oBZudZrRv6rmE2GzFOU9JSk6qAbzoORhNSLWhxqL5mWgjCVAOW8cGEje3ZrrfMx5P9tnynfb5jYN8sr/npjz9kPptTlAVaCTonaFdbhArU1ZqfP/0c77b8+P80dBa6vkCIDGcNtpKkBxn/4Hd/j3unD3FC84d/9CekSQYi+tCGEAhtTao6xOkhsx9+j/nP/4p7V4KNUiSzgq7uaF3LKMs5OJojCZi2Hbw+Ddu6J02SKEedpNRVw/L5NbODMUVZ4loPmSOUmt55VsGihWQySO5aY8nzlGyc0RcF202N9gGpDHkOqTYcz1KqpkVIzcFohNY5ve85madcXwzeKUCwbpAHH3RohBzQTIJ1VeNCT5qfIJMMrxKcDShhca4iuI7I8DZ4ok2iEBLpPX0fs2oUtIM9oNYJiU4I1hBMTMi8D1gpEDrB+ICSAi2HUq+IvblA3DnpRA+6TVHN0xPIsmjFKISPlpbsrAu/Ia7+PwvL/+8MIQM6sfHUwo4kogZY4F3Fx9gdt07y53/5c37+4QWkB7zy2mucP32B1prHrz7CmY6PfvFLDo6OKEcjrhbX2D6qOp49f856s+Ho9BGiNXz37e/wiw87VsslIQSSJELiQIAUyOwWnx6tw+7Ym31lyH3zcwdbYyjV35KCdkNrTdu23NwsqapqT7LYHb/DyHpvhobLoDUjJFJp1IAeECElmlFrLi5aLi//ikQrZJ4jPfzig8/4j/7em8ON2HCzumKxXPL0yTO2XcPNck0xKpkfzFitNlEzXmX0TUfXOUbFlPFozM0yUPc1fdfhRaBtK0JQTCYT1qsXHMxL6rqibjYcHEzZblqUyqAxcYtrAlvX0LYd0+kBpm/xVIymGVrmbNYVfR/lT/OsoByNY/07PaRqx3z66ZKrqkdnaQzMB/c5OskYl8XQK+nYrm9I05S2bTk7O6OpGx6evsIrrz5EKRV3B4NkxA7bvri+5MsvnzKfz3n8+DEySTl/ccHFxQUhtC+VsGJgiBd2MpuSjaaoJCNNd5LMLZv1hoPpnMODMToRpFlEiuA6vDG43jDyY0Y3DfaPfkqQDl83+N4wG02w6y0/+fg93nnrbU5PT/floF2QDyE2oXeWiwfzOUIIuqrHB8d/9vffZZZ0JGlCY1sat+H5xYKbjeSnn6y43kiQllEx4vT0lGI0QqQFRVEg2ltmaFVXNNWaRDnsR1/y4vn/jqch5D1ipmIDthwUFU2PVpK2qnGdQWUaKSP9vuk7eueRQiGTHILHeIk0cadVrbckWiJU1GbyQmGCQMiEdVujraOYlnS9ozeRbW5Ei0wSlBJ4ETi6N45lqxBRWIXIycsR28awqqHvo7Ji27YY45AEEgkykaTDIp+lkmB7EhW9SXtvOJyXvPH6CWWhaLY31JsVx4dz2u11hEkOEtBiv1OP94pz7qU+TWxOC4x10bNhcMIKO1E9hh2FhGj4ElUkIz8m/jw6QMU4VBTFvg/1TeNbEdxD8EgJSokhuCtCUAQUO2R3PGEFXrLZ9rz3808ox6/T6zESxzvvHJAnkjxVXK9vSFXCdDzm7PkZV8sl1hoePniIVxKVpdy/fx/b9bRNgxASM2RHYUDDSKHxYufWNBAp1FdkbvH7By8AwjvUHXTLnerK0MyRd4FPNE2LMf1QWxuIGSIZqjcDcmHAIO/EhRA7vP3wGYSMD4QfkA8+4K2EVqFzwadfXHD27JyTkwNuVjcsrpesVxVV3VN3PWlaYhxcLJZDbS9mC9YEnA1YG1heX5LnmnI0ImiHTDR5KmgagxCa0wen4CXT2ZSAQSmomsF4JEsRqQISlJIoGVXykiSjbTccHk4wvWM0HqGDI3hJ13mC1xifsrkObBu42fZUtqNdrXn99dd55ZVXEM4SrI32gz4wHo9JJKzXa168eMHhwQFCOKxtAU1U04yJw+5BKcuS733ve7FWqzVIzRtvvMF8Pufs7IzFYrEPqnFLD/fvnXB88oDORx2hJNWcn59jjOH09AQtIi7bug7pFMIHXNvSbLbcXC+odYrNb8g+M4gguSZw+eIMdxBLXcdHx5gBeREfenmHQBXI85yyLG8XHkCLnvms4/d/NOU02SCMxYsMKwJVm7HsSs6u3ufyBqR0FFlOlsaF6eFrryGloGu2dG1HVdd0bUsTDI3veGoF8mFKiaEfV/TjaEZuQ2C9WXP/4IjgPXXdYOoevw2YIAmkJGWC81BVBilTpMppeotMojVgtW0ZFwU6i/N4uVwzTXOOjo/RztM0NaG3XC/XdC30Xc3sIKWcltGFrG8Y5NvRKsFaopmMNRzPR8wPC6TKIIDp45wKBKYxbLfbQe/FghMoFEUa/YsFOW+//Sq/8/e+T5nC2ZPPWC/W/IP/5Pf4X/7lH7Btuj1IQwhB23Zole0DbpZl6BChl2VZUtU1xjvyPN9ft7bvUPKWfbyTEDYmBn0lo4iglHLvrLYTUNv5t37T+HYEdy+wJqIT4kcatCNk/F4sv0ejA+8lbeepWs9oXiB0Ct6gtaIsc0wTGYh5knJ4cEjdthzfv0dexAbqaDZhPp/z4I3XqBvHex99RNM0QwNV7h8WIRVaJsPKKgYiyFdYcUOQ3QVnb92w1RuGeFljO+Lj72b+YQ9L3GlgCJL4935JvwNZYy9iuDs88p6kJwgxlHQia9V5QdVarpcdf/xHP+Xv/+7fZVtVrFeW1U2FVikCQ5YqrBLYYAmyx+JZrjZUq5Y8H6NVSdssCcEymo/Iyhy0YHWzpa4qLi4WCBLGoxlJIhHKM5mUjEZzlErxHl6cXfHw0T0Ijs265up6wePHD0lUyeK6Qqscbz3NJgbh7bbBOcF4/BqHR6/zb/70r3lxc4Z1LZPxmJSeq2ef01QV6+UKHyKO3xmDMz3GRBhr33c8P/8yTtUAwey6GKh3TdXIcI5Z0nQ6ZTI74OjePY6OjpjNZlxeXvLZZ5/tfVQn4zEPHz7E+HgvWO85Pz8nSZLB11bhuoY00+Bgu13hW0voOsy2AuvRHux2jayusT7hKY6nXz7ll88vSRNN6yWJUnzxxRd7FvVoNKIsb7VPdo3U3cKzvbnk3mFLoVtkd0OomygpmxoSnTAuRhyVAkKCo0FLSaI1WivOnj3j+fPnbBYrTN/v0RlKK1xwLNqOa9cy9QHX9Igkw3QdtTPMsmLIUHsIkKYFremo6w4bAqMsZ72pWC5qhEgZTQoOjkqCSkhywWw2Zzqa07cVq65COMtkMsMpjVOa0cEheEs5kVxevaBrHZNphpKaoIhlzU1DXXeMywTvBcFF5yOFiiXV0EeWinRkY41WCWJccDDLMcZE6Gpvmc0mlFmCcz1aae4fHpMKjfSWIk144/Ej2loyHeV0vaUeoKh5nke5X+v3XrHW2SjJUBSkaRqlgkcl9s4uTMlA8Lc681LomLgpcbvTDN3eS2IHgKjrejDhzr4xrn4rgvt62/EH/+6zIWuXWDPI5RJ9KZVq0dpGdURvsSFga8vKnJGMDumcQxUTrNVcXl2hBmu6qmsJSjAqS3oXfRXL8YzZ0QlJVnJ19oSmbfHWk6U5jXTRjd17ELFBhIziBn6o1UkhCUqgI7AXnMJaRyd37dbowiPYbbGGtusOcyt2AlEh2qANWzbvI3TTDG40O0EhIQeXpR0SZ6/XKEiwKDxpXpAVOVmekaU5o3SCzjJGZcK9iaKYeP78Z09wtkUpQzfUhqXS+ADSJ/i+QQjFenvJvcMHlKMJi+sVbesYTUus3dD3NcV4TJqkzB+e8JwLpuWEm2XFZDSl7mrqpmErYLuoGRdTbHBMijHeGILsSBLBqBhhWkHdV6RpjhMdbdMzyqY4G2jrlkQlSEqefbmgaiqOZjNm43s8eHCfruv54otP6dt+QJLEcllV1YM+TyxhNdUWgsM6O+DSPXs12SELFsEPpRZB29RDY7Hn4cOHaK2H7F9wfX1NCAFjLM8vryPss7Okec58Ptk/aN5ZtIBmU7HZrNhsVhjTUhqPFAGkoEhyCtMjbY+XCiEUfRCsuo5809EnasBsryAMchcDS1INGi6pVhhjaVtDXTf0jcGKMas6pZQjVJbSyoATBqESapviRIbwGwKecjTFOWjqlmfPX3BzdYVpTPQ8HYJNYqJOXSsctbIcJiN0G3eFal4wThKODw5QKHASpQPbTUcxO+J+AefLJeu6xnliPd8plsstWZkxmiQIDeWoADxSJ9hhThfrim3bg/AcHRxQpCXW98yPj0nSgoR6YApDlqeEtWWz2tA1G7KsoO9akiQnVSk40FJibU8wHV1r8SpFixThPYezKV3b4YwnLzO6uqVuKsblmIf3HzAdzUhkj5IBpKIsMorxmKxz1KuYRER4b4pSyZBY6KiNo4bY0FX7OQ0BsjQnkY6egDH9HiyhswxnIw9FCoF3sX6vpERLFasLIspE970fZBR+/fhWBPdN1fMHP36C85Y8y3hwKhiP4pZnsVxyev8ep8f3kS6QKU1wjt/+LcG73/shl4st/+1/9y/Y+pREgsZxOJ8yvXfEZljhhA+MRxPunzzgzbe+g0ey3m4xtufxm69g2g7ZW0SiKNIU0RqMUOATHIHGGeq+xZue1AtcpilkNJpeX62peljrBNdF1TZrYrCRA/EhgltClAYmIEXgYD7h5PQ+o9mE9WbDcrFktV7TGou1bii/vEw0gljeUWnKj374Q7776JBxCsV4ivR+qOl2EDyubgmdoUwTXjk54enTF3z++RlebMjLDoch6AQvBdJraCMD72A8oms2pOmcB4+OaZuG0TjF2ZIQHMFYetMTRMW8yKi9QxYlq+sbysmY8cEIRyCZpCRKA47xOMeFBqk9Vnuy+QGT0THOTUjTiOXesEUHDamkzFPKXPPFlxck6Sv85m++TRIUiXZUzZp6u8Uby6QsY7256yJcMs8x1t1xO/KREeyHXZMP7MTedto8O2VBIQTTyZjOeF6cnw8Wh4IXlxcYazk6OopGHpsKVMokzxmHQJomgMebKFvhnaepGjbrddwdEPDSE3oHqUcFjbGBkIAQKV6myF5gnSIkCQo5kJFuETrODZZxfczyqm3DfDqDECVjQSFFytky8D/8H0/49999xHa9pu3B+3h+H332MR88XRP8CmROkue0nUNZWFxcYesa53YuRQIfoMMT+V0K3Qba0JOrguUXC0xvObk/w1lH1zs2m571ume5aEkMHE9H6CyycvMk4Xg6YrvccvN8S1sbLs4vSbOEJHgSEaUiIMGRs1y35LmlHKVcL5fMsxKlBE4ajLEUQmLw3FxeM5lPSNMph4cJdrDnlDpFJTm295HN3hj6rqUsSrzyNHWDDVHZtN7We5y5dx5jHEqmlEVJmZdoqaMAHYEgJFIrgtbU3cB5CAGpolSBtR7rYjHZBzCdpSgLfNdFBJaJ95TpXSRJDqJ6YmjOeQdt27Pd1oMxT0Quaa1xSLwLJGmGtY6Ajsqh3zC+FcHdB2itw/tA01Ws1w6to+VZVKcDITekUpGrBG8sxki2257NtsUP2+zWGZTwHB8d8OjRIz799NN9rerB/JC333id50++QATBbFJy2FW8VpaM7t/n/uEBSaooVIJwHkYFs9kRMkujQ4wS2ODQSpEcTCiEQvqOs+cb/vDHv+LPP35B2zvYYaK9j1nhTuydgAqeaaa5dzzlH//D32e1vuHo0SmffPIJ1lree+99bhZb2rbbE412sgF7YhSCIBTz6ZTf+jv/Hv3VU7YvzgnXK0ZNz7i3HLmAMA5hHboskK/dwyUpF0Ky6Hrq4LhaLpjfm1NMC4KtkLZC64CWGi8UwVpkCOjgWV1cUeY5WZZCA6PxGG+a6AtqYaQ16ewI6wO2D0wnUzaVp9CaIFpGicMhKMsRjw6Pqbae6xcVTz4/4+jwmDRNKLMCEQwueJIQQEmenz1l42u2fUcaFKNRQl6k1HXNYrFASbm3QxNDQNrpiuxgi7EXHfYN1OiVGXsldijfCGK21HUdKslp2o6zszOGN6YoC5qm4fz8HO9BqJTxeMzR4VGEJQZPXdWsNxu6th1UDf2+tiqVwMpBjbwfSkKJIOBw3mCdJLWCsVMw7BNvgQS7AP+yAmnT1JRFVGRUSjEaT1nXW/71Xz7nL84Fdb0l2Fj/9d5TbVZgLELn0WQjjeebZRlPnjwZPqsY0JYDGzLRICRW5NyIgk5BmI54/cGc5zfnXFyuSfIcKbPoGWpA5CnbrsFfN9gkYIXBthvefnzIwizABaQPbG5W0cAlSXAprFaDd4JxCG+RYkyWKYxtyRH0NgCezjiUmqKVpBzfRycpsaqaxLKGjOXJvrMQNGmS78HRo9GYpmnIskgI2jVDd89YXcdAL4A0y9ADm/YORiJm6sbSG4tM8r2BfGccxkY7TOsG3SmpYhJpoow0w/2wK9PY3iIEOC8Gk4+U1jqCd+g8oyhyQNC1LYTos0DtUVphbYha+N8wvhXBPQD9AOe1LmBsg636PcLk6flzUqHIpCaVCqxDeM+f/vgJrQuYPkPKHikCqYKTkxNWqxVVVbHZbAD45Qe/ICFwNJogfeCobThddTwoczIVmJQptgCuV7BtyeScQgVEpyFRqCJDTkrSacno8T2oO0yXcnj8Cs/7CePHkm1dY0wXg7t1BG8xtr8tu3jDOBW88eopb/3G2ywXF4g84eHD+3z++Rf8zu/8XX7yZ++zXNxgrWU2m+0lA3b4WQJ4qXh+fs6f/fgn5JdPKV5ccc/BSZpzolOOi5RN6KgKgZt67p9Muaq3aOnJE4nINUI6bm6uCHrGTEuy3CMTibN+kA4QGNPRr9eRNWksWqY0TcuXz68IrmEynnF0eI8QJLKIWdrNdsPi4hnjrGRWJkymY4LokTLF2p6x1IwnmsJqZG0oy5KjoyOCt+BahM4YT8Ys1jU3q2sutg2V6UjRaAlv/8Zbe69Ka8xLXpc7KPjtjXUbZKWU5HlGVUXbvNEge5AOD3bbRmSM/EoDs+t7lsvlPhAUeYl3jmq1JJWx37Jabem6Ln4e4q5AcEtm80EQdIIIlhvtuZxlFKmhkCk3TvKk7akEBOejwXK4DbS78VV1R2MMLnVRnlprOuVBQaoD3eoZOijwEcVjjSNTBdEGp0VIwQe//AVPnn7JwcEhZ2dnQza4g9vG35N4ASLhs9Dx368vmCaSUOSM5JTHj9+hUy1BKJJ0gkrg8P6coBVWGDJrscoSpEda6KqOpq7J05RUKlSW43uDyoqB/KUxfSA4iQ4piR4TnCTVGmdAqQQlLaaruQmg2uhilNQWnRSY3sb5Dx4hAtbFkG76KNkNkuurmwhXDR6lxMAP8KRpitb6JVmOWEdP9hEqEMENYjBtWdyskWkWYZVhMKhxjkRrjDFUVQ1SkOd5rJ9bC8PvSsqcNM3YbDZRlsBZ1nWLcxUQSPKUxvb0lYsyxGlE1nkXkE7R94YQur81rn5rgrsXCk/0o7QixQiBG5pFrbVI16M86ABJgEwp0C5qNdwpWbz99ltkWZy4rusoioKyLPniyVN+9pOf8O6rj3kwn3NVbfheOuatMkONUkxiyI/HtM+eoa/W5LYjKVfIRCOyBJWlEATiYIy5OuP806dcpzO6R2+x6Tpap+nbG2zfDI2u2Fxt6gbCoEMjBF3r2P7imu3qBYur56hEsa0q8izj4w8+4PzshqaNsKdd0w/YN8+863FDuac6/4y/8+gem6an946+rWkmE5pRQp+lbJQnm4/YNudcmBtUqfjO/Td47TeP6H2PCQGVZUyDISXgkQShMC5mGErpvaJelqR7JBMBVDCkaRYfnBAhm04ETPBYPLkqSGR0svfeobUn4GMjVwy9CSf3yJ/gPJhA0DlW5fyv/+oPMaajbQNeRFy/AZ49e8ZoQIo4e2u1FunearAki7/zbpxXSsWG+uj+PnAD7ISJjDF71uoOV951HZ3p92qEUkqcMWipKPIciWN5ucC5SAkXYQd/DUODO/4OK+K8Sg+/Ulv+m6szjhPNSGuWRvBF27EYaRAJvrPo4Om5XdS/OuLCAU3TRMlqa6NRd3CR9OQ9BBUDQBjuneBxiUQE0NLQNBXrdc1kMh2mwYFQ+8+cJAnjtCA/iCW/96+vmIwPuDe7x+9857u889o9jKhRCqRIedsqhNC41IP2jENAJFFAKxUpL758gQqam8UWJT0HR6dU1RYp4nFZWlJtVqQ6p8hLrIkLbqYkq6ojyUssCmMKOicjysQGnK1JsyjrrZQauCJugDN7nA30fcS372ClITiUHnxwiWW9nZOYHurdcsjolZIooePcDpNfjkqSNCPoDKUjwEJJxWa7puk7CAGVxn5WZwxhULdUUjIuclSasKm3kd2KoO1u5Qe6voW2Qcjo1xBNcmKpUaJQNjZdQwiY3nzt3rg7vhXBPabsJjrGExAyynyaEOnfFh91HFxLKiyTLCPLcjrhCSi09UyEZjaZU2Y523VFXUU0x2g0oa4afPBMxyV5maE0VNst27yn5hCrPB0tj9Oc5uYK/WKJurrBSUnINF0q0UVKIgOj4yNq01ItFzw/fswf/+R9fnHR0LQ5bbXCBbNv8AkExhl6G2VAlcrIdQqu4cMcDmdjtEzYbG6o6i3bakvdZ3gEltiQGSiveO9wzuNcj5CB6ajk+VXPF9OU119/hMtSFgLUwQz9+imt6SjHY8qDOT/92U9oNpbpwZTvfvcVHr06AhmwIcoSZyIgfBjKR0OwjZWkvVyBEEPmstPN32vrDC9k0FwZshzhb1l0MUDF99hJtBLNB9lJNIggwSp8knHTaV5crjma36fuN9xU0VeS4NmurrFdjfAWZ7p9325g4AAAIABJREFUbyJRGuc9zpoBlRT2Neud4YJSmjTNhzLHbTNqf72EAOfQIZBmGRiDFwI3SEoIIlM0DEzYvmlx1tBbERc8Nfj+iigPmyYZSkkcHYlUuKbh3GZc+QTZBFLhQCpCnnPy6JTvHJ1wdfECJWDbtNR1Tdt1dF1LgFtI5jDjMonG1AgG5VCBR+CDJuAIYTvMDwQhkCEa0AdSZJKik4yLqyXLdYVQ2aB8mTKbRf2eLMkJUnL25Zc4FC2a69rx84+f0WxuUNT0bQMeehMDbBc6PB7pHEmqUULwxmuv88Zrb/Cdt9/lr372Htb2vHi2QkhBlmeDbo6kGBXxxFLofFQvNS5S/+u6RaqSrodeajKlkNqTaknTt+g0oe6iHrszkiRkUZKg7+I94S1KCaazMUMHhizLEQLqpmVbd/SmwwSLlAGReITygxaDQng/1Lg9o4MxD195BanKSPgi0DQVzkWkUcS5J4Qg9lDpruvIEo0zPZtVT9t1WOMYjcZIfFw8RNibz0ffBkXTGHQSZcW9szg8Ao2UKeIOs/1vGt+K4K615uhwdms5xWjYXu2QJgoVLIVoePvhAd9/5w3ysmBRWz5+dsHFomJSHrJarfjiiy84PDigqmu0Tmm7nsl0xvTgENNUrKstsyIjPUiRxx2r0YeIrCRL7/HkueH66IBsfkomPAmBUZrRbirSJEGPc+TDU3pvqBczfv7Rgj9//wOuN4ZubemlBy32MKVY37Vo6Sjy6BGbaYWwgnbdcb6uCRqKIuXR44ekiaJtajorWNWGzimm0wOUUrRty/n5OevNCoEfjEQc73/4K371yWdkacbrb7zObxQTfvmTvyDNUt5+6y1ebFs6m9Abh9COo+MZqYqNIBUCUvho/6Vu5UdljMIxAw2AGgyYGUy591np3axyOJZ43F7cfi/zv2N57v63k6cdSpoegvJIHfB94GbTkqYFo7ylbiRoHe3cCCgRXZr0RANyn21HBuIgqLSru98Z3nvatn25ho3HDeJNEdIKWsHRfEqRaq6WS7oh49t99qgE6Gia9lbpU0RPzKIomR8dMx5PKIoC6yym2wJ+sGjrWN2sEAS8FKQq6m8uFgsmkxn375+gdcKjRA6ZYzSd3lYVT58+5dmzZ3gfhaOc82RF8VJPZjdinyHb//ur8xAgwvmaRbwXdEKaasqyRGlFb3qatqOpo8et956wrZjMD7i+2NBdXTIvIBu0hwXQ9T2rar0vpwoEZZrRLD7i8w+/ZDQak6qCYAVpViKkgKBwvcOFnkzlKK0iE917gtN4HIGW3np822OMoKdFTQq0tLTdhs5apI2GLNb2yFDigqLuYhbtnEFKS0DStlt0mlKZAJ0ZMO1DAjMs4FJ4RqMEnYAQkV0e72MPONbrJU8+/YBURox7NIMJ+L4eTOjj0yKERtpBUdRaFBYZDPP5nDD46eZ5QXpyGE3H+xahFH3fs9ls9s9jmsqo8mk6TB9loLumjRyYb4qr3/jT/4+GkoLxuBi26+BDdIK/De4SFQyzLOGt77zG48f30DqQbDsuNrBt4tZ6sVjgnGOxXJJmOfPDexSTOW+89R229YrN1QXPP/mEpnecvHrAj/7TA0bdl4QOvK752ZMtq6Mf8aPf/o+RsuKDn/8FmZeYZax1NWHG5+9/xtl2yfl6xfNnLdcbS7veIHpDLzyJHvHd736Xw6OjyGJDcTwt+NH3X6Vtrzg8POKV09PoSCMlHodUgaPjGeComiVVl/Cv//gDPvpsSZKOEVLS9/1AkIhUfm97vJD0XuJ6waaxLN77lKr1vP54TlN7/s2f/RV917K+uaZZNTw6zvlH//iH7HSt93/uYPH3/9o1cIdYHdGC36xnsf/J36J58TcetyP+4Om7nq41KKlJ04z5XKGSjK5taJsYaMbjMQ8fvYoQ0Q7v/PycxWKJcbsyzW1A2zEFl8vlV3YTUcxp10CMQmA+aocnUJQJZZdjtjXAfjcWpye85NQkhODw8JD5fI5KExCOtosuPdYajIkP7Gq1wjnHK49O+a0ffI/N6oqq3pKWIxaLC7TOEKjBgIZ98GjbmMlHSF2E2e4WqluGZHjp/O7Owa9jVe+u546BvRM7897vZW328sVZh2laLhcVvlAcns6Qwu7fX6M4HM0i5HS4acoiMohNZ7mplyRCk+TjfWlM64DWHilTfOiHRVfAQGCUApwFZwVd55AixQSJ0j5yL4qMVM9JkzzuxgJIFWvsiAKBQKeKchRVO5VWJDqalnSDzETbdrjWo7RgVGYI5bh/OCFRAeFdZJLiQHhUcLz24B7it97l/nwWS4EqkvKkVF95pjTGWjbrNc57Ei3RScLJycnQVNX7zxI5Ew6kjBpIfUeWJWS5RicAFmM7TACC5vLFisuLJf/iX/3pr32uvhXBPRBwvr/zwA3fH74OQeBDT8g8UlkIHa53EQ3gOvCWsxfP9mSPxWJBOSpxSE4ePsarnN6t6L3gZlOjg2CkC84uD5gmcxKX8/zskq2dUi8rPnvvQ8al4OqLa2rTkY9HbKqWP/m3n/B0eY1JFSSKru7oW0ewAq8USmoePnjEw0evkiYJxltSkXA6n3CYtayrM0ZmyzismN0HZypcY0E60kbSdBVaacbZKzw6Lfj0yWKA8MXgM5vNaZqevrMRQuUNXrhoFhEgBM97H33Ch5+aIWNxBDwCQ+JgNr6H8WHQrbhjByjFy0n4V8bdZt7fJlb0f38M6P0hEElxuyb0fYQCWmfxQ5PKi6gH03cxKGy3Wz7++OOIXhGR9JGkKWIILDukyl1bw18X9HZaM1prpLDRx1NEi8JyVLCp231DVUoJbmeYPOxshvLScrlkuVwS1MuaH969TG4LeJJE8nu//x9SrS+5vL4gKyf8z//T/0aaRrenJIksxLquybKM8uiIoiw5ODigqirOzl7EXc+d8/l1f78063eu391j43y9vC7v8u8dcc/bHtNu8abm8dtv8OrJlET1w2sHsTTTRZlgQEjBdD6nKMt9gzlL0pdY3oIaQdTpVzoMshpZZJkSdeaVKsjSnBDkvqxmXPTG9Xg0CalO0TrBWkPvavywwwwEinKKSkZoPfTNvEWbZn9NnQMfCkKwpJlks72h7tc0myUGhbCBgCEoh99uePet1/jum6+RJ4OBivMD6TEduCj7Cd5XLZumpu9bnHeRSZ0kEGSEPRNfF/wgVDLKCSGaqQsRSDNJ1LqShCAhJNwrS1YH01/zbMXx7QjuIWBtN8xKwN91NQ+RPCAJKBXQGvAOZxzeBITXiCAJwd4a2TpHkma01qHzknXT4YTi4mpJ1fV0m4q0P+Kf/8tPEFmC1jlegXENqX7OLz5e4NTgoKI1ljV//fOPuLrS+DKL9XMncN0K33cQFJ1KGM+nvPXWuxTZmDDg2fEQTE8WDN//TsqoXJDKa8qg8PSYtCfJJNZ2jAtHkYwRowc8OBmjlSBe+hiolZJMJjOaqhs8QiVBOKyIGUsg4ELAmbgljgZaET0gBXTO8+TplxxOH8byCfF1dwP7vtjyMlSDr4X0X5MJfuMxX38Rt0qbQ8U+xCa0c4G2jefphcAFC3dKdRAzSmvvZt1iT066hUPusA4DOUTK/ecKgDM9EOFpQITcDGSxqEooyAY0RZqmJDqhrVv6fteEu323EHayvObltfKOD0tcTALr9Q1Pn35Gnjjqeo1MEhCeZ8+eYm0kS72UhQ/nvSsnvRS4hyCyw0vfLpxfn/Ff5zUwXLJhF7W7N+7eGgFne4KrSHTPD37wmN/63mtIovm4HJRb1W7HPXwMXWSDIUjMZtWd+QcQzkboo/JItbtu0aLQOQdCDue+Y3EqlAyYgYPigkO5KLAXUWmazhdYbkEWQiuE9njfsbxZ09U9ykd1z6LIIyEw9Hhv8EZQaEE5nmL7huAk3gRs6AnSxr6EHuGFYts1kawkJd55rN3cmX9PEIMOe9uyXC6RKppuTLoJiU6QMmLrpYo9oUxoZIgKsNY6rB2sRaVHqUCSKDKdQUgQpPj2/wfaMoRAuGNoHC+luA0g3iNCw0jlTKVEmhbrPb0LWCSdF9HSjAhHClLSE8jHExyBbTU0O4Ik6DHbZssvz1e4Lw120E52UuH2aAGBlwovIztMDPVUr1NUcCQhxbSGsKlwtscriUwSHj5+QDkf0fioCaJlINEpVzcbOj9iOpeMywacJxhHW2/JihxvwTqDThNKldC0Fi1LslFBVZsBImpw1jObjamqNTe2xnsNziOti65Aw9Pod/Xt4YHKAI8nHZX86uMnvPvGI/JswOHbJMoXEI8PiFvUx24IovUX7IPM7aV7OcjvtHC+9h589WsRH9xw+x4WgZUWT0QGVb1DaM29oxnWGzbrLXVt8D6JuhsS0LcZsScqeCJADpBxv2cIg7cWNyQQw39IGfZ8ghACNmikTggkWO8w1lBkKVII0iyNiJsiY7FcDubI7JORWJsl9jDEbTb81eG9oGl6fvnB5/zoB9/FdGvOvlxiusBms0GpBGv9PpBLKfcBcteX2pmM7KGLUiAkexintXZv9h1LXhKtkn2ZJc9zUp3GgDwIWe3e2w+7PjNY1+2MnZX0EHqSxHE4y7l3kEdtFymHBmBUc90RwEDgQmBTxfpxkiQRBbfvU4BwhuBsXAi0igi5YdeVpAlKR115iJ7G3ntUMkiFsFsMFG5I7qRSiKCibaRzsdQhenSisEayeLbhT//sZ6yabm8co4QkNB2pNvyjf/g7vPXmA7zscX1DbwW2DzjfIQd9KVJBkpRIHzVfQohWlkIp9KAB5Zwd+iIWYxyJLpHaISU46yLSyyoEKiJjdMApAYPiqxmIkAzJKiIghUX7iLCRUn3tifrq+FYE98CA4b7z9Ut5X4gY6CKboVWKc4HeOrbbjs2moak6jIkXerONOOZq23F0f876ZsvZ0+dMZzkg6XqDR9IHh5Cavo9IBCeiKh0MCJGh4bXb8sfbsUcIT5ACZ3qscQSp8FJwfHTEq49fi87t1g5rt0bqlDJJefjKPcpigxKxxu6MIZEB5UJ0tAmAj4JGq+YG4SaMkoyKLSBip9y5CFEbj7DWUFUVzpnY6wnw1Vz5dleo8ELgRcKXZ5e8uFhwer9AEXWnA1E6eF9+GQL8fggI4jYTevl3fCW474wsvhLUvnYbiig1sXvPECLexqq4yDdtg/OCw+NDTk6PePL0M46ODtlu6iFbj8EieamMEKIW9u038PbleflqGUOIgNJRGyjC5IgZVYjlwChqFxtt3hmMCSidMp1O6Pto/HJrlbi7b4fMdffnK0NKAUHw4Ycf47qW5c0NjTFonXLv3r1BDTB9qTdwl+8A7Ik3+/MaPA3u4uD7vr9TUpOowQA7hIBWmkQlL7337lrsdr9utxMZSkrWNGw3V3ShZ7G4YrWcMC2iiqr3LvIiWnPnswnarudmtQIidhydwNBDstaiRGBU5MPxFmtNbJwLiU40SZqi0ywaYwz6PlJpklRFqz7vafoeJRWmiY5qgzcedd0Qgo/kOrdiPrvPbDrjwclD2i8XuL6mWncY0+C7Nak2GBdtC6PdXdiXLQVqUGoVQynFsFnX/OpXv6LrOsbjKcf3T5Flsrfo3DlBdW2E2eqQDJiywRFN5ihxi6931mBNj7XEPw6cDzGD3yFofDRkGY9GkVT4DeNbEdwJt9vN+OXLwV1hyFLJZDyht4Gmh8Z4NuuWZtvTVoblYk06MFpLnaCFIvSBLrRUq4o83WVUUfUwiJjtuCHDcoiX66Ii4IJ86QGTIdKzg7DR4zIIUClZnvHG279BmhbDIhN/jzGSBsNvvP2Ah6+8Bt0Sr1wU6E96ZG7ojUB4TVJkyDRBqhkq5Cw+axE2Zt4+RGSEsQY6QZrGINB1Hb6L87QLql9tJIIYtrCe8+sNJ9M5V1dLDucJWpjoNBSXslv8t385GAfCS9H6ttH99ci1d3z6aub+ldcKORiz3DnGC/BCIIWk6zqSZMJbb7+FELHpdXx0HzVoqkRpAbf/JXFB9nuvSSA6z7ldiepvXpR2JS+45RLsAv2emBRi4uGDBJmjSPaCUNEV55syqN15izuLSsyeq6ril7/8Jb3psQiOjo74/ve/Pxhc/E3ol7DXjP9q/+Duue/gq3d7D1E4TezPK34i8RKuf/d6N9Dq4zNy+wworZlMp2DWZFlGVdVI16O1pqqqSPbqI60/z/N4yaXav7+UMqJ7lMIYw2azod5sKPNiyLplzFxNHzHwMvoeF+PRfv7yvAAUbW3oMjFkzQYhDHUdG9+jMkPgB5RPoO0aRpMxq9U1zgpef/2Uk4cPBrJRhfOOtlqzvH4GOKpqg0wCoHBWQJA452ndTn45EMKK5c16z0Wx1rFcrFivYjJ2enrCaJLtm9HbbUXXxt1NlmV4J/AGOmcIIZKvuq5iubik73uklBTFmNF4jhBpLPEKIInQysvlDdVwvr9ufDuC+91xp0YKO4RCz+wwqjkGNJ2xrDcty8WW7arDmZ3WuRiYiAUigGlrfOcY5xnVaoMQnnI0YuscnhCJH4kiuBgI7tYYQwBEbJjtGi8qCGJOb/C9wfmo6Xx08oCDe6e0bcS4lkU5BGVojeXFsuL9D28osyNkOqWvG26uFtSrDW1j8NIg0sBomjEeFZxfNXz2bMuyjqzJ7XZL19t9UEiSWP89OTnh/GmLtV/PTHfY7jzLsV6At1g0loTlzYYQ7mNsj/N2H+B3W/qXpSfjPPDNqKvbl+6atL8muO8+m5QhlreHnRHE3ZMN/xd1b/JrWZad9/12c7rbvffiRWRkZMfKakiCZJHFRkQJMGyAgAY2IHlig4IBwwMD+gP8D9gDTzwyYEiwIUAD2RPaFiBbkKEiLJImrSJlwSTBTmyKxYrKrMjM6F57m3PO7jxYe59zX2RkVQ08SB4gM15z733n7LPPar71rW8ptJHJRstuyfZ2y/03zmiaRqZJlWvMmyXFo+JpbviY/nCEFF5veCfsOTuteRi6wDS3t7dZfQ9SEpqkD4k0JpSqUErTdR37Q5/7oNSnPvuzGCpy5nI0TcNqvSYoxXq9ztlYJCV95/3lvhauO5Sag3yvrXRKlvWdApLsqCTyM/mz82cNfpY6holjPWnH54JkoXB6f0Arx5v3V3zlK1/h7bMamwRXXq/X3L9/H4OdnMt2u6UfROdnGAaurq6onWNzcsqDBw8wxvD4Zsvl5S11VbFcrZCu0oDR0iPQtgusaXHec3V5ReKWumk5Oz3DOblWZeW8m8USrRVNLROhmmYhtEF3ICkxynVrWKwaQkrU1YqmeTNDHBY/btGxB5VZOylBsrng3qCVxRrB2LUxnJze4/3338+ZliUkPUX3SoEPkp1sNhuWSxkjCYqqFpiv1LtKh2sMMgcgxkhVVSyWK+qmIyaZo7rf79n2eykaR8e4/WuDuRd1PmiqBmsqlqsldVWxaAKPzlcsVhtcEqrc7cGxP4xEH1h1C3bjSNs2pOBpKkNlLCkMhBS4vnzB5dVLzs5OWa9WuFF03GMI2Nxqn3zKEEU+CZWbP3IRT8R7ZT5U9IEUElXbcXr/Pj/yhS/RLVbUVcrt8OImEgGlE995csmH3/2IYdiTrKXJ+jjJeaKPJKOJ2oG+oDYDPkZp4kgBP+5x4whRYY3OnZmel9fXuZaQOzGzbSnRTtt1k1LhoQ+0Tc3qpEHXNW23ZhwD0Y2oIBjoFLkXJk0q9jhb6SPj/plxqmLi3hql7xb0UsnIMoasxLhrralyVOZjYDQWrRpWy5bQdRz2W26uNe+8/Q5dt6LrFvSHG7QqRdTCuIkT5n18PolPO75XTnn6ylpDjJ5hGCbNFqUi5/fOZVZo3/PipYwErKxls15zfX2DPxq6AFM98lMFyTu/T6IvHruGzeaE8zcfcnJySkqJi5cX+BBp6uaoMH10HfmqQgg5i0k4P0oG4B1930vj09GlxhCIKeGdx4eAdwGjZODDcQG6OIJxHBm9m4MmpdAmYszI229/idVqATqg1Qz1NFWFxhJ8wGrFWdsSXqk7hIzDp5hYrVd84Qvv0+97cTpRWuC6boExlovLS7bbnjYZukUnsKc1JCW1EFF/rFguT6es0zlHVzVU2cH54FnbFSFJhqG0OLhxcGw2Iu/RD72Mv+tqKqXR1EQ8KJEcDx6MllmnKQ/SUFqhjZpuqzIGg3TLawWjc4zjwKHvOTs7o+3a7DBgwlGT0HZteVaSpWlPRSEyT2OKyaFTom4M3XLFKixEA+llYnf4a8BzVygaZakqhdKJZVPRNh1nZydUlUUpx6EPfPuDT4QzPA4olWhWK370wRuEqNj+wQWtSUQdWJhEUg6Lox9GtldX7G5uOduc8OZbDzldn/D02XOccxwOezgcSClkFTvLGACVqBgJoXCb85iN3BW2aJZ099/kp376Zzg/PxdxpuDpR1EodM6BH0h+wHmf52mC8wM+5HQqBpTrp4IXAOGClMhiQ4kUpFkphigMmRy9DbmluWAoSuWoWxsWmzOWqxUpJvrDHtyeoDS7Q8WtbwlmyX606LTAJCuQi4oTK0G+lw04NemUe5UNpj6qTxwfIQu8hAmqmPOw8hOVCgsEkpe6Q/BCa0u2gahZL6FZKU7vP+DPv/OYN95+l/OzN3njwUt2W2keisHBdHeApEjY/MDl6Dkv6zEf/HWHGHKpLeyy02zbls16zU/8+E+ijeH3fvf3WHUrdvsDKTjaRUNjtRhKM0fLKa+N0RalJoWDWUgMUEnkbV9cbrm87dEffIS1VdZp95nn3Byd73SVhBDQSQy29y7Xa6R2sljULJYN0Q2kJPopxhhspTg/WxAjXF7c8uL2Gu9fL3EwZTXH98wYMJ62TfziL/wky1WDQiQnyntiSAQfs3yGyAGIr0+MzmG0pu9Fd8V7T1PXVFaG2Ne1XPvhMHBxdYPziTFaPIari2v6j0Qzf71ZU7VNrneM9H3PsnnJou1QSnN9c829kwVvv/VwuudWZQbToiWZyJOPnnB7ec0v/I2/gdaG6Ef6rdQFmqaWwSVOGvaqSud+BpFWNsZMNNUiA2CMoWkbTJ6uViJ3ZQ3dshMYuBjzJOJ8KEVIUaQtVMxOUNCEEAKVqrBVNUF+IVND/TDy+Nvf4vLqOev14rV7uRyfD+OuFU1tqRuN0qJ77X3P5eXzDLWIumKBDYwWUzsqRz8MhCiLqbWiriogMQ6OmAaGXirWbbdkfXKGqVtsmgulgoG1VFWiWaxxUaHrTiQ7+yHrLYuH1YRZM8Rqri4v+IPf+11Wy5UotUURLEopyjzY6EhBKt+ljuBLxxoAER3kd95nJbk087IBNPPDc0yNk0jKUNKdiYOtNCpF3NDLazKDRIXAYd/zvY8+5oMnS+6dLVAhYJLGWmFaFCqd0lJohBm3f1W4Sh8VQ+fjFTgHMqMhwzX5OmS6uyJGcsOLFO+0EcW9PhyIGlZry0//zFf4k7/8Y5ZLg7WGt956m2fPntP3PYfDcAfvjpkd8/puzbuFydcd5Tyd8xP1sW06NpsTNpsNv3b5a7z77ntS/8gT6E9OTxjc1d0JXa84vleNu6SI8vU4ivZImad2eXmZ35vufEZJnQrermOasHFANFp0YrE45+Rkw+VlYL/vubm5wVrL/ftn/O2//Xe4vrrl//jnv4po+hSe/mdDSMcOURGpK8PZ2Snb3Q6bAiYzzC4vL3n8+LsZ9ol03YKmqakqi8/TsqqqIoSY6X8Ga7IOCzKMxmT9c5FtThx6x+g80SjqThqfDr3nMPopy0hJc31zw4uXFwB0XYu2J7iQcg1BMtQYHHWM6KamW25Ydms+efocX7TUjezPwYmuU+9GrDEYK89l8CKvUCA6oyvKXAWpF4wo4jRsvdwrbWQmrbVWpCuYAzlh+Gjatj3SvZHXVJXMYXXOiwPK9z2ExOm9N1ifnEy1oM86PhfGvaoqHr31Bkp7IEiacpxfZ3GoufsrCfad5q68smB1XeO9Z7vbYsbEGKTL8eT0HsvNGUEZPnn+ksMwMhwkghY954pxHIm64ke//CXu3X8TH2C323PY77m9vWbsr+j7A8EHRufQg+fm4gW3ly8ngS0KPS1l3nOSSFZpLY06aZ7JKVV5KNGtAqwuXjwzFJLJkbK+Y0jn7wuEVIpogdvrl1MRMYbAO4/e4eDg+YtPuLm45i/fXPHWOw8waUTHiNLSVk1GNcTQzzzpmROt8s8+O3IvMMLcIKUnw14yDIlmFT4ooSomQFekoBgjHEbHkBQnZxt+7Me/ws/+3M8QlOHJk+9x2EvL+GKx4MWLC4y23N7eZjhBXPDxGh036cBRVPpKUXjSllFhwp2NMTRNzeXlJW+//Tanp2csFh3e+0lF8uTkhIvL7ZwZ5Ossn50meGv+GUmmP00NVkfg0Wxkjxuh1J3vlVKk0gVa3pdNjctBgjEz9n1+fk4i8qvf+JcMvWO37RGRwruQyad57+rOf0YpvvpTP8FyseDi8hpC4Ppyz/b2VrqDvZs6Un3y3O5D1iTPESpjpkwqMjcKleRzC4Qlw2iAZFBVi8KTkpdiexIxQTf2k9AbwMnpmnfeeCPDqBKkvLi6ycY4kYJDpYAdRqJWoKV+lrcrCoXVouOUksCQEc2+94ImKIXPg160tiSfIMv3icSwDH2xSgGWGNRUA4pREZXG+0TMDJySlWsrD5ze+zv7UeqMwooJXhyEQgmKowTS9Dng/H7H58K4KyXSIRPzIWUoZDIKShYyG3GjmWYPpiSj04ZhxFrpMvTes9/vScrTO8v5+SNOH72LrWrRfh9DLqLMrdXeB1TG2ZbLFfce3OPeg4cslguWqxX77S0vPvke3/rWX/Dxxx9zc31DtT0wDrmhKHl08neczh0J2iDZhs2UwnJfpsFAZTGSw6hcJCQRUwfq09FmMQxVpTk9u0eRtSUlTtYyku0v/vzP2Q9jZkCQ6aaJq92eF7dbauMxsciazsZDEoC77JiJBVMid/26yH13csdkAAAgAElEQVQ+jmd+QhYd0/P9jFh8tFkbe2R/GNn1kcE7docdL69HYqi4eLnl3Xe+xNOLa1I6cJ1pdScnJxwOA6cnZ6SUMt9YKIklciqF8OPrOB5wfvzvVEy1uSFHqUla+OrqiufPn/Pee+8KJJKLlN577p2c0rbtxNTIuNUdUsDxURqr7mD0vJptiKv/9NJOXmPCwl/9rRsdT58+E7iREg0Lu+jxdz5kHDwhaKypSenTqoLHGVoxxOW/L3/pbf6dv/l1xnHk8vaSx99+zIunW9brNZv1hrpdEhWMXjG4sp/i0bUoRPsnTftDK8lJlGJyjpFIipLVeB8ZYynoOkjQVZblYsF6Lfo9VWcQXbmYHYQSZlAOHJSyKGVwSUtfS4jokHIwk4OWXCvTukQ4CjCMkxMV6q5knQmtAlpLIBBixKCokpjTEvhoLSiCy3vOZ7qq83J92ipiEvrtq3Mbyl4uA7NLliW/llreXb7yp4/PhXGXpyFA6cfMm1fnGL1s/kSJWLTgq/l1o/Mc+l5EeOos+6q0pHdeugzbqhIKlvfU1opOhVaZk2pIKcpsxRRpKoVWgd3+kpAOXF59hBtGXnz8MR8+/iu2u9tMSUuoOKJTRKWBjdrR1XA1wCFJBT5EPTkpoxKLtqJtaozVoi9hKhk8sOyorKHrNE3T0bQLPnn6kt//gz/HT3S2o4ckGyTpmhTx/n44kEJge3MpxlQh3ZV1xXBzOTnNw6HndrulqeKkFHh8qIy3l7+lcsQNZAOt7hjqcgvln1I0nXU2BC4KEywTY2K7H/jg45fsDwPORVwAH2t8cCQio2/47gfPGX7jdzh74yFPnzzn/ukpf/T7f4gyhhgTzgX6YeTs3jmrzQneS/Q+5jmggkcXqeQjox6zQVcZIspRd4wJQsJoizWapjGcni159uwp3/6rP2dzsuL66kaaXqJE4NbAetWy3d6gzfw4lWudokGOHL2ixKfTmr2KipQodP68NC/y9P2dJ4gA9OPIeDVOP/Q+8uLlZc4qQakSoWYcPQmHX2WDpvL9FrxcpgFZI7Njf+5nfprrlzc8e/Ixu+01wUfuPXgkfHMFXnCxyXhncylGKxtenScPCTKVlTaJhOjxXgrCIXhSnpdsrMU2lvXJZhJSO8niZiqJlHZMI4Q07ceUNKlIJos7kTWOClccfr6HwQfJqpFiqZogtCSTunIRtTCRivyzdH3P9yBGctf4USDE3EQGwgZLSNYqQUicupxLxlDqXKXbOWV51hRz5J8AJXr0fz0i95RQ0eeIJE1RT9KZsTJZDgl5YozSbZofyjGPpRu9x3iPtpagDT5FhqHn2SdP8EciSPvrK3w4UNeVRHJRy8BgEsbAv/rN/5OgDKuTc6pas1w1vPPWIy4+esru4iOMgcpoWCzYnHWSRbz8Nr/88xvePDH8+rd2/Is/uWbrlqQkRRGlhTb57/3SL/H1r/8CPgy4oedsvcFWihhHhnHPvfMzQrRcXY/8z//rP8s0KRFxkk2Z09okm3noDzx58iEl2gtZlkBr4dO++4X32fUHxnGLJaJjhd97dNSkEAkgzul4ox5h1HcbfkoUV6CbOXLMbK4pajV61gYvA8Q5rhsALkZeXO+JqhWxuJiHkSRFiBWfXFzTrk84jB/x8Ycf8h/8rX+fb/5fv4ULkeurWy6vtzy/uJZJ89ZOAxYWy4XAQRq09hmukJbuGBDMfHQ47zLjxOUipYYoBshaaFvFuz/ygGfPv8N2l0AFvB9YdgtJ64Oj0o7TVc1FrRijJyqLidky5/+m/ZzEiBYDcRSIf+oo+H/+Do6gmwnDO3pjzA4ijHGCGqZoOCbATAYXIklrQlKsuo7NokalwMXVDcPY03Ud7733jmj1KHk2N4uGl08/YX9Z5xb8BVVtiCj2fY/NWY7VRnBucpaETA7SxubzF4gj5sKh907mmyaPtYqua9l0MtLQWoutZBA2kO+hw/me4HUepiEd2irX3LQSGYpYgkEEqotKssc6awLJHkxgpJYj/Q5++p1Rsk6lDhVzXagEOzrPUy7BS0qJoNN8LyjRN9N1p4zZyblqoo8TghBTwpqCJghsE2IkZUcbcjdKicPuSl+8/vhcGHdgwtwUUigU+/V6zxRTkqEEyObt+56maWmahnEc6RbLPCNTCnb7w57h4+9JsSY3gaADtp8vP8YonaWVYLjKVrz11jusNwu++MX3OD1Z8879+6zXLdvtLW4ceXB6TyLF7RVffkPz77534ExvWf7Ym/zut3bsRjthgCmB85E//OM/40fef5+v/exX+ejJB5ydPyBEx+3tBcrUhGrN5dWOX/u//x+evrwlZq/+agpedHTEw8sAZRJoHae0MKXEk48+Ek5/CGhjMNk5aiWSsiRLKvJ/+XgV33+Vby3rnqbXkqQBKccYlOaYcn8mOOSoSUzpmkcP3uKw1zy/OOCjJ2aYoGDeV1e31F/5Mk1Ts16v6BYd9++f8+zyhkN/YBh6hlGof23b0nUdIYSpqCWiaYJZypBpi1YytKNt2+mhLBN5Qgjss6SEUgpjDT/2Yz/K4+98m9ubHW3bsuw2DDuJyJyX5pPTzZLNsuXiZofQVj7dUPbq+n0WQ6Ws2d1fln9mo/RqRl5m9epPvy3f2nTnnpAiRsHXfvon+PrPf4397obf/f0/4MMPRZr3/HRN0y1YrRYsG0trNZWV903ceqNx3ota6eGAUoqmqidRNaUVqhIHkPJgFbk/ArEopelqy/n5GevNiqoydNbSHtUsIolksmMMAaMiySRiGLEJdNQEpcWAh0SlRbc/BHcUkAj/vKqq6X5PYmgZzgzR3Vn7ApOU2ot3ceodARm6rfOzKeuqJgmLGOMraVe5IDXt75RrcscGuhRUQ8ikC6VQmf6civHPz/axIulnHZ8P467mhQGkgKCO2AXcxQLT3QyVYRimhd/v9yyWRVI0yhzEpDFG0p4QIygpdoxuxKjSwptZKFpT1w0hRr71Z39C01q+9ed/QNNUqECmOXq0MnyP7xITmNDz4P6WkE4xVZ3ntkaU0nkijBwJzcHBb37z3/D86ppx2HPYDTx66w08hqcX1/xv//K3ubk5sL3pGVygrmqS0Xcgjjs4MpmbLn8AbUQKtWyyOI74JFrVUy0gCiQlhllPn3R8HDfBvNbQ5/y1bPYY7kILnyp2J4mSJuNGoqs1X37/Xbruig8/esZ+mIuZKMU4jrx8+ZKHDx9yOBy4vLjkcOhx44hLipCbbUpmUM6lruvpQZb7mjU+xigCddO2u9veX9eiHePIkgNK8dajt/jqV7/K4+98QFV1bK8PlCEMJIlGm8qwWbZcXl2hVSJEKSAVw1ECl+Mu4sInf3VdyzkVo5JSidhl05d1rWw1acEYY/LXOdONicGPHHKn4/H1ltcbozE6EMYdh+0l2+sL3nnzDd5582FW2Kyo247KaCotUrcJT0hpaibT2tB1XYYassHK8NcwDAyHgTEJr77c16qqOT8/5/TkVMbb6UhtFcYUwwvDBAsqIlkvikSICrRAM7aWgMZ5T6Ii5qxojGBSmArkwoqzoukSwqQjVNajcOOtrSYZACnWMtcGsmNQR8FAVdVQDP9RM9mxLMTUDFaCM6WnRrGpuzrf71IfUkpNpBCpuWU6p83zYV/Tif5Zx+fCuJfi1fzg3zUc5TXHlDaYLzDGNHFeS+prjMGaJClXNKAitlKEKDoYMRpSLAWLkkbLZ0o3pKY2mq6rqGrFgwfnPHnyCQCVbYhREaLCphFllnxr6/m1D9f8xPsP+cYH32bXLGh1RYjVNOw6IaPULq93/Pa//n+JceTf/tGfUNcaHweG8cDlLuDGSBgiYQxYoyFrgLyueJmi6F3PKnoJlBROJ8ql0QXOE2w25CjEKlSymY10V6a2rHW5B3c54gmOBgUUI3qnSFjglwJNvArzMJDSgcp2vPvOhrrVfOs7n3B9fT3z91Pi8ePHkxzvr//6r3N5eYGLmmQq2qaZBZaQsXPHD3VKEEIxmkBOdY+NZFnT0oWp0kzZJMGf/umfst/vCTHi9ge2ux331g94+PAhN9sLEYgKnlXXcLpacL3rZXpQNhZ936PziEVjDOv1mqqquLy85Pb29q4DPFrnsme01lSVzTAB01rYDHOUCDR5Ga9Xagc3+y37YZiuYyoelow4ybzhrtHsd1dYHWUyWFVnmyNQhmwwkWBIquwvOZeYEjHMmZn3nujDpD0v56pZr5dScK3riSgRnOf6dss4brPiYYvCAiarvs3ZNFbWpW076qbGamibRoThsJgpYJEs8vbmhmXbTPc2pTj1IZTouOyTYmwLX1XohsLymXEz7uzj4nStMZNRrqqaMpy9BKqlJlb+nhSxZ2MduRsAHZ8bSCDade3kQELurIdZcvr7HZ8P487darlKc+fh6w+JEnWmWSmgy3jrIaeHtbGMBLSxjCkSRo+1RophZVpPjupSdKhccDnsb/mR99/n+YsXfOG993j/i+/SLWrariGExMXFNW6MDGPABzBJRMUuU80//SPPN/7iEy4OHWPmxKM1SWlifrD2u1t2O4ELIHKbZPxXiHLzRi/Gl5AgKQwxO6PMez/C3XXBDck8YT2ng1Fp2spQRUhVJOiIihHVO1IUalYdpMlEOu30ZExK0RO4Y3hgNvohq9MpyAOH5fcTD1vP8ssl+iwwcSJ3A8cIUQab39tYvvTem3z8seLFiwvG0ePR3Ox7/vjP/gJbVSJ1nLGHRKBbdtKxPI646Lne3grOmjK3PkV0DLlgWSzcDFnNu6/8m9BJoVVkoWv6g+O3fvO3ubx4wdtvv8ujR28RxkRMnm7RcHUT8U4msSmluX/vHm174PnlntF7QjByGyO5AKa4vd3lfa1ZLlf4PAVKazWtnQxrro8iTCDFjA2DsZamrjjsDjx7eYlQDzI1rtyflKiNZKUhRuqqysPAE7bS2ErTtZqTzVrmhVZWNJPyOUy0O5Q01mTGhjKyJ2JKpBhxrmcYRsZxkH0bpd5yst5wfv+cxUI0Y2xh7SBVxWQsjVkyVAqU0Ay9j2yHgYMXfH0cRvpxwB3BH0ppuZamYbFYUNc1nY5YI3g7KTH2e9aLPObPKNqqxoySQZhcRI6x6N0E0bRJ04BIIOGjF1ZNjtwjgJ4dgBsOGQpLIiVtjGQoKWWnmBi9wxhR6LTGyqhQFIMb0NrkupSetZVSrs4oRdN2ufal8C4yDB5jDXWdndFnt2tMx+fCuAPTM1Yq3sc890/hkdNzOr+pFAHF6GmsNlOjTwqyMdyYvbUW8akSweU6viykDzx99oLN5oSq6vjWXzxmuWr4uZ/7GierB9xeeUIaZWRWrUiqwTuHSzWXUXPdkwt1Y7azKn8v0AiTYyFXIX0u5mR5VzXDUyVq1ilh8jSLqqomWVElPUsl0JmiNO8jXkMVFQsfWfsBZxz7MOJIWAPj4Kkqg7Eem4d9wMx9ntCXHAF/GivOol/GkFLI+GGaoqFSQCot28fFQYne5vRVpYglcrap2Czf5sG9FU+efMzHFwd2+4HROxnYEXNBMEnh1WjLoqtlaLjr8cqQlJWHNMn6ah+Y/7LsrbtY9DHEBTopVDY2zkWCgwfnj9isTmiqhqYWPXfIM3K9MOtFSkGxXLTYquPl9S2X1zvQeoJ7Stpvc/Rd1vP25oa+30+wQVVVNE03pfjWWrqu4v79+5yfn3N+fs6yrfnOXz3mV7/xa7LPrfCtC5c6IfvcGoPpLJvViq6pUSqACigVaVctxlpGN+JSyCn/TAVNeDii9hpdYVI1SRzM0JLi9PSURddhtaWpmwyTaZI6EKNnjGU/gbEVRtcEr6iswVg1jTk81Ya9T+IoUuIwjFzvB/p+oO8PjKNjfxi43e558eJC9lCUoq34tkitLYumoVvULBYtp5sFXWXyfFUvLJtYEAPJFlKWmS47QaVECAmTBIEM5ME3GWYrCgIAzo9Sz6osMufYgUq44LDJSqCkIodhJGUoR4OoSybB90svQJFDrmvZn33vJngvRk+Is7bQZwe/cnwujPsPRo/kmNL8UqGcf0PRsG6aJj9ANm8wsCah63oahCwbVedGAMSTK4WwVRWXt3sOfeDlsytspTk5WbDf/RuuX+64udninGMYRw5KovNSGLFIvaNEvjrX65UujRrHzUIKtDQ/KKXQjc2OyWKMzR2SFVojmvJ5UxX9F60kWpfJ6OPc4SZWBqWkrb9N8AUXscFzQeBDIzUNHzwhVuClIq/urLNGHcEun6U4eQyVhazHf4wZxzhnX8eqn8eZQPncGAMpimTrwwcL7p39CO/cPuSDDz7k5YsXkvJTimOZM280m0XNwsoA4sMwEtKc5aSkSMyyqKpEZHcc1fFUILJDSlMDTNM2/OIv/Dx/9Ed/NGUiz188F7aE1iIrwVwMIwmToms7rm72qCSwYfm9MYaYRN2xGO7FYsFi0U4PsUAy1Z1MKgTHJ598wuXlJR999ITVakEMibprGQY/ZUbHAmgpOYw1tK2l7SrpdlZiaGISQStJ78sEADHuzuXmn+RIyGB2KYgPRK8yFAFt13B+vpG5q8bgnScERz+IQ7WVRRs5F4FNxIhaCz6JrG1KgSrNOHQkZ32I1lBXabqzTcb1A/2h53a/ZxhEH+rQ9+zRDGGG2rxLjKPnZhfQZuD58kDT6qlvwQB1jCit6HJhPSaR05bxd4roRb5BkcTZEUhJmq6U1oIuxFxLQsnsVD/mtRE9/0TksN+BUtQZ7vJeJA1CGCWTgnmub4yE/MxMTXm5m748X+Q9ciwg91nH58K4F6PwWcerv0tx6mObJAlOTtYTU0JrhTXSIKMyPGBzOlY2gNZa9DkKRpsClkQZOBBdxCsNWjG6RN9rrEk0lcNqR9vAQuncoiFRQW0MlTVTuzAqTRQxrTVGgVZSkDLGyM3OGjFKy6Yu3jynMIQc2RfAPBEAL+dZmBNaNOK10cLbLUX4GNAq8NAceDgeuLLQV5pd8lxeXGItVLU0hRGPC6garWfDUtgFrzvmyUBx+l6KnHaCdqYiaT6stVkr4wgDDR4VpB07xQFL5KRR/NSX3yN+8R0xhkihMhToQGmiUrhxZLfbcXWzZXvo2e927A8HdvuR0c/tRDEmdLzrqNKrcr0xobUYEu89p6dnfPWrX+Wb3/wmX//611l0K/7+f/cPJlw1hCCzAPI6yLUqnBvz3tWTql95KEvXblnvuq6orJkcY3mwj9c9Rpc1Z0a2uy1PXworx7Q1Kikqq1i2wgIKIXA47BjGA1WlsJUiJSfPTPIkAnVjWa9XE+On0kW6QU0RuVxHaRDXKOVZLhvu3TuV6ydgbMD5Az5Iz4hK4iCUAhOkTb/I5BqjiWnAB09lOmLUGR45kicG0AXWm2xnzlYVrdGkKrG0Fa5ODK1GdStcFIfpnWc4BMYhTNTXq8HlIS3ZXqiESo4Uw1SgVErOT6lyP+Tfuqpp24auUVgjgVZd15gcWJU9bmKiUqLfHrPEbEQRgiL4QIqeyliIIiOsdU3MA9/LeTkfjjLfsmfnvwGJkGb67vEz9brjBxp3pVQL/BbQ5Nf/k5TSf6mUeh/4FeAc+F3gP00pjUqpBvgfgZ8HXgK/nFJ6/P3+hhS+wl1mxivP3DFMI4Zwrkgbo9ntdnmKjRStymfUVcUwhjtebkpryiJYS6MTi0pnUaCKatGhuoa6sZycLNislrRYUgr4OFJXhqZpJSUuDT5RZmXGKCJfB584OJESSEkwZil8ijRtDIFoDOMgKV8MDpOx+WL0PIHjbsfjlExSZxlOkJDP8/lBikZRGUWdFD91tuHn1Rm+7XDPP+Ffu8CzZ88Yxh3vvveIdtFhtJmN1fQwzhKwr7IuyvezFvhMTS0/D2E2+McbsXT1FYomyHQaoyQC8n4kkWh0ZkEYRVfVGbKaM7gAovWzrDhbN7z54IyQxbJ2ux1X2x3Pb3pub2/Z7XYMg0eHJt8LWdXg47y+qfBaZJ3HceT25oYPPvhgWpt75/fuFL0STHpBpcgVYsywH9P1Bz7NPJoLeoGBY3kNNQcltkzcEfE5V2oGNpKUwdQVNiQqI87eOSe8/eixVrB1rRM+SLOdKJUGTk5O0dpw2B/QUaYhBT+3icvUqYr1uqXN2vXWyKhLeV69ZFvIXk55PGHhaocQZJBJinlknETGMR3w3lFVSWR9tZpYICKZAMZK89REF0wBkx1ucCNpOACwbBrOVifYrj1iTmlcUBwGz2E/4HJtTOoCjnHsOQyOoGqUtqhYWHgiekbyqF5mLmjVTyiBjo6mMjRNQ9d1LDpDXaup8N1UlUxyy/UBgUs1Cpv3hSYEkS8Q6Cag0khxbDZrYhXdolIYF63A4451qR2WkZHf7/hhIvcB+KWU0lbJUM5/pZT6F8B/Afy3KaVfUUr9D8B/Dvz3+d/LlNKXlVJ/F/hvgF/+fn9AKaSRKaeTkbvDerW625qrlJqGOistE8V3Ly7xXjbUYd9LW7MSrDoERwqzIfHeE9KACTKMd1HVbHzPaRg48wPBNrw0ESrpZlWDQ9WOqqs52axZLO+xWHQYVXNzfcvhsBfeaYi4EBiDw8XAOIJ3xxX60n2Wpgg3hDLIOuXOyUJ8k4clBT03vTDXFlLG71OKxFicQUQpg9bykFWVhSbyNPQ812sOL3vUqNDtgv2gML0nKBE8S9FPkIDK4+/KcSwlUAy9iznDeAVUU+SmixDya0rIVOAriCoiPkkEyvq+RymNsbWk5baVa0Fhy70GyGtlsnQrWe5WICpxrpXR6NqybjfcP13ytnPE9CaHw57dvud2gBfPL7i8vGYcAw5p5okhF/OzaFqMiaH3PHv+go8++Yi6q3HBUVWWN954gxcvXsgDGCQqi7lyGmPCUXHwahq/F1XJvaTeoqNGJ5U1xh3RSfG8UGdlDVNmxFh01kKZ9OxJjAFiCuiYSHji4PBBsi2VoMKAMpik0SmRosvsjMh6uWK5PGE8bPNIu0BT1zQ1VFbTdh1t09B2VgKH5AGHrZqpvqK0OJSSUQqdZzZmCg8pgLakPAc1pEiMYnz3hy0pbUUj3da5B0HqSE1b0dRW6jmEab8rY0RksGuE7dNkOqgfBd4MnhjAKsPCRrqVBgwRQ2QJSSCQ233PTa/ytConEX6pUCrpik/REMjCdiTAMgRQg4Mbh1YOo/zUP2Gsxpos+FVVNHVNVxmBY7TMjq2sZMmF5mh0IGWyhHI+Q2PZRsTSMKXyOUnhVlHl7uu7jYevO36gcU9iRbb52yr/l4BfAv6T/PN/DPxXiHH/D/PXAP8E+PtKKZW+H0CUwMeELnQ8XRympHfFa00NMSlC9miFKlbXNefn93n8+DFPnz1jsz6hMpa6benakf4wTEZVYIBAYyqqtqLxI49C5Au24g2luY4GbWrsgwc0naXtLCebJaebdqrQp5S4ud6z2205HGSSenLgQ2JEDNvgIs7NXNeU8dc7EfJRGpxyMbRAIQAhqokBMkV2Ey5r8gaYoYZjposxhn0KfGNU/M72Cu88F3WFw1C7KKJdaqZgzUb8rnxAuuNQSiSa7pznNF5Pa8gdnyGESV3RGJkrOZ9nnOCqmb8//02ZRsMdp25rQ0U1saScmxkvEs3Y6fySyl2EWozEsrGcrlcE3fCFd97k+fOXPP7OB7y4lnF+UStSMtkXlXsQuLnZ8vi7H/Dw4Ztc39zw7Plzuq7DOUdd16KC6CXS8t4TE8J88iOKXMxNhcmSu0aTFu355LPhTBgl9EatBONdLVsWXYcxVoydFVpszLi+wzCMjtGNHA6J26tBDFGc19KYsh4OkMh5sznh/Pwelamoa52ZOYI1N0Yq6bLHEiE6SnAoa+6OYDghLFttMpSW93eQYezWSvHWJ43SAhmKfEAnEX+BxaJMORrHURReQ2C3L0XFGlsZbMVUn7B36hGl50WJCmua96BCEfNaxsQ0W7deNWzWHY+SKFT64BlGxyHAoR/Y73e40ZOCQJxl1OBR7ixrmhQOgwpIADM4kpsbp6zR1ErnIEsK6E2TaNsmF9ahrmYtm5yDI7IGGq2FGh4Lpq80o5du9RCOhpB/n+OHwtyVUgaBXr4M/APg28BVmjUnvwe8nb9+G/gwbwCvlLpGoJsXr3zm3wP+HsDJZskYs4y4UlhUfjilTJRilCgl4+uvGpsyLPbhw4c8ffqU66trtLbS/DCILrZ3s4iU1hpdRttpz8mw40u65t0QOVGQjCLlKSpVlM8unXXlIT4cem6uDzJtps/DBpzGBxEIGmNgGB3D6CbjLoOGj6fXz8OCZ0Mq17haraSwmYcaT1hcvoa52EZ2GjFztUdi9FmrXrRm9i7wxDlpxvCWKkVsFRnHgX7oqZpqOqfjNSpOo6x7KUaHGLH6bufsMXQzcXJjnLBB0eWX6xCjGCZsUTZ7wWplrReLBTHO1DyRjmPqJpXimM3Z0MxZnou5Iivro9RhrLXUjWHwiaaqad9+wL2TJd97esmzZxfc3OzxLuECxDRzosfR8+2//A5f/OKXqOyOP/njP+XJkyfT9R5zpiduMwObOqE74XUX2Ecp2avaVBn98CQ8Rmuaqs3GTIpsJkd7sksSVqXMasqUTR2orCKlmpOV5WS5wLlEzJCfy87bGJkRayvNctFx7949GeRS17RZL74MgPFJ/tbE/R98Hgpd6gFzjUreEwkIfFc6Jitb39krKe+r0jykNdko52jcalrT0IU6/101wRbDMOC8o8/S20ppFl1H18owF4WamgSDn5o30cqARmR8lZLsKBYVyYBRUFcJNzowitRUDNqQaElpk58dw9DL9KNhHBgztDQFh0kLM6sUBpLCKDNPVYqaES01n95jTMToAdTtZJTb1mR4txEShRGncBe+U5NjlIErIWeM8VPQ9avHD2XckwCqX1NKnQL/FPjxH+Z9P+Az/yHwDwHeeOMs3e56TI4iGiWFC7IMQdlwrxbmYMbqQbpH67oGBf3hAFomzT/kis0AACAASURBVJi6BZhSKDEOnUh/+j0/Zg0/iebMiMri49gTlg2NFcnXppHUahgGtttb6Za8vOL2tpdpOoWDHi0JJYO2rYgXhTgbx1KoVIXSaC0xzdGxPERyS6aOXSXDmkuTSGn3LhV250JO00QJszT1TBK2iC6GM4JzdcmzdDV11MIIicfMkdnAhxCzEFec1m2OSgyBu8035SjXNjeQlNFw8/i28nnFoR29G2PsJCMQgptUOwVuSJPhkDVVuRN4djTHzB6tNTqns/J6g9EeYyVdX7Wa5WrFO4/e4MMPPhYjv/fyQOb16PuBGALfjo/5qHvKbrudOqJ1Lqop5g7Nuqp443TFF96+L3RHpTCZhQUQYmA/DLjkkSKny45L9kWZ7iO6/oXNpTCpxiZzZDA9Ondaa6VZLKppL8UQGZ0UZY1NdIua5bKTfaykW1vos0dsphjwKVFXNgcwjroxGCNZlAzb5m53JxqwuFHgFsnOjvZMVU2DWeaeh1fZUpEimSFt+zOOXRobh2GcukCVgt22Z3uzp+976rpmsZSJbQKPiIie0UbqMYC2NcpI74xWmpSL5baqpmCrIf99peials2iyc+dTOBKmEm+pB8Gbncj+0PIEIvUIELypFQ6TVPOjAVWdCnifdmbkg0MbkSpRGIHiEMvFG5UKbbXeT0s1mqUDlMT2/+vbJmU0pVS6jeAvwmcKqVsjt7fAZ7klz0B3gW+p8RSnSCF1c88gg88fXGRL9DQGUNb1yyXS4lQTUSr+BpjIHFNebjGcZQuxRAZwkC3qjkcBhZNR1VXkI71GxTDOLAi8LX7j/iFVGEZ2ZlI4yL3Ht3n5OEj1pslJydL2rZCMzAOA9c3N6yWK9woVDqJHhREaR5JRpG0wkfpSjVahIkkys5RS4ySwgcpvhUDPY4SaZaOyX70OOenRhGXC5UTdzy9qg84F6hLJCR7QCABZxTeliHIOeoJ4cgZkCNif2Toj2h8pZPPzp2r82eEqY17kiXID3oIHq0VbdtO53ncTaqU8LLnNmymGsA4jtR5eIHOMBJIIVMpRdPUE10PyIZCUYSgqmywnPOoFKmNYMlaKRbKUNcVTfUepycbPnm559mLS4ZhYBxF+dOryM3Nlu32gBt7rIrYXAiNwaOT1BaUFuGr9UKUOlOetDPsd5MqIimhraGy5EKizbzpnJ0UnfxY8HX5nwpSVPT5XlSqISk9UT+VyUG9ApVptALneUieYdhTJBV8yPWPKPsiBHFQTVtnplfI/wp8VlV1drZ+yl6VUhA1VonRrGxFjAH03EovRe8Sxc/PbF1X+f5nep8q+vma4BPeSYaTYkQbmU/aNhXNphXZ31ggvx273Zab61thJI1uinpLE5ixFlsbgrpLxW1sTV0bwMjeSprRybXVTY1Lnsookk7YZQVYYoSYGhRrkjf4QZqtnHMMYeRm3Aqs4xzOhzy+MsOww4hWNQqdB5lLs5NS89qkIMFacTICkQ1TzUn6D1y2ieYVW/jp44dhyzwAXDbsHfC3kCLpbwD/EcKY+c+A/z2/5Z/l738n//7Xvy/ejjTCdN0aMkNht3fsd46ra+k2tZWiaqzI11qRY62z3oqxZlr466tbxsFDUtQWGqPYRYdFulD7Ycjj6jwEhdKRrl3ywd7zzZBQFm514rurBaZbkFLAaIP3iug1p+s1qwePUFroTcGL9LAPgXEc2AfPYRzZ7fbc3t6y7UcOg2CJQz/QH0YOBzfN6BwHRxjjpAk/DANJCQ+6MFZmTfjcdSlhXX7AJHqVSFdkbb0PWcNFmELBB1IIExVUpYi1MkE+hkLVimhm465U5HU89/K1EssrG3Iy+lLF1xqqyk4bVmoiVS6yzg+fya3iheEh3GKPIhGdbG6jNbWx2FZPtQZ9tJUmGWIlWKVSYozkb5rMOnIT/9+oRLIKU1m8l4el0hEVAstWoc9bTs7XPHr7lKdPX/LxR8/Y7weCHwkaktZoPBZHpStUriGoDLF1Tcd62RCIDP2AgozrjrPDU0ea4fnxm4StlZoCBZnmVOonCUyxxVLENLkZqbLZgWeOetHNL4qLcg9kTnDwc0PScBjZ6QMc1Wj2tqKpG4kU7azXk5Jkp9aKpnmJ3mPwoAeauhY+u0rTTNcC94whTjCcZDtMgcMMVwrk2XUt1lTEMBJ9cf7gnKduGrSSgKfo7jStQukWkOwihpF+2LE7OGIykCxaWaEI4/BEHNLjctYuaHKgsVouWLXSZVpVFo3GhIDNUJT3AodYneURAF0psIkQNcEbku448Rb0mWRO40g/yAyFIVN1eyf7ZHSSwXitiMpkaDGgglAkp0dPAXrmygUSKYBLkNxRM+RnHD9M5P4I+McZd9fA/5JS+udKqX8L/IpS6r8Gfh/4R/n1/wj4n5RSfwlcAH/3h/gbmf+tMbpG10XfInv/KBHDdrvLr4UqY3hN20wR6PPnz2XgL4I9FsGqYegnWOeYVqgSfNLv+Yba0RhLS0ODwcca9eELztYDLy+3LBYdy0XH7ekCpV6w3Qrt8upiz3a7Z7+XFHHvA31WKQw+cHAOFzM8oLKmTYYnUsbpUpwVE2OMpKPiKEDpsyowRCKgUsRUFWdnG6nGK8Pl5eXcyh41ddNA9JjMPpCCTpOjvorSyg0lxVYTtlqw73IO5ecimFRBLnhOdQAk0ir4p2RGmeKVNbiF5qUmWCWGTDc70lCZ7svkZNSE20/neQS9lIi8vL7AQSWLM3qGFeS8pHBfcHvkSsXIK0O9WTOi6RYdZ5sNbz98yEcfPeXy8krqFykLiiGOYe4gznCUtQzDQIhMUgF1I9IVd57E3G08XW/ei8fXEnNUXVWvNJDZsh/mtSq1moldAVBZyA1cSimSUrgj+rCchpx/WR+tTVZcjfghTBBa+W+3cxRdeK01tTUs2hpImDArLVZVNdFdp2J4hkPLOZbayzGttO8HFEMerj5fS5lrenNzPe2nsteslcjXWstytWSxWtLsB25uD/QHmaHsQ8SnlB20fObhqscYcV6VrbD5fNq2pWlqVitD21TozLlPDFPxOcaAH0equiIRqbqKlBQu5rpPXpvTTe5FSYnEA/aD1NgEVh24HXp6J5DXOIwEnyY9eYAiHDYfCmUWwKvDXV5//DBsmT8EfvY1P/8r4Bdf8/Me+I9/4F8+OpQCW0npCaswUShm0t2psaqmbuqpyOV9yN7wQLzdkRIctn2e1iK68FHFaZLM4XDA1M2diMIpTZUUBMWlFdpdNSZUdPjeoa62fC8+lfSHzLH1Y+b35mKoK5CHUBR9fsBC1quOqdDrZGBIzq/l9RmWkUggP5xKNELmLkMZRqDU3aaXphFDfXp6wsnmhN2u5/r6eupiWzQyM1OcipoeFh8C0QdUZlRsTjbS7BVGrEpHxtPMRvgIJy/nTQk8mR1wMarW2rxGM3undEHCzIZRqEmpsnzOcUF07sycGzuUUlPkVj5nsZAhweX3xw5HYBEmOElp4f5DcTIiLFf03AXz9bSVwaLp7p/w4N4J4+hyrSVLCeeCH4hzqI9w6JSSwIhHYwjFKcwNSikm0tGM1JSSQHpH9aNSWir34FhIbGYWMe0JYVdwtGYzPp6SQADFmJbsydoZqgSRXig+qBS8j4vppRM6JRHEUlrTtM3k1MoeLesZYqTKmUD5uUCMTIZytgG5rgJ5QMbdhsPiNMpR9uRxvaqIb53dO+Hhoze5eHnNbjvQ957BRW4OO8YY8DEgigARm6D3ER0MJIfWB6w12Fp6BbRSVHXNZl2zXi+wRnB9azUVIjAWFFmuYnZ84zigCdi6lkEfEWpb0zQtiZoQOlyUTDtE6bAeh0DwsN1tGUeZ3Tx4jy/ieEpGbpb1+kHH56JDVTYi83Qfn6aHQ+AIgSAmYaWqxrZLYkrstlv2u12GPkTvIniPquYHvR8ci/WGPhv/oqERXKKKhs5blAMfHb1KWUMAhijDBlKOQEYXhao4L/GdgKyMzCo/VknkB1IQxo9SHMnzZmZAYbfmh71SHEXQ5Ok+sz5609Scnm4Yx5Fnz17w5HsfA2L0Qgi0dcVm0RHHXqSk0qz9rfN1awxJJ7q2zbj93aiR8j51N0IoD5lCSWZw9NoyL3ZuapJIUBgnY8YLZ4Nf6iQFtimHtXaaj1mKs8et1qXQVox+KbQdF/qK4RDjru9EjSGmDI3Iuo+jn6JB770ImRno6hqVLCEmBhvo6hWja7i92eE8kyFVSlFlhcZJ4hWHUscORyJ5eWDD1J07OTIlg91VnHV5SjFVDMU4fR2zIVDqLkuorluJHp2bnHzRiKnrmqQ1+90OrfVE5w1HzC3JlPVUYylOZXJiR8FFuU9VVTH0PTHfD+ccTdNMezXFyOjc0fSiIweXjlRLmes+ta2m70sh/Hhfkdf5+L0y/3hgtVplRyYyH+vNkq5bMDpPUrAdFtzsdzx78ZI4yiPY56xQB0PKDXA+eJIW1lGBuSyROsNVm82G1bqjqgQ9WK1WnJ+dsl7bmRYNRGUIaFyu/Rg10lSN7IGxp1IaTcBUhrPlmhBFqyilTc56NPv9KIFbDPiQOARpkIw5yP1+x+fCuINs5xTKKC7pUhPoArSu0bqSinSI+JgYvGhG+1x0qfOmqoDFomPRmKlSHYLHuRFrpD1YYdAOhhC4DUKNijox4Ikp0aWK2Ht8GQGDPMyi2FiiaGkemY/SeDDjp9IAUn6XoQsEOijiTrFUzJAB2tbIaDyJhBMuhDupmXOBy4tb+YtROK+jG4BIXVcsVx2Lbja8RhvhVCOTZmKKKOVYLZesVp20YpdQPEfUIuUapsgwMT/cE1zkw9xchPD3lVJ45yfIoUyZkYg0R5H5dYXepVWRS435/obsFHw2/PVEtStjzBKijFhXtTyAScSoYm4Mc3nT13WFtdWEY8cUZX4laerIjbFATIIHE0b63Z6YelLIa5GhiBTjJC9RNq2MiptrD1VdoXQzOTylVK4rjNmxaMj8d1kHC0loeZv1muVqJR3POdGbGB4UKVlxUkMeEj13KopwVtEil/3j8ni2yOAcKkohvt/t2N7cyD0Dqqyn0lgR/JLmHoHd6qrGWoNShq5rqatK9pTWGJ3o99tMcZSoeb/fT1TVEAIh9cIm8WEK2GI6+n3OVkpNIsXIarGcnO3hcJAsITud8vNjZ15w/pKhjePI4DzWFFjKUzeabr3kNHacnLTc3PZstzu2260QGvKkM20VVaXwqRb/qubehDEm3M5xu7vGPL+mrjWVFduz2bxgs7SIfEEtvTfO0rYa5yJd25DCOAnEjeOY57YmKt1ADhxjFJhwUdcMY2DRGqxZc+/ePaFkEjj0PTEExh8wsONzYdwVCsusEhh0bpLJ1f8IEGR83Bhk8rwMzShYLFRti0+i23J2/5xVDS8ur2ROpobD7lb4pJXh5uaWcYCQ86mQZJJQ8J6qrtCNzZK9Mz1QHE1E5yG35GinYPg5NqEAFuKXJvACAJ9EOky0ogVPi75EZfKgL7sFTd2w3W4lisi0qWltEiQ/QygkmQqjtGKxqFkuGsia5LM6owICSUUZH1ePPHp4j64KJNeDFUniGGZJU50LnkqJZKxWs+Y0QMzt5HOvgXDuTdaslmLmHPWZrIUDZAnYsvWkCCcRaIVMAs27Ij+4cq1GnGXJilD4BDEqUEI5NaZCo7OBizifsg5Nyucq3XGiyQOjG7m52ee1nhlDNhtwgY+YHFPMLf11LjSilIjRKT3h+EULZBzdtI5GK+kCRoO2WSFH4IYyQaqylrbrCLmFPiVhpxS4SwqcCpACnKlqjOlywc8RvZyntSW7SrmRiGwQQx41mbI8QZgU/LUuapspG82aYaxQQNt2rFZLyUiiEu2YlKaxcPVmMz3HJcsqfQWLxWKuAzXiOHyUeaRa60lZsujuAPQhEjPlsNRQ9vv9lKl1XUdKicPhMO2TEMRZ3N7eZmdXEbMTtyarvQ4jJkRUipxUDav7DfHeemLy7AcR/PLOcXO7ZbtPMts3SwGoLMFqYm4u8iP92KOMIbmRZxcDL7dzxq0UrJuWrhXphrYZ6CwsOmiaRrqi3cBqtWS5WsnAEL9HRREnq6mIBDDSFObHW6JzjPs9bVXRdC2qm0XxXnd8Low7UJ5Y+TLruYs2g9DanBMe9ziOU4o2Cewk8DpNk1LcOJKqevpgBXjn0HlzrFZL4mJOO2OM1O2aT55dEHygj/3ksV9Xt8hB1ad+dlwke/2bAskLVFA+uNDoUpTo98orUuwnnFVrk9Uls2EkoJVDF0eiEl1bsVwtaJqKDHaL4ZmMq0SLicC9e/d4840z1qvlVPiMUbr3SgR2HKWHGFGEaepMuVXHWkDHuPsM44jxKpzwmKV3j3Hg4/8KvFDOuURoBXMtEf28R8RIWVN42aLOWDj0RaSr7JVhGERB8LC/c99JhUM8X/tduYUsP8sMXRV8t2QTLjNyyntsJXzyEpmKk5TWekpzT4qcn5+z/v+oe3ce27ZkTegbzznnemTmfpxTVV33gpDaaOFi4iBMaIHZCAkLqV0kDGwcDCxAQgK111ht4OPxC7g2LhJ9L9w6j70z12M+xiMwImKMubLOqVt4u5a0tV+ZK+eac4wYEd/3xRfnsxwAnL0y5NIVUprRppygAymcM/DGN/jFe8/eNUYb2yoMCM6ZRt5b55rSSBu8Si1tHTtrGzylUMMqwz4az0GsYDK7jaHPS5/rKFDfnkMhoubWapyVvWXaWMx9c6I1pvEP+r7zPLeJWzoEZC/LjZGb4FTYME5HQA5cY7hi02yfiKFNSN+Kl+89e7TGuOVlRaoRPkR8fX3F/X4HnMPlckWWirIUwnJfZU6vA6EiJ76fajn95TrjzW0IgSujaOVAjFxtmVrx8pTw/CxePmFEHCuGYUBKCdPxhPnrz5xcWa4wx3HkxsDy6Jf1S69vI7jTLjsGS84qMSyRc8btyhafumD2DTUkILcuEMXMjIltUznnUGoPNgAQB20o4oebimuL8XTiIQrzuvUbuMtc+d86jvj+9atkBxEMlT7/FYDBrtOMEnI1zUCJUOGIELzaADsMkTBG/UysOokDww9EPHSaXX9txz6RYEzB6Tzi83cvmKaxYdOsasjtXu6Du5J4RRqndIPC9Ak2+vXalaqfnzdQx+lzLQjOt6/TINpwfNN5BQ2++u8AE2jqLQKgEZOQz7on+mqtbaNrcNNyHYbtoFXR4WxEfwD9pT9fH//+ANLr40SDm3/e/z+wb17jqjDLfTweRnz68ILT6dSuHYBo6zV5yQ+fPZf0cIjqFJ9uvAYYr5YUDqCCuvMLYpuJ3AKeAWClQUbxdeUM9oQ2wEQsAORthbW89g1EkeQeA61m4U3yKvt1miYsy4K0U+DEGDFNU1s/qhpDpXbQERHO5zMul8vDKE5du7r/NdMPIfD0JtMb5ajUxklpdRG8x0F7LioBKKjLDB8CIggp3VDKgpfJ49PpBcZ53M4jSqkYYoTzA14vd7y+vmK+37FsBd0Xig+/AoNagCTw7UoEIMFZgbycw7IRfvzKB9bzOeBwcK0XZBgLfAio4hyrajeuaNNDz8gvvb6N4A60DQiAXdy2hHXdGKvcZYl7pQSggbR3takZUJFWYUBILxmW0IkijrEEwrrccLmpGQ8P3K6/cij+2mn5Pp4rLi9/A8C2Ss50MaZOwPHOc+YcAmIYOPgIzjhYYBDrUecdvCUM3rF5lvyEdVnw009fuTOPgLLbAAAbLB1PEz5/fsbT0xEOj1OXdNE0T/gdAcYKhq5W4eBg2cRKnodmweolo4GPtp550u4G7TP+/c/TYLWfQwlAggg314j3mGTFFdjSAybbmr8EutPPEUJo8z4J1AIDd5j+sbveviIBHm2L9dpijAgRbYi5Wi0oAdrWW+lBcxgG/P4f/SN41yGuaZqwyAQx3rBs1qUbuXsTdWnpmvp8VIZtOvGozTu27ohx7IZCCKGrz2xPluoz2H9evf/GmZaMBG9RyqPtrNmthz2MAqBDK64nHc45LMuCEEL7GSEEpHVrxK9WsM/Pz40H6EFdTer6nmxqLmMe4sSnT5/aZ8spwVRCWVNTUZWaRAhBKFQRokOtCct6RYwDTDIYDJAoIRoDHxw+vDgcDyfkPLEp3a1bJeRcYRtqIlCjclA1cUJbDLb7DGulG9eMmDeLWr+ilorDIeIwMQx0OBwenhURtXX2a69vIrjzuenYUXFLmNfUmGBjrWiGu+aT29D5ezmTBKyU1RaWT+oklrPEvhwExvi6msM07HZdMz8MZ9sEHsCgNmiGrU0B9k5vF71TNLC0Tyc98ffBMHEYPNsIj9HgGIltFkSjr+QpKx6iTFcyDfuxhrgRidR3Hgg7aMNaHjF4OHxGLRD5WeRRbJJp87QbC6CCSoWTQSb6yqlIZRCbll3JbGN4aox1Fsay+x1DDFz+d2iHEOPYXCs37bSDg8jcUSEWC9YLcVuR2dBESn0jfQv0YDvAg1ccCmpvhoFDqQnrtuJ6n6F+JN6rQsUhjlMLBKkQikwbqgJdVTJtOtb+QNN7S0QyJMXAuA7X1NLH4VnnxOxNeICc4GzvXl3XFdYQzucjxukDpnEEqMJZi3Gc0Nw8rcU4DIjDAANgnm9Q51DvA4YxYt1WCUS1be4GEeUCUOVObNO9zZ330PmhxlkO+kRIOcES5NCltoZL5U7bWhiqI2KhgPWs+a4li0zZwPsIgm0SRsAgp8Te67v9vVf7OOdal63+0myfAy+//+i61YKuBc3cB1FqtYOKSlMGGSNiNyrIOivYGZSc4JxnW+kQMfqITQZj55KxpoxxmkCSCGzbhuPpiFLYrsKJdDeGAaVkzLevqJYTtOAIH54mfPfps5D+XIVl4glsasN8X1Jb9yDAONcS2HlbUb/OeD6f+PAxBsYWOJdYG58Y3uFfAbVkLOuKP/X6NoI7AbclYb7zZBUAD37FSlb2bO4xeyawrEnnOpqiPsiAJYIHUJhh3GVjrBnlk54HABs2nAWMZR0syeQkSnCWMMj8SIUGHKTxYeBpLuMQpIQlVsYEhxgZUrHOIdqKgE2yDcbRFKpwzsI6QtqV9I1Mc9QUGpzFpVYZGJkJezgGTNMocHtf6O0OkWpwe8bes1MxLNMBINU2MqsTer7fawIc7IOPOYzBvPTy0QBIaWUDq12jVghcmfHzdSgk3aPScUzQg7zCOQ9YtTy2sM6jbDOut3sj6gA94C1ccOxnss92hYCtYK8VnThkLOfrGvyVJOdmLNXJSxarh7Z3bRhGOwxg4GQ0ojMG2Rps28o8Rq0Yg4cPPA1sGnhouzU8aaiWxGe4lIkp84a11uJ4OEglwOoSAxJVi23Z5p570u7kWrvOny16CaXyoU87ma8PAXnduLt0B4+ReNVYqTqICHlLqLmIz4lHydzqz3NXed/pXh2GoVUX6v2iMIIx/WDS+6s8S4ebKqx3DXYCERsJS/KmAV2rJIW99BfA9gYpZ8zznauhYUDNkGloXHGlXOFF9z+EAGMdrLEInscIBheQt4whDpJkyLAXZxGjBzbuk+D77LGsG5Z0aZ5R5IBDPLTPyoNDpOdDrsGEgOuNjcnSljBfL4CoxrgCmjHPGcYEeD/C2Y07r1t8/BMcH76R4F5KwdevXwE8BvX3r55d7crNtoF5LqRzgA8WcYhw91+eIqtlKqjju4Z2szbJwMHAgxCjx4cPH/HyfMT3x7ENqAYAJ4b8qgIplrCVbrRfwS3xQAJMhhWMmMvr994rRS+ufVZdyJ1Q7HCGwiQ9szXISRo+JLA3zxvq91khj0a07e61bnItq/f48v5aWnkrMA6rL/qIuH1pr5p12uHo+oxVcqc/2xjTIQAJCBrEc87SfNLNwzQ47EnZfSOQ3FDoDXivl97/XFb49ICp17jHdtVpU8lD/fwOXZvNBx5//fF4xNPTE8bJYdvW9rx4Mk9tPycX1jS3g3KXzRpjQDJlyHv/QEBqJqte6Ouq05+MVHroeDZVlN29G4YBXkjffbU2DmPLjmut4rXf+Rmg9xoYa+GCb+uo1oogHJfi5freCpm63eG7h+b0Weg90TVkDE9V03Wha2Mcx52ZmGn/5pzDbb4D6FyBMaYrtYTjcCKTfP9+uvZ5HfF65NF7vu0f5U68Dw8/X7tm+ZnmZgvNidIGRIgnkdgpF+DlfEApon1/PqLkjFIyYhxgrcHlcsGXLxf8/OMdaUsgcEKjh92fen0TwV035l67+qeYYG786IuB8cMEmILDIeI3nz/jeDrj7e1N3k8Php79Ez3ig86yvEkgfHx4fsb3Hz/jt7/7DtMhwJkClxeZ4KM4LPiAIMViKyoKnPU8ZajhkZwxGAtYGcm37/rU4FRrBVSWhsfRbfo1urF585EQRWjZ0n5sXM+IOwav93j//py51zakeN84oiWlEmC6GZT42xPO75+dYqal8IQoyGfU99zjtfugrNI8VWpoEK+gNqJuTwTuf+4el/wlPPj9/+l76SGhP39/L3QjaWDYf/ZaK7xBu85aK8ZxwMePHxreXPLWdN36HNRugg8/0e7LujDGNE98VtJw09n+8FLyOOfMAcQHeK9OoQnWElTOWUrhwPAuMXASqPQZ6fPVfzPG4Hg8Nh5BP7/ek1z612mWrf+mfEcSTL2t31qxCS+gzzvGyN4wO0WW7otSK8Y4tMRAf9fr0fW8V1mNwwgXfMPoDdCaozShKIWrDL3fmnRwIlFkz/TKlQ8X265TDzNdN9bysBm9v9ZalFrlYBDpc82olqsuKgmohLzmdp3npxNS2nCfK+RycTgeME0nfPeJxKRsxrqtWOYZrxLffu31TQT394Fdf9+rJVRe9OAN0278gmorzk8HfP/hE16eXpC22hZ2LkV0xrvs34bdFRBAFd5WHI4jfvObz/ir3/4OhzBiGD2MLVjXhZ3lXH8PsiNgbKPjPAgxqE8Mm37lwl7ZYOSA/bp3n1HHklX5HiUqVfq1f7WMyXRlHA435QAAIABJREFUSQwDcgbbrnrbPK0542O80ztCHDrpqIFbNxfj1T0r03v/vkMxl4Lr9dquS79/T37q92jXqbalE7EMTN93TxJqINXnre/r5DDSgFZKRpKNsP+57wnYfeWwV23sr1nv3142uS/7NXPfQzytK3Ptg19Sytik+ep4PGKaJhyPB1YLQZqKgBaMaq3IiXH5LunrHkL6WbzbGXeBMEzTg0pHs2MACD7gvt2xbaoXZ88ezXgbZi1qEmu4OctKw43eMwC43W4NelJI5f1Bos8qSeasWb51DjmtD9WRtbYFdz40qFUE+nP0+bTmHsG99eernBZyH52siZxzc47V6o6DbD/UlKdadtp57oIu7Xnoerjdbk3dkxLPRegHf/ex53jVEynnnAxpeUxWtNsaYN+cUhc8P58xDBG18szgaTzAUuFBM5RhHeF4lMMMBePEXNTxNGJbE8bxGeM44na/YV1W/G//+9/g117fRHAHfl1SyBAKiXa745yGgLStyHnFYYr4/PIJL8/PGEIUULhijBaoK0wtQJXWkUbKWlAhmErwxuJ4iPj9bz/jN7/5HqfzERZKtiXBigNgQgsakFJPOx25NAaquMDVUnggsQVssC1IVYBLcmORcoWJMpHd8Ug569XPXUz5KzdXkIxwy5lYIikbzLmAKQYQiLsRVXGUMyyIm2yIHTAr8UaEKciVccUKvha1oivacYmdV4ZcK0AtO7EGDfflqsShwkK4SpRUUJatHVIGgHEWt/kudseKcYsLonasEVAzE46MIUv1YvkarYzrI1L9vkHOBGN001nB+JlHINJOWbW06AGfCVP2Nmd7c4b1eOoXY5/sh+OFP9gwDiNQDdK6olT27/aOOzfP5wPOpxNnZpVHJ4bgUIpDDB6rePAb41AKEIJqng1CDO0QU4WNwjuVCDnxoJdhGFEL4+jGBJRa8XadcTwekWtCSQnjMGKaeF3FwbXBDkR82KZS2KQvHtrhta+cGtxlDbbEHicxRlTDe44990eMdurQZiqopiDvhoEbsO2GBjh+c4NpOAIQdVaWTFYO+VIqYoiAM414LbVilE7lNSWeADUMbNa3rnCmIss0LNoyjPEwxoEKtftaq1YyvBZqZSdXDtq81tKWQV44DsOwlm/umGp5Aek2Zi9PGMAUrpB17uu23DEGD7LMO/EeAACHkghzWRFcRBwDlmWF9fy5TDY4HrkxbasbYCqGiT3nDbTyZWdZqgVD/GXYWV/fTHB//2rZGWSzyV+qDJhIKcF54Le//YzPH55xCpw9GEhjBmV8/90HfPn5D/jy5asY6WtG42Bs5ow2Rnz+/Dt8ep7wfAoPJSRa5dBvYtjdsc74A87K91ggDgHOs6dE0fcCe4CXLCWcU1tc7q5kL3QP1R0oMeq9OudxO3luuLlDShvgK4zlgF5qJ6aMVQsEg2Vhb/JSCoYh4XBi06dSK2CY6FlXLqcPhwO8D7heb22TG9u92xsXgILoYsMgc+bhEnxtVhY+/76sKmddG65s5VAkydTNrkppf7LUYSZjRNMu3apyj4g0GPkG1ewhOB0C0XH5feWnnIdpWTLQpz2VWsR/hQPt7XbD4heR2Sas2w2fPn/E0/EjAM4qv379wuskBCaWRbJqJRsdxxG1GqyrShm59+I+31og4YApNrDGCQ6bAKOwomFS1lnUZnqmHaoRMAYpZ9TKJT8RCR7NVsgprQBZLLQAhNZJqhCCclJEBB8jbGU/Exiupoxk/sxVCW6e+cBYxT8nRjb7c8ZiXRYMw4B15V6JTT57KRWH6YhCRao34kHdqSAb5XIKnl6e2BJ723gtWgtYCx85kSN0nN57j21lCfS6roghNodYnVeskGDOYgdNwBAnkNcBL6IAshzcFX5hi4oqlVuBC7HBPEMImIbQ9oORxC9nvv9DHIAQoIPT7chjEu/z3Kplby3WmauUtG6A4eYmaxzSlpBTQggj1mXhPeT+QoM70Actm8otuFTR/MnPhxG/+e1nnE4DYnAwVFlGKkRSCBYfPj7hn/yTf4wffvwJuRrEODSCIw4OIeowXwPKCVY8L1pnI/UmDi3D3ytNFObYE46KbZvixJtEyDMfWXIpGRJnOLGVsb9EnO6lYnsCUbE/blHvUIMuRH0Pfek1b9uGesky2ktNwvtBdbvdwG6Jof38hik67RrlwLiua9NUV2n1v1553K4GAP1e/Txaautn3UNvCn9o+Ruif7h+VXL0XodOKO3ho34PO3S353D65wIgw7FheH1YY1AKyxetZ5XRPK8NSrpdb3DG4uOnM377u89w3iKII6MSnjnn1tXJkALh5eUZxhhcr1dYG7DHc2utUubzNa7bhrSljo07h3mZ4YRA1coxiHRVP7N+PUORCd6ZB0hEs/RxFFUV+P3u9zvGcWwHgcIJW06sqimMka8bd3krubptG2rmrP58PrMixnKlHWPEPM9Yt9T6KIads6uu6UqdGB+GATlk9sKXe7MuK263W/tcagin8ArHBx4ws8zsTx9C1K0L7WrVg3/f7KR7e103ITFjI8x5TjPJobSKnYTtkmXnMa8Mj03TxBJL203eYICy9b3Me1Z98f0f9ULofSmZifMYI+7zjGXJAAHzvMj64Pu5V2z92uubCe66+YF3ShgimCJzCUvBaTrg+88fcZwCfDCwtsLUBCt+IHyjwM1CBHz89IKPn555Oj365lcCpbWIewcrI+6stTKIt2esqlbQa9Mgq/+2D8DrujKeah2s6XhrqWx81YlMNXrqhLLiqHvsWn/WMAzIcr2diecpRHs1g27mFgjQJzNZa5ETWPZpCCqr1EPJOYdNPOl1yAKApi3m+6NDDLKU9BWpkpS+tt1XJQn12e7JzN4B2i1+91jtfqB2I+1KhZGWeiY4uz/8fs3oM7S2a7irsN/v1xjguTqoQE4EgFAL2128J41PpxOCDzhOE15eTjCuIkaPtHQHSx2erSoLxqjR7vHpdEKtwLp2KV+MEaWm5k+v6heVFa7b2mSQKput6IecJiyakaaUACrwzrcDWJ+lOjemlNpIPV1fehj1ipQJUoDX6bqujTdJKaGk0nB9fU5hDLher+1QtxJI36uzQgg8cvB+h5PO2nme2VJXkjrn2IhtT74bY7Dl1PZeKQUgh3XOIHIoWaFM/trb7YYYY1vPei26pm+3G6976x7WmkoN3zcKafKUdyQ0Z+kMxenBsa4r1rTCOu6v0WRqf291D5zPZ15r2wZr0CaxGetwfZtxPB0BeKnsunCgKex+5fXNBHd9GaiTY2nZki8FwXt8+PAB333/HQ5jgMEGUGlzC3PmRpyerHIGz7gO8Zg+aaJxBjAYwFMAC6ypIFNgxLrEWotiu/8L0BswiKid7hrwijxUfVjK/m8lc0OR02EUBpCMVSVmzg0towLQCKQqTLv+7GVZME0TphgfTJV44/bxdE2KKIsweo+Ue5Bi+CIKJr1T+Ug2WIp6h7PBU6kVUVq62wGm6iQNpI6dJ73nkYNeBnrsK5z32bN+Vg0q+mdVpXCzC/8MIkKWQdL8tVqVdHnbo6RTddE8IyAJL8ALghpOHEIEyLL+2DukNTcfj5rZO+R2v8E5g0+fP2EcRzyfnxC8R60b0jaDagSIp/e8l2EqzMWj1AiHwyREYG04tw6NNrailJ5EqDU0qz364asKj5RZ+7+HEXUNViIYIpFlqhdKwTxz+78OpajihBiH2CS6wzDAWg6qcOxOyYfCBhoH5NSHn2uz1jAwMQqDVtlxQuNgqN9//uypNeKs2wrIYJPcEh5eM8PIGfPkJ8Ci7RfNfPVrc8q432cewWgtCgpC5CBY2z3g+0x47NLVfZTEQTJJxbVtG6xjl9XT6QwAyLkiDgPyzp1UtfxDjLCw2KRHhyunjJQTxsCKKSa7XSPMo0yVu9148M/xcARChTOsUMo5Y16Y79iWxH7wY0Spa0vC/qHXNxPcHSmZw2TCIl4Qxhg8nQb87vMHvLy8IAQLZ3nuqLGAh0cu3P1opQEJBlBBoS4I60WBoVJD8a6hWmC9Rc5MXIUQYKtFhc54ZHLRRVEZ1IphYIMhkgVUiMnFaTwhbVL2xQEhJbjgcb/PEkzEW6Vm+OCky0+HPFfkvEKk8DDWckDddWqmnJvNp2ajl8sbiHgqUZTO1EJMXgLC5APwZJkcI85somf838nGcoYJIkar++cdxpEtVWtpLe8tu7RW7GjFAM2IX3pmAos3qt5BtOerWPg+m+csrGObAMtGoTGcq+RGPhrDUk+We3IDCvcOFB5KLjNCCdQwfIaVCCVVHkhdpZoiQkmF59Nm1TInOAscxoBhiDgfpkb8GUfMJxSPn7+yZWwIAS/Pz7BCyqjHjcpTSyl4fb3xvReYT+GIlJIERpVTGhxPHBQqCEnkhSoh5SB8wLoygWpgsBGrVWqtiEOEd0zwz/cZxkWsqcKGARC7bEorB1nPQdxaB5QKa9G0+sZqppgQg0E4jEip9v6HnFDShnWdUatFjCNCHLAl8YeX+7UkbgYajwds4ti5lg2pZpxOEwd8VW7lilwLUIEQZYJYHFj8ACMHW+ZmJzgEH0ED8xQFLFYoqcJkMQKLsQ3/SLkg5xUwQMq9AY4IqNjgnYfXAJ65efB6vWEcBxgX2KjZB9SU4a3B6IGcCUNwuM8ztlyxbiuGzM/AOIaynOgavWM4rlbC9XoXtdIAG1i9U3KFH0YAihokBGfgWUCHkjfk2r1/9nMQfun1TQR3IrAvu7XIOSGlBVQyjtOI7777Ht8/TZiCkfJWx8N1UydjGXPvaXvHq0nUEnt9N4g9RVSeWKnI1HSRB1qRXO6qHmsMgjWyiCOrSWTA6V7e2HBAw9IzK1iibmaV0zG+qcQxB0HNvlR21rHl/nlaowvQMgAtW0vhABWGEVEc92DYStgRENCJ6hgjiisP7w+pIPaSUS+luZJyHScXpYqWhrvGMt38ej9VN84lLNq1Az3r7pDMjlBGz+yd40EJkPfPOYNyYQEVSZNUTmrfgdxkaV3qxsRfbZDJ3sRMoZSUEo7HI8ZhwOl4xDD4RqpxtsljHPlZBhyPp77WIFI9sY611mKeZ9H7p/bciBgS0Gtw3gsWL3xKyhhyh0O06lIY7OXlA/vpA6hyIEKgEoUd13WFheq3I3KZkVJuFa7zgRVLxjBRCbZlTrngdrvJM2I1jw+8R2Kc2sE0DAPWBcx3wWNb2bBOq05VyOihfbvfufvWsYywEtsYO+uQ8Qgh+iDZtdhNbMvGh5Csy1SLHKh9NqsO82GilecBJFUBGcuEJtCSJef7JChV6eS8whiL4/EEooTg1R8+YU0LBulbYAUTQLU0lUyMEeMhtt4QTW720ImBbbMLuAIreH19hbUW0zQhxhHLsoCIq/lBut0BiNOtB2xvfPzLaGICYasZJTGDPXrg04cP+O6773A8HODqBpIp73uSqeF4MgV+r0vWRnMt4/aLbQ8XaKDQTb4nJIGe+UM27yR645yzdKCaVi5bPhVaoLDWwMiDA/DwMwDWvhI6SVpr9zbXg0CvTyWFg/hiA2ibWRtN2v2R/wMUTvrjBiPFNfvX9O4/oBOsSiDp5tN/069pX4tH/FtLdz2s9gfVe3imY7E7qarpTWZauTjnW8MYJLhip1MuNcPkHiT4+fafyQdmf2/diOoYCLByRG14+R5n1NrhNj4E0dbIvumryWSpY7PjODZCT/HePWm//7515WlC5+kE59EIzmEYmu6e38tCz0Jdf9UAm8wKJjnsIEmNumPqs5jFpGyPgxMRbq9XTOPUdOsGrq2fODC/My+5JRO5ZETvcL/dAbAz6bZzr9yT3Kz3J3jjsCxpxzFlXK9c2Y7DCOcdjsfOW3jnsS18GN9ut4dOaOUQvIsP3IirHdozhjt8eW7p0mARQ6YlFSWzmZjCoFvaWF6JzhE09RmAeZ4xBN/cXYkYmlyWpcFnOadWsOoaybmAiD8L1d5noPti29ZGyvI95GpfYaVhGLAmru62bfvLMA6rRLitCwyA8/mM33884uV8EuMrmWxEO59xCYhNbUC981BbwM37GZXoRJ4eAnurAw2eewxYA14SL/goBkzW8pAFu7OO1MW8z+DJoHVU7lUj2vhBor3Vk5xxbXrIeN8fRPpnLdE77s6LJMSI231un2UfeFogqN25UK9X74duGv0ebUJSQm4fmDVAyhs8bOo9dGOMaYdSJ1L5Se7v376BCIT2mTlwV4ZawNkhHzD9WQGqNkptM/CGCA9ZoWKw7YCW739+fsbhcGi4ta4H7kjl5zXPM6bDAUlay7UK2x++mghotXK5XFqAfnt7awfj/v4T8OD6d71eYV23StAMXwPHsiygyt4mxrA53TzPrSKptWK+s/ZdiVadnqT3aX847Q+2IQ5Nzpc2TnrWywy38MQsY/s6UEOyEAIMIioR1mXFKkFKr1urvlrZijlGzpinacJ8XwHyqCXD2giI9/rlcuHnmQtq4XXx/PzMY+zQ7+/z8zNAFtfrbUdYFpzP53Z467ps5KdzqKhNhWPs494qhT36Q5hkX0SEwWGWvXA8HhG9Q1oXOegYktSKvMMmETzAm+PVsq3tegBgWzdErShSYqWf7KunpydYy0Z04zjier3ykHZQ6y/5i4BlQNxI8uHDB/zuN7/Bh8jdngZsx6oNEQ/NJ1BfEAuqGY64RDXW7DJFdmS0u+x8H8B7OzjjvN73ARGaLWoLtzUGcff/pexhIPkYIDmdeUxdiBHW2QZdOOfY0ZEIzlph6LvEK+eCLZWH69TNYGUBahdi81YHHyCbLmL5+z4Yb9uKeWayR1l7lVO1dnd0OaEegHvyaX/f+J75hyyey936EDiADjsB6u7X+YL9nNz3hxeXvnyfmBhLwkd3lQg3au0GbFgLS2yHHGNsXZ6lZMTBS3+EXr/DOA67AyVi29bmGx6luSwEj2E4PKyL4DlgbjobQEhNKyS8dXLI1dKevfqYVyLWgosu3EgnB+uWLdaF37NUJiK5gSxjmRdM08jrEeaBsN7Wrd0zABiHEafDEa+vr7her418B7F3zHyfOWASMyxUdfg0B44t8fU5H0AFmOIRhTZsWwKMeuYTQvCwhgDDSqScMj59/ghrLa5XHr+XU8Z9nkFEeHp6grsZngIl+885xxCTsdKcxd71JVekyodqEBjUeY/xMOFyYTWO2ltsop/nYFhgnUdK7NFCxGZf27rBOq6sQwxY1qVl2vzZOYvnhCDCGZZcbltCCGgVbE6JZwWA4C0PHoEx4h7K3jEFwJYSrIxqcI7hJz3sNCkN0gtAovBblxncFeulUSpj21ap/IWfKUWmdfVD+dde30Rw987g3/43v8f5fIazBR6VR8e1stp2fH2HpWtwtcY/tN0ba2AD37BKPLZLSa0edAD2o+bORYYeDIj0hnFXmgZ4QBwbbYcu9jfPGIMsXWRMlPID9c7BSgB3xsJbh42AGCKorsilS6+MMaBSeBCvVAzP5yN88LjfeWBJEDxSFQ9Q0mpdcb/feeZqpTbCjDMRapmUZi979cpewqmNLI9t/z3b3QcRzcwVg9z/Pwf/bnpWa2nSMX0536WRxhgE169NSUaSTljveoNNWwPGIE6MUxYqsNUiV5ZMwhkmYImbkAgFzgLOASEYPD0dsW0J9/sikrc77vcZ27ZiGEYAE5R0UVURAMwyZBrSGINSQMbCq0GbsTwa0Voczuf2WQhgGWut4IHvDilXxOhgDOG2sALLyrMexxHe8yDyYRxQErCtFYfDAOtY4z7PfUwdrBwypSLRhvF0wvP5CW+vr5jXm2jbCbYCT4cjQmTF1P12xzQekXNCNQTjjBhU8UFojEXJhGE4YUszrAFWacQxiTPrvbXCUzhiGCJS9q0KO0o2vMg81FJLU6V4F2BMQRwsfDTwFHG/rSBi/x1rB5BhovI23/Hx40c4z7BGThXLsmJdZ0zTQQ4/kUs2HTkr19TmOqWEddswDGOTu1LdUEuVe2RQCjAvm8yPHbAuCa6uzeaaZzyzq2nKCQRuOjLgfVwyB+BtybDOYRwcDk8n3OnW9m7OGbfbDTmnDiG5gkwbgIz7urBSCRW3lbP0RIRpOsnc14K/CFfIaRzx3edPLSN3ZJprHsDeJ3scVbMkt8vi9MUZCn8PkZBNxgDok2X2L80k9fVAvO7ek0iVHv3f9sQmAMny+2g4vap99gugNYxYa1FTz5xjjDzGDB0mCpJpL9KVRuCB3zqQIpeCQo9DFbxnVl4DaQhWXOz2ZC61n6HXqHifBmSFAp6fn5FzxsePH/Hly5f2vQrX8E3oc0S5wcS9O0w7lg+wmybfM7UXtsDOGVH9RLR62A9aaZn6u1JYn64RdcT9dkfZNsAUhAAMY4SBb40tT09PUKthxmM3EAVU0V/f7+wuqP4lavilpXlKCeuyovjeC2AMZ856/zS73v95n6hwttgnFymBWkpBbM9+bZ/17e0iuumpPYcWWJ+fMM9zg4OiZ939tm2tO3aTDlKTEoj6YRliRC4b7vd7e09jOt47zwxB+OBwPB473m0dDocDAGBZF3z5+Qumw9SeNwsksuzjAheG9mcNwikluNr5pEpiV2BY3WIlMcuZB2Zv64br9daSC85ic5uhCoOGze85jnmeG5Sh06EUvrtdrj2WSEPaOI6NXGZOIALwIgl28jM4EdvWFbQbD+isQ0GBs6zfrz9WWNc5vhACPn782DTtrHIaEYLHXebD8lwHJ30svKa1h+AvBpbZkxXWWrhqHkx4zK7TUIO8En4aPPZZKEAiL+xG/7Dapq7EoQwW3snVALQbtseAOzHEZWyHFTokoNijtZxxGzHz0H8HHkk4bVY6n88Pn4WsRWma9D79iDHiJFnF2jLvOAxN9jiOowTmTppycKSHe6SQTIO4zN4rHy1TV9xfr+GHH354uN9dp90J2X32vn/fvTZZNej78X7GcL9A2xy7g3N/bfvKQ4OyGo/tOQAu11fYSjidJ5yfRjhnsMxaeg/tYNFr11JXDzidB7o3OSPDgyz0uW8bB0QiHs84jCOyKEZ0jezdFlUHXmtp3EspBcMQGtF5Op0aCd+MtITU1gNV16UGbjLUDqNGkEpg0qalPS/AB0jnb7Ztw7zdMY5DIx0BC21MW9cVRq5H9eFEhPEwtI7LcRhwvV9x+cMFx+NRmu6YKNZglNKG0+mEp6cnzt7XhOPhiHmeBcpxmA48g5YJxt5FqgddjAPGsdtQe98PfiV6dZKZ7lXtJN1Xm3ov9bDWdbdtGwyZxjcBQNoSxmlsgRWG5aL6fAHzYOusa0mhox6b0IL509NTx9+3DZe3BQAhhAG1Fgxx5ABPvLeXZQFs75rf74tfen0TwV3ZXyXhFKPtaomu2d5DCu9hBf0elRbq/xljUIEmSeQA0nFe/honsEx/X331hoF+sGjQ0ethb44e+HTayiiZrW70/WGhB4guKPWo0PuxLAsPNpbDgCe+u+Y9DfDQBS8Dd4tAMs1sqx2G74Zev8PqFPPfE43DMDTVxOVyaV+rmP3ebIq7BPmp6d+NYXXIImZZ2h2oARGGWsDnTNEhhohiu4JHD7WmcDDmj+6fXgcf4hbbltpBeT6dcBwGbmpxDIEEPzWiTTembmL9vEr+PT09oZTScGu9/iL3XtesXqcG+WI6Qa3Eq27yH3/8EcaIp7cQ1iE45Gx2+C837bT1u3tuXY7K1amqX2DRqh0i4jmf73gS7ZQ8nU4IIeJv//b/BRGJDC9gHId2X3kN8qQivR9xiIDhJr5xHBkakYOP5YS5YdMxRhwOR5TARmD3+70piW63W9uLConqmrrfrxiGUUhzHrzhPQfPaZpwv9+xrQUvLy9NVOBkStb1emUCPG3cyLXjcXRN7+2FdZ8vy4Lo+fmqM6Sz/cAnxncfuLot8WSneZ55n5quWtOKRBO03rFdH4QbSrbrcxsHHgPphEtMW0IFJ4eXy53vr+UE9OXlpVma/9rrmwjuBtx8QACqMSC716lzSzigDZHEWKx2GlKfyt4fGGPApuHlesN656tzPNZMM1hjHUp+VN5oRumcF5jovfJF/UssnOtBisdg1VbG8/vYhktzA5aDtx6AFfKHpZCvb1fW6CoeaFiTWyphnA4IcUDOtXnW5JJhiyxG60CVR5VpQE1bQohDP+Rq9+bWxafQimZ1StjusXVjTDtgjsdjuw96OFWxVOaKqEop2QM4/91JUCoYR9bhpyYZK9wUJsFc1Sm6SfQXVzwkMBmTxVoJOcdZ2XQ4wlmeZjQ4iy1xBs/dmk4COfcgGBgYI4QtWH5WCiszhiHKNB/C4TByz4AxCJ4z2pRWfPjwER8/vsAYgx9//BG32xWpZPZaIQ4EV1E8eO9xOkx4fbvC+4hqLWIIQsDyr1offehDcK1jOysBWatUpjwd6nQ+IZcs/89V0TgyccoDMiJKzpgO3PlYKjBfLogSzCtV5NrHL6okL+cVxjrM89KsMaq4KOpUobxlbvhyXuyteYgFk7aEGCawWIErk+PphBB47OT9foe1nfwchgHXG3dXf/j0ET/+8CO2lLDMzNUs84oYBsRosMwz2P+IiU9VJOXMWbuS73qAW2OxrAvDJ2CnUxDPdtWEhSsf5vgMt7EDkEPT294kFiPG6cDNY6EPEXmVgUM6+JzlrVuvDmXA/TzP2NImnCC1JjhrDdaFfeO5ITHAy0HBltws1KiV8Pr61iqzX3t9E8GdCC1jdba3/f8x/q2wiJHsp7YORpZlVTjHGyRXC288CkmTUGWZUWsyInAHY+HDolbeHIrR6SIHuPsz1woH1xzmUJnsAICct1biabZWagEKt47XyuU8EWBsFLIqg6V8mdl30SJrAxHj7QdY75FyQhYCxUr3qY+MNUPM1GruJWoUJ8lpmBqmt3/tM3clmfTgA/qoNM1w9N8eD9xH3oHtb0lwclWplLbwWUWEFghSYifDIY4CU3RI5VGj77rutxaGWiSr14yZh1V4hBjjM/CWAAAgAElEQVTw8uEZp9NJsqINBI/TidUu67qg1lXelzshmORkLJkhigIq7KtdKcF5g0oZl+srSmHP9uN0wJcvXzCOUTqMOXh//vyRtdiSMGjlsiwLigXKJgdlraBS4Ayw3G+oFTDqahmM8BHa4cuHIox0q26scx6n8eF5ehthHZpkzjoOqNZaGFdRK3iWbnBYtgRjHOLEcAtLJT1mGazhPGeawyj2sym1YecG7HZYCic+61pa45ezAc9PZxhjGkSkw8e9C7AmYZ35l/esK98yzxBl0zGHGAfkQrhdZ4Q4Yl02bOvWxA2Xyw1x8BinoXERewVXq25qadXw7XZjK3Bw1rsuK6x1rapkUpdn2hqjSjP2VzqdTnDB4b7MvIbBTYExRORUEANDXkNkDF29ath8T1VFUfbhLPuB/3/bcvOKIjLwjjmSUgru97kdTCUXlkESIUQrUF5tieuvvb6J4A5Qy3B15qW+9uSZZuallmbz6ST7qi1DFnmgfH/LHPOKTUyTrLXI64YqGSmXTGhdlooLK7ZJRAI7UIc/qGLdEntwgDGxdV1bh16z/kQngzQzu1wurezeY9Ha+boPxtpAA/Sxbnv9OEkfgGbR+rX6dy5Je7Dcv/aa972EcQ937A+tPaGp92mPcetLv1bhj3HsgUjlZv3zdcdJfd897r0vhXNOqHJgKOFVa8XxeMThcJBsqBu3Uekma40QlWvQe8xwWh9k0gdodB20QgA//PADZ9rH04PvTym8+TQxiDHicrmwla8oXwA0WIHQ8XAibgCjzInFNE3cKEedkCUijGOvnnSd68/mzI6xWj2Yc85NhsmqFNcggUaCWzz8DG1We319FTXWASoU2jcJNQi0VPjg23i7ZZlFb26lwjvtXBdrewZaPZdS2B5BoIZ15aa+QeTLg/fsfBlJsnwrz5PFErrfpvHQnl/OGaE+Vt/e+cYt9B4G1vVrcqP7QX2a1Etfv17FCgqxzPOMaTxwhTTPcpj1WQqPa7zHJr33KaVWfatCbV23loDq9ev/t0qZknQa5z/az+9f30RwL6XgcrlgmlgBEH3XO+9JEUDw9Qqep9lwbM7EO77IN9FKwGCXvb4hvPdtMyn+lXenv24iDdIazIMPLQillBCH3nmpgVAXCjsK8pT4dWXbUs1WNZhphaBNOXsYBFCc9ZGM0Y2ohE0ufYZjX0ChldEKsczKwO9IGP2zBkkN1HvZpN4jJfoU59x//55U3t/jPenMQ3//mGRVqWB7LrtDSgOA/n0UozY9oPcHvhJ2Vmxu2SN9hkevMFiJ0SWx1+sVpZRGXuo1Kfaun0e7WI/Ho3i2f33gG/S5KZ5rrcXLywt+/PHHFqz1uSmmfzo94+3tjdU+KQPGNXkgWxI8qowuF35+nQztNhQAQ4cKT7G3Ue9zSIl9w8OOuyAiuNAVRxqQ9tUWd3Z22wR9TrN4kE/jxJm8HBzH4wnWlSbD5cQhNWWXKnHO5/NuzSUs64LPnz9jmia8vn1tFSAnMgFFYMbL5QIfPEKITeVERPj5558xjiMOhwN70xfpITAMTy0zwzHPz8/yOXhGac65da3qoRdCkL1CrfJSPkaJ6X1Afnt7a892GDrZr+tH1zR/7/bAaZmU24Hnvefu9m1rz1APagCyhipKTY3z+IsI7mnjgHE+swObbtb3AR6QRqAY4ME+JU2VsPOTgUEL7A1T3wWgUgqMkFf6vmwAph4vnRBrBlmWTbKKkJUhRsB2+2B9Hz2Zb7cbLm9X3G639v+HwwnW+n4g1IoqD1pfldjoSjeMAWOiIQTklJv/9X58mn5OzkK2pu1d15WHKzjbfKn1wNJNvw+2vIjZC3tPXu+zIM3KNaNR7fr+WQH9ANNskDMf/2AF0QLujiRNKbVyWX+2vs80jjCmQ0nvid+cM7zpzR3DMMDsJHe1FizrrfUQ7D+jZuu11lYCt/Um64SIEMcReUtNN34T3fs0TQ0m2jZWhBwOh6Zp1oNhHEfAeCgxzofJhC2XBic5z9K+zmlYpMTXo7CDHop67WoF+/T0hFoL7vMMVUkd5PBaBXbR7k/r+bM/Pz83clcPIr0P+sw0S9538a7bhuhjU/x8ff2CcQwYxgEfPnzA5XJB2qoMi+HPrln+9Xptg2HK/Ybr9dqCMcnwjnEcEXzE65drI0UZoiOczycAwPPzM6xx7bN773G73xBlUhMT42eUXNqz0Oeph7DCj/vKKIRusaFVEDt8ahzpvj9d5sp7XyXHpfQxkgolEtn2HNljB+3+TsPI3vA77kvfXyWeteZGvO+r5V96fRPBHQb4ww8/AMbgfDrjOnNziRKaVfTlxrLPunHc6cfNLZ6NewJ3ExIA7yxoTTstaAW5XlIqDJMZBOYOSCV02dq9zzItSop4pLrCwCJXxumoGOQMKY0X6GQXzcBgDeI0tSzTiE4dtnvJoFIbNQaYNvWoFqBkhqCMZV8VVUS0Tro4ApVLVCZ2RbaWmHwzziMXgrNMEg3DxAM0aoXz3WoAlLhjczgIbJFAVQcqF1DdkCuQM2f5MVrMM/uBFytzW01unitEG3LuZCpvCO7GZZVIlQDLRlaMwzuUwgohPbRyShjHAUOM0rL+WMl47xrXsm0r2P89gFRSqfBBFXuHBMTAGc/9fkc8jTgeR9QidgxkeYxd7XDOfgNx8Cf4OGLZMpZ1gYHBcZpwuy8NuiulYt0y4jAxARs9jsczpomhntfXC3wwGCXT21LCceJqxD0d4XyfS6DeNDeZKlRyRt1BfgYEZyyoFtzudwwDV1zeGswL+4knqrAgPD8/Nb13Shll5Sp3HBK2NcN5h3VhAjBTZo7HOtxud6m6HO63pc0lGAYL6wk/ff3CM1m9x5orkNgnhuyKLTHJG6JhQQFVDCFinAaedZoMhjjAGYfb9YbpMEnTF0NNMRSMY0RKbNKl1eXr25uQ4UKAGot5WRDjgFrYLdSAjeLCELDmDcak1rnNa4WwLLMkBkxSK7R2u91bUnI4HBC8xbowlNebkO472Bit41WTnGFQYzMjsO2CRQ5JZ5kTcbLfzucjVzgzq72maULKK7zzgCHkkiSZLS2JOJ1OfzKsfhPBfRhG/PVf/xvgxgDC69uM19dLx9iJ1SEKmTAJRjwVJgS2nTa9nDwdD3iaDnDWIPmEcZrgAlvF1ip2tKhYxSyIKtubqp9JqYS0rG2IQCVuBXeOkEtBSoRt63CSGk/t2/ZjnJhh9zwmjcCWtaLzARE72BkyqGyyILIhgV8qgVTCSMC2ZrG1HWANn+KGeGiABk8ibrXOZFDINrOzsmlA5APEWg9jeEgFE6UJ2zZjTdxllwtPfzHWwhoDMoAacKWSmMCtAGDgg3ayosFUOa+SGRlR/qgSpQAojC3uMv1SqqiDmIfQJqhxiM00yxj2XNGSXiVw2uDFATkjbYTq+rCH6HzL6nl9TIKzjyiZAFLJZW+ocm5onv0hhGb8xffQYZgOyLcbjPXCAfFz1UBTawGPvOOfTaXgcrlJlTFhHFODjoZhgFuleSwExMAZvQ6fLnlDyZsEcA6MVCo7OEr1UbSCQ8W2LrAGyCkjpw2jG3E6HFiSOUR4f0SMAeu6Yd04SLy+vjXugfH9UVw8B+jsUS5MDWqhpuhZtwRypklAOSP28NFi3jKM9YgDj+AzIv2clxmVeK+Uytnw0/lJui4THy6R79syr7hd7mzZAbRkzUd+JkYratub89L9jmkckbYMkIE1VqoP7kHh8Ya8LruVSMHx8MT/ZxyscXCO16COqCQieBFTsHVDlXjClbB3DrV0WJNRBE46QgiYlxkxBpxjaEFfY9o0qfTTApE7lOdFpZwW1qoKLbeKGABeX1//ZFz9JoK7gUEtLK9isnF7bDZw7BmuRMZ9Xlt5Ywxbc46T7+z4dUb91DswQ7jgcOAMsJXbwUj5q8QV2/pSa3MUiZhssm1bcb1eUIsBVQvAS4nEMMThcHiAk1if7HiEG7w0NAEwVsbP8YR7MvL5lazCLijDylEAltW1TcxQxlYyLzzs9NAwKJSRBePme6Q2DUYkYg5m5UYrHwIMMlJa2z0HuozMWoMCbixTmakxDtoPal1AjCNYO2SQ8oaUDI4H3wY7dH16bQ0zComoZbFuNiVHa63wzu6CqsXhcGhwla4NhXiUh9AAHSMfDN5YvL6+dp5EMjdNBPTfDodDh7Jcb9xSyEs7fCsB+Xpt/j0gkulKtZGqer+3jT1PhhgbgW6Mac6iHz58wCJQiTYF7YlL/VzH4xFPT08Ny9aXNsHo9enEIdbCzw0imudZMsHcYBznbFuzyqOocqZzUcDh0OEmEoXYHv7Utd+bs2rXtAM4xKFhz8YYEPp6YEM+h1mudRgGOG+5IvesIb9dr7hlVilNE3e+xhBBEzWs+n7v9+VwOGDT2am1O8I++iT1zk6GVgpyemtwjXbiWsuTs7QCPZ1OnbMA4aeff273LoaAaRyaHJMtnPswlZILltr7M1Qrz5VwJ0wVukqJxQOqUttPyNLPoo1Xv/b6s4O74anE/weAvyWif2qM+bcA/CsAnwD8DYD/jIg2Y8wA4H8B8O8A+AnAPyOi/+tPvfeWEl5fbxjigBBGuNPQ1CrGGJaA5Sz+Kis46Q19QRUOTtx8wFnpl7cbB7jCk4SGN57AVGvFMI7wETC2B4RaCcGHtmBSTsg5tZvKDRgGII/gJaAZNsLSrH3PDfBDZJ8K7wNPNSoFsJL9EDEckNnXOrcWbbGmpW5lC6BlCwasGefPXpEzcQYqhwSsQbVdWQO8V8mwVVWTEwGw8r367/w79w5YwXPV4yeIIVelTuoxucQGULkwT5DSBUUIJA7WhH2nrMobAQi+OmCaxhbAGVbptgP72auaYd5uV+hgcc4cu58NEWFdFmwCbagKZk9s7vFMHY8HFOho1v1G0nXC0Fm3lHW2d3zO89w0zppo5JyFK+jErKpnNNgAaEFBg17YqSoUilBCD2Cc9nw+N6JcsXLF5Nlr/og//OEP+PnnnzGMI6zvtsM8bck1hdDeDliJ5Hm+N76IhQkBMQ7tMNlywmE4tENEm9ca/+Ec3HSAs64dHMt6x/E4tWYs5VSU3/r69QvCEFoVfJCehT0/drvfWpcs8ykJRGi2CLVUbLT9ERE8TZOsA4sYffusMQ5IWxdXDMMAmG5zQcSj8NSyQj+fwq/Kgezvg5LHuh6cc1iXFccjk76XywXWcNWj67evYz4QTAjwISAJh/Mg/ZWGqz/1+v+Tuf8XAP5PAE/y9/8WwH9HRP/KGPM/A/jPAfxP8vsXIvrHxpj/RL7un/2pN7bW4Xx6amqKaksjFRSWSTv52jgeYO0oiw8Yh4gQunc7jMFPP//MagjZ/JO3iN7j6ekJ03TC09MR0yG2hVpLRSlKVLLRVSVVJAghCw+QB5FDrQbGUs9qiEfYcbswyaCACmMjUtoAkSvmwht8SwkpF2yVF7E2NYDY2U47XPV3aw1g2HMnywIGwBUM9VZ9AiGb0qAg/pp3HbfWCXEs2bexTU6pm/mx/R8t6CvWbaxYK1seKmBNkM/Lh8LqCqh0e1O1d9g3lijsoQ0cRBV1l5nu/c/12epmU/24+l3rBrPWsC1sKaBKiNIduleD7IOqygZLyfJzWLO//7ma5XOQc3BBNP+1YksJWTJ03XB7aWUpPEDF2W7hrCTy29tbJ56HAZfLBYMQgQRez9M0cQAXgl/dQ+dlxijSXpUJEhGen5+lN4Czwg8fPjSLBFVPAZDstA8M0ZeShpzxLw33b6qyXfZ/Hge46FsiwTARH0Kse2eZsrblj+OAdb03y4ScM2IYmpT5drvhdD5j23mWj8MILwTu7X7v3jEiCdZg9/Xra0skmuUI0ILwsixNLXM6HdtBxmqVAOfQpnA55zCLSuZ0OjU/l+Px2PoIjDy3ff8HO7hGJnlFVaSV3D6RiTHidDqxgkme8X6MoK69nDPSLklQW2hd/3vl2y+9/qzgboz5KwD/IYD/BsB/afhd/30A/6l8yb8E8F+Dg/t/LH8GgP8VwP9ojDH0PsLsXqVk/Pjzzy27ArGBmJ6QWyrIpYI13zw8YBwcpvGMGAOGqM0kauHrcTgND0x1dEBOCd45xCFg2xi2uN/vzSqY4ZEKZwnWOwC2BVXnDExlaISvw8F6xsUh2BjZEVvm8jwXHktXaO3wTiEAHjkLWUcVWfxVNKgx3KQdmf0eKfzDhCQEY+e/G7D9sBJgpRLIRGi3JXviUIN+1HuGAIEgtMUd4ENIdfMEEME6D6ODCwmwqcI6EjVAhV1WOFPb9Rmw06d2Drq3Gw4DcBg4ezoej4hDwGGUHgEzYFsW5Dt7Y1MWF81pYH8XygxJkYUXjuQ4jhicw+12Qy0ZWy6YRiaMg7HIeZa29oqSWSWkum+VvzHZnsGDwkvLYsuaEENsJToILUP33mAAt/f//d//PbaUUAxhy1FGqxG852qSkATLH1tpfTgc4Lzo2mkDVYtUDE72BX4Y8Xa7wvuAnKv4DgF+MHh7u+C+FJxOR6RSkavFT1+5Vd+5iGWdMU4DV3bGII4jgvP4/PmzQCgGX99eW3C+Xm+4Xi+SJfKDHccRg/RtnI4HPL+8IEnQXtcVZJl/CnIgzvMdBzNCNfLOOQRr4b3F4HlP1FJQKXECYQ2ens/tkFrXtZHwDgxR6oHWplQ5hxAijBCyBMAZA2c9ciqopYAiB8UvX760gB8k6zXOYjxMsJcLXi9v2NYV67bh48dPqGSxJR6I4dn4VTJoxwmLs8ITEb58+dJcU4/HI4y1GMZJKm62/MglY1k3uNu9qV4USiYiBB9xPJzExz3i6ekF9/uMUgjjeETOG3JeWgIw3+8YxpEh2lpBiWcKv1e5/drrz83c/3sA/xWAs/z9E4CvRKRH/r8G8Hv58+8B/N8AQETZGPMqX//j/g2NMf8cwD8HgGmM+Lv/5++6rrP0CTHeex4TJy3LEfzQzqdTK+ecs6it6YaJwDjxGLF1XTmoW+7Qs5aJy5wKSi6YxkPLsIgIhqRxplRYqQKazp7Y8tfCgWAZdxaL0ZQzrrc7rrelEaxLSkht3FYGz2ndTzEiQIaKNOhEAvp7mZO1ap4GAKzXBnj8mZHIbNq38wFhjIPzFrXmflDIBtG/s/0s+8DrlxRiBU8ryfnd5MGxeqkU/hr+Bp7Bunu6yBUtS/a+IqcMawb4SLjcFszLjLQG+OBZL20d4sCBYp5n3kgDj7XTRcw6du6kJAApF6wpNxw9V8K2JjlAAkL08J79uy9vb6iZD8Hj+RmjSuWIME08IJrb6y3ieAJVJvFV501wXGltK2okLGtCqcDp/IzrtohthkNBRV55LNx0OMEY4HK9gVvwPchYXO8znLNwYUDaNlRU3Jc35LIgRIuUNwCRiX2BZSoZ1FJxuy+MtRqLn3/+GUkOAet8a49nEpdJV22iGsYRLy8vuF6v8kx4MPQmHEitDPmpZFarlU2arqyzuC93zHNqlVKMvhng8TPiKbxUC2aBMK4CocQYYWDboaqQmNp5a1MaE/BcafB6J3i/MaQi1V5aNx4oLfsoVx7OodWTn3yr2nXtHI/H1nBkrcPb26UdWgDbRDvHjpdvbxfAFMQYcLnMAtU63O+9oclY19Q1/DmAOAThZbgRrMp83i6r9bhcmDieDhVJYKnbTaGZ2uAj3f9Kmq/ryjMiBIray6d/7fUPBndjzD8F8Aci+htjzL/3D77jn/kion8B4F8AwOePT/Tdd99xsPYeBpwpKZFRDc8p9Z4HISsOvG0JW55hEgDqjQMpJ2QSXxjvMR4ionEPpx1Rb/Aw5rFRqtaqzWYNqjDWIheHEEdQdSBiLxrGERfcb3d8uTLZy3i6mhMYkR9KJyJ6F6kxfVPwzzawpvumKPap19TIHKL2u15ju+7d3wGGXPSYaP9OXcu/P/1bWVgKb2hjYSESS1MbuQsAVAogBwDLOBky23tMm1phCldb1QOXyx33GweOIVgsE+PCIH7uJIdSGA/ww4h4mLh6MEw+uxCRCajGYd5YbXA4P3MJvyzYCkAuIIMzxpIqPDzu6wZjRzwJROG8w+0+o1SPbV2xZfYjScWjGIswDoDjhhIbAzfi3HLjTuYbw2whTPCHFwTLUMNtLijFYPAT4nhEUcLbCFHtB7zdF5TC9rHeex66jIy3t1eG2gxYyVNcK/MVygohNHtdJSCttWwH7bsUD0Sw6N2Qyo1Y7xq3kNIN8zLj+fmlOTK+vb21IRvLssB4D+wI6hgJ40j4+vUrbrcblsXi6XxoUAZRhZemHIVDdN3ujdc4wPauZIW/2BUTD4eL4topJe4bEFhGCVeAD2+tuhQDf35+butZYafD4YBpmnC73qAzDg6HA46HI25iOrYsC67XC85PfHgoZBKCb8ZgTiTN2mjFfj2FPYwkITXy84JcJ+/NrnFXKEq7ZpkQZ1sO7d/Q+66fKeeMnFKDZfZNbL/0+nMy938XwH9kjPkPAIxgzP1/APBijPGSvf8VgL+Vr/9bAH8N4F8bYzyAZzCx+qsv5zw+ffrU2+ortYVQSkFBavKpXBJQumMkwLJCiz4lxziHwXeVBTP9HGD13/Slf96Tj7VWuJ1Gm3+3KABq4YkzadtwXzLu9wXX65WbPDIrRmqt/PDByhaJ1oCYnvX3hERZvRpq5lT7wM6fkdomV7LpfXavWds+aL//Gtql6Eos7rW53JlYYUnM1hjgBMA6W6ZdDZxxcLZj8DqhXV9G2Fld7E5siXmQiUfwBtFWxuqzHDRSmgMMi81bVxcAGXbhcWZVu0KNwZISiCpAFiVxZr4tG0+8zxl0z+0Dm2tiK+kdSUgYUDKwFYIxHttKWNMd6kSq0BIAmCSGbbJJbSVcf/gKa1hPzcpqh5wr1rwIiZphLMEaYEnyfGCRSgFDQnwvvWGb1xAj7rcNMGtziNTAfjrx6Mm/+7u/Q5GmGu1wfLt8gTF9Hu9hGFviknPGTz/91IzC9FmP44Tb7YbD4cAj7O53fP36tRnEVWsxTpOMfLNw3oFA+PDhA8MmaYP3psEP3rFowQAtO98HqbafBepRKEeDHxHh06eP0Ias5ltUupMpw0KhcRoxRpyfnkBvb42X0ffStadNQoDMPPAeRKU1oi0ru69+//GjqG/uuF6vOB75ID2fz/A+NrLUWvv/tff2sbYl2X3Qb1XV3uec+/Fev/7wTM+M47GNFRRLkFjGxMKKDBERsVDyj4kcRSKgoEgQJCKEiC0kJBB/BP5ABAklsTAoIBI7DklsWUDi+ENCCBw7YzuxM7EzTjyZr+6Z7n593/04Z+9dVYs/1lpVtfc55973xu3u26NbM6/v+dhn79q1q1at9Vtr/RYur67L/JZ+MkLnqy+n6zAOA7aNP8Se4Xa7xeXlJZ68/DKyrxnW4zgByEVbXzJLdiEAKuTt/m9rdwp3Zv4BAD+gk+K7AfwnzPzHiOhHAXwvJGLmjwP4Mf3Jj+v7/1e//+nb8HZAMLfNZg2jQUXKJQ42xQi4VHiMASiJPcCoGHtwVWB23iOTJpRkS5X3ALliJgkMo4JNo0JSrBSzIELSieSIsBsnRPIYbkYMOynqe/FsQIpSiZ3BmLKQO8EwbEgce1WVM8QkoCLPjd1SHhTBazxx+e8SW9ONQgRaC8VwidMvmwaqkC2fgxBcEJxdfQWh64QlT7M/g/cIajk5L0lK5DLanjnqKr2D9yAXQXNWoNnzNSjJBItDgmPdMMghZxReei7+AY3LV2/ulLmMFyDl4aRGrRVtUepmWA2lmhQGCEmYhdBCi5mXqWmRPESgLGMl17JCLZIbwSBk2eXg4eDYgVOlSABk/o3J6uB2MIq5GG1XJXC04jMAJYZnqSg27NSP4yUB6Pp6KxDUGIU0q+8xjnJu4/UPQRgZwQmjpuqv+jWurm9ko/Ze2UM7pFjZI883Z5jGUX1GAZvNic43yXR+dn2NZ5rNulqtsFqfIgRjOxwQ44hpAh49OodX/4clpAEoIYIxJfA4aO6ECMTVeo2kEUZWSrMtM8gMZQWN5fmOgxaFZpT4fqfXNdqAnUKiNi4rjeSKsRL0SWEYKin+O616dHl1KVDtZoOUR41B35TrG0SYc4YjIRCTEE95jplzieTa7baARXk5IQUb9Hk9evQI0zSJUjgO5TdWEPv09Ew3p+r4t2zbOE2IKamf8Xbx/VuJc/8zAH6YiP4rAL8I4If08x8C8L8S0WcAvAPg++46Uc4Jl8/eFW081jqbllZtZpvtVh7Ckue8JBQgEwz8JwKQGb4TDWYaIwACBQZQeeAFL3dCOZAznANcJyXpEqR02pAJgya6DIPDzVYq3Nzc3GC7HZCTRNJUpFs48MxCEEcnqrrMLM5Nfc8sm0GNi5eMQ62EWu7Hafikg9AbBwKIWTL8+voIS9amM8EtNKNWAcZ4QAIRQuNIzZ6K2k1qOVhNSJC4Uj3luTBsrAygCmJrTF6TsDSKRwWyvSd4eFLriqgUEy/Cmwie1XrSjyKlku/AzCKQ7VmSB7kgCV7QfpLG9tvccJWL3XwJpsmbQZMyhIyNhXo2TlMR/CQOHUw6DkRqFdCca2cnrMRKKYxSAtKsBYtOsueTiZEoN+MZQKwLPitTKGXExBgGB8IaYAKSROxwAhw6nX3yPN56dwfTE4gyCIStEquZY7nrJxiNdSZCZg+/egQfPHoinD9J+LpJIru+8pUvw/sOOVOpHSvrMWO3m1RTZ0xZag3nnLGdEsJ6hdVGqKdN86dOfBMnj84QdxEuQiOCMoY0IGvEWRc2IETZsFkydkFAJsGrh5SQYwSljFXo0J+cyNocR0Rlmjw7O8OqW2Gz3ijk64V40BFC32PKGbtpxMqrNcAZq75H5wI2Jxu1+rNGikkCZQgB/bovVMHX1wmn5ycQ4JIRpwndukccBhBByiSmBB+qNr5arXC+6nF1fY23334bKSU8evQIp6ePNKRyQqclFyWk8hzeO5ydnRem135VCfkOtcUOLz8AACAASURBVBcS7sz8swB+Vl//EwDfceCYHYB/6wXPK86cvkcOoWgH5nwxfNnMIhNUln5sURwoQkRSwJmrkJymWCAI0cJEONnOKJEmVPqTUkJ2HdKYJLLgUvB0G2zjAT/ksdazoAo7ER1FHzdtGuITaGN4SVEaS/qoTmMNxxOVsggKqFAswtxLKnoNuUtCN6BatvcelLNmvimvDtUSdUA124twJ0LnA+YG2BLiWvBcqKVk52getl7Da2gpl5h+NPHlZm0ACsMA4Ez1r8l2SITO5eUlrq+2OD15LA5GJ7QEMUm0i0BljByrk4uBcu4Yo4SZcq0HK0+J1Q/CODk9xdnZI7zz1jsN3MWQfAdJiiMQHOlfn+o8nY1vAlEoBSHIAeTn3Dw2D+rYyViQbpKKX4lFWtSBWpSFoD4bMXEBQGoAl3MB424qs/RmkOLXgBR5ZmYED4Tg0HUrvPrax8AMqTkb+lIMpO973Gy3GKcE8h1ONhKgkKOU8QuhR05AzlCYyHxKWWgY4NH7Hk7hlHGa1OHPIBJYaRx3ipevMcUJQxxLwfGYGidn4zwdBo+u7+EV8768uMRrr71WMHh2osBM0wTnvYQiKxycwWJ9NfLG5kvV5gkXFxfYbDYSwx8nwNc8DDCDuhrqm3OGpwSn42MyzfIVrq+lvqrRb1hVrhCEEsMUmouLiwInvav88cfavchQdY33l6jGNwMVC28fnCVy1M+FL90ag0utSyPfKWZ3Wdg8w9kZmrafEkDAOCXc7EbstiOuLne4vtliarQ+mXxJFwc1WqJop955iYVVyMd7JyGF5MsEIOcAV4mxHCkEYfdRBGF1nrahLjbhGLUkoMSKVxrerE5NScP2SI4kFt3XWHPiJBbFbEwbId/8uwNhq+fIUTNiqY69dFr7HJCoUvzmnJG1D+ak8iFAaAWyFCLOkmzGLD6UKSdMuS7uaUy4HhjvXknRBXGyK/WDaujI2RAsAAbPNWPqvHLqAP1qhZceneKbv/mTePvtt/Cxj30M3/jJb8bP/PTP4tmzy5Ipa8toGuXckVO9HgiOQhHvEhCQQTQVoQuHUmrSLArzdxhvibAcovwmk9AChC7IOKcMYlYlxanFUDdJIgacRQnoOaqRhGhGJoCk3oMpZdCUQNtc9RTXwbsOEQSQx5gJrlvDe4eND4DCY5LSP2Gz9pimUSw2iBW96gWqeeeddzBstzg7OS3MnEmtlaLE6Vw0zHyKE8K6Q8ihZPMai2LOWTM8VxjHAY8ePcJ6vcbVxSVGN5as0ZvtFjdjZXs8PTnB6dkZdttdmX+OpOxjkRc6T1NKuL6+LolxwzDgpZdeAgh4dn2Jy8tLqc3rHKaG2M7goJyFE8aI2LbqBDdnrRENAsBms1b+JF+SKQGUPI5WDh5q90K4A1VTikrFS4TihW9TzIGqLZrzxIiMGE3pPV+dpkQkFAaK6RNJQQTXJOqMicHkMSgXyuXlNS6fbREnRs4e3vUIm053U82eVT9AFyR5yWKjDccPqm1DtWCiAFDlOoHBESo4JTwszZypywiYHOf4LgjKd2MP2mAd1X4YCMm0UD2nZ3S9RKo8fvwYnQeIc+lnWf2qtTM51a5bS+T25pARVHu2zdRomQFG4gmRt7LJqt9imBSqylLtPjknqeYsMfevPfkIPvfPvlC08JEzxiTZic47uNCDqcOYoMlkhJTNGjO6Ct+iR2J9oXJuZ7UEffAI/Qar01MkYlzttnjn2QXe/sVPIaVKBe19rQlAYIAEzplSLlakadvmTHY6V806qnMYCt25Qs9bcVUucKUMMIEoI93slEnUzRQD7xyCq5XCnGN0vUGAuuk6pZY2w0mHRgK3CB4diG1OEAREtH7KZkY0qpUisKEHwVFWDXeF4DKC77FeyXN3riYnvvT4ZTy7vMAwKg4OBnHCcLUrVqbQIdTnM00TesfFSWvC1hgtJYmsqyn8utZMqOacseIM188tUZMNFoXUr1aIUWDYahXXfxatYkliwyTRYlJ0XeLk14o6GNmf8wGsVmbOGRk8K5lpm7sVOSci9L0EmRilscXMW5TUbe3eCHciFA1KTFvB02NxjaFAM8Z7EUKHnEZwpqL5TOpJHnctOZUIdudqmj8zwQURwkzAbhyRAOyGCXGKINfhyZNTeN/DU4fQqTAnFHgki/6iGwmVgiMpZ6UxjhgUYwOAKWaARdjnJCXyMnMRNt5JpSdwrXs5i3Yx4djIVnKkpbqmQk/Q7GvwRDh3HTahA/mAy2GHkYDNyQbrdYcuEJQ6XIVTg1FD8X5Aywy2Wr05X4P2v35PIARkBFcrNc07TUicEFWTEzI3gNwGzAQ44Etf+gqup4Rv+uZ/Dk/fvZDkkLCGCyskxY4t5yArLp2y5h4I4ALxa+TZ9c0fYk0S0poPnBNfTGZMKSEx4dnlNa6ud1itt9he3wCRsRtG8Vk4B4mBj2KaQ0tFgiw5AZyC5E8orm04oiXmCcpSiccIwDhZ8XAZ09AFsShMm1W66d0wibOUPazqkSF/Ng0ka5iwXrniw3LeoevNGpONyOlakGfNShxn/EconEgFViQS4U9KqZESgo4pIHO1cwoJAkXpIZ0XAYTV5hQbTdG3YAG8IqUiiQjDOGDYKRMlM1wQC3+3qwJw1Ytj1MoPTuOApDxFm80JbsI1Lt55iqTlDr132Ckn0263Qxwjku/BRFhtBJsXODQU6uFlKLJZvcMw4OLiAl0vVao4MzK0CIk6WKXQujhxU8rYXV1LdajNSmmPq4IAoGRdS2jrBiEAV1fXErjBGV3XF8qM29q9EO6ZGeMgLIzMGVO08mId8pQQOjHtOk0QGIeIlIBhF5VzRbLKJDxS6mRGRqPhCjUAEDFNSo8bWf55IILBbgVPAacnG9loNLDFIBhhhrTCxPY3FuIgZkv8Ea/5NE6YoobeQYRhSiKQjEMipYTOV7L+7DNYMVTnOhWUaomQYeyNU5LEARlTBrupJFo5loVAzmEdPF7brPDIrTBm4Nlui9D1eOXlM2z6jN4P6HsPcl3BeMXKoMJ5772XYCXVLITGti/hjdJkk7KqN0H3LIvKIcryz1Us3XsndV9JxJL3PRICRo74v995ht14jWE74N1n14DrcHX9DN3pKS7eEu2FyQu2D289qA5Z+6SV3Lxf/LwYQGQ2C5dQvDiNePutp7h4N+DJk9ew2yU8uxzgIBv1arXCxMAw7kRzUximywxPQCJGcgxPtuEow2lWR7ZZS06tTmg+AwOkmb5GMwHVDJfWq72PtqlRtdB0ZwBlicaBsBhg9EloAvot1uuVZIA6wMqglWuQU56dit8w1aIyxBLGWYaUCGMeS3Uwsfw6oewGSQgzs5S2nDKACYEIHqLZ+RCk0IwL8MZ1f8ZwTuazwSI5S1z5breTknkkfhHnCauuRxwndC7g8ukzjNsRfRfw+NEZxmmS304RaYjwPqAncURfajo/pwzXOVxeXeH87BTn5+cFInrnHfG1GL2ArXsjYEOM2G13WozlDNM0KOWDhIqebM4xjlb0XmrH2twzGmJhFCU8fnwuOR6odZ8vLp4pFDTh4uJZqX9xrN0L4Q4AEZr2Tg7XNzsREpM6pAaZ+huS9OdhmjCNyh9BQqSFgYpAE2Y5DyYW8xVQHJUQVkGcaNmBsyvcH2MUGmGGaC7iAIoVywZroYwKLYyqndcFFwpOCojGZA5hAOhXHuRqRXXOGWvNyrXMNjTJVpJ9ZxAL9K+hxfq/PYSEJA5bAdQdA7++e4aJJes2ZMbvWPf4htdOcb4hrHuP1UpSrU24Gw+KvPbCJaP/k+Nk3LMWaRZrRiEA/Y6yaXsaskkKgei92UZhznFGlucBh+2UkbfPsNtGfOmNL4N8j8urLfKUyzWrkDZNUkelFdbFyuHZNdtW/Scq3NUEt35aosnbb79dzGFzUDMzdtuhRJ+IVlC6UvrFaDBrVAvJLCPnHbIBYiypXF4doWayc3O+dh60jcHwrpZKLFYfSer+yErUBUaOI3ajENc9ftxjtV6rP6RujuzY/LYFRnKN45wgwr0+D4eU6q2btc32PA7cR8qVFmMaI4ZxAsHobiV+3FEucKfMsw6hW+PR6hzhlQCoMpNZclDYaEi6URQtCYHCaiOF3XfjDTIIKU6Cg6cI10nhcaFbnuAIBQKx6BZjbDQOG4NbrJnzVJzAoyqa4vMzOgLJCcgan3+Gk5NNyRruuoD1+qxQFogmXzlsLLelrSB1W7sXwp0BTAkYdffaTQk+k2qBYuZlzhjiDbxGbbAXzVKUEyHPqjdLSADIufIALLNrMm6YRErQL+FUMSUJZTKz3TkkrpmhgukygvdYrdfKDLkqmKv3HsH3DWxhGllTf5IcjHLQnIatICcIfm6Lc5omjDlrcW2ejZdpoQ4EyiiTEOTQrzYSxqdRPY8eneMrNxeIacCJ8/jE1z3Bt3zyYzjbAB55LvAYJRa5DdF0jUZvArVocGgcfnYac5yyaa223BsIDhUFYhYefWYGp0mYbNjj8uIGyY2ImSSePFUmwcQoggE4LPCIrIf2WGtECbDv0yDV2k2Y2HclYS6b41KYHyUPwzVjA5A+S91PAE4QjhLtJ9U+hK5Hv1ppQRqt/uU8oL4Xsz4TahBA+3m5DxYoyM7RJgzlXC0mg16y0kfEmwETP0PfV/zWe4k06VcdvBYOl/9F5MLVb1BnuzkSQljbw5B+NXMCEIvEvpeZYY502ZiJs2x+bNE9DGSx5KF0w+Ra4T8gOFcKnHu/giCBGX23EtgQQongtGhNf3KOm5trjKNwTYGEmdaesXceJ5sVxmE3qy9s67RNvDKGTysTWsddhL453W9ubsB5B6tslXPGjeYFWBSfKIpbrFYrzSSO2Gls/G63w6REcUYadihSr233QrinzLjajVUEhBWc8nAwGHBOE9MZ7Lxg3zkVTvGcgWlgbKdRQ9sITFTiQY21TqAT2SiIxQXlWPKjPDkx7YI4sXzXw6mnW3gtAvq1w3ptlKcTxkHmsIVMlblo2n5CqcKecxYu8JQL9ehms8GTx49xenqqMbMBIAcfAi4uLvDFL34RN9dXRatrGzcmeNfw0hNJej6TAzuJPthdTtikAOcIm3XAo1dexer0HP0GCBDtwFR9ZrVcOCs+Kt8Vl2zbFWpeuHkfE2VJ19JNQcjImmNc1YxlE2KQ65DYY6AJ0a3hXAIxYRoSEjwc5kKNSEofljFphB2pFk0LgV+sqqJJLjaEhfBfntO2qELz7NzMvUwKN0nilwp3l+tuBsAS2Xxw2Jz0ODk5xc12nEVv+a65L8Byrpqs7IXvAKLolBBOaoS5KQJB+MlTSsJ0qM/1ephwtRuLxi7WhMTgh86XZJ/TdcDj80daFUtYFDlxSaGXHV/nkhoxTh06SaOUgrf5IHemKlVrmmIeVivKi60rZrEKQRLhAwC7nBBGpcHgCOeE/A8AvI9KW0ISUaRZvKePHsHY0FNK2KjSYILZKSOsacwmpK14uGnuJuxN4BMRTk9PNavY4+nTp2WDsJKDxqMTlA3Vona8d5L8BCjTZ6VobmkcrH5qazUcavdCuDMk+1DkuGiKOTKYoz5snk3s0BFACdM4Km7lEKMrWgqA4pk26t2cZfJHBpgdHBy877AOPXov6fDOCcSTUgJ5jy6ssO7XWAcZ+GeX7+LZ5aVWyEnAwJiSxBAEZnz9kxVONid44+kNvnyxxQBg4mbB+oCXX3kZH//4x3GykeIQ680GAEv23TBhN0VMMWK33WLMAGu1HxOkhmWyhj3GDLBGC1HowSCMySJUxHKY2KPza/gA8MYDJytcM5CSRFgQdwA3dAYkFZNkjZEurCowODNirEIEyIDGJBscNCXWHCOBgxITjJq4PPcsTriUs1aQGjHGjClHrF75CD7xygaJCUNMEIIzIWBLUX7DLI7PpfZdrBjZqbSfemuuwaTtA9RCJFrWq8AmIngqFBTHEdxUn+/6riZgKdTm1XnKJDA2OIFzrPOTBZNerdd47dXX4LyVr8u4ubnGNFzJetB7cc6BglpS6pwsWRN6Y1kDGC2VLjPDOVa8W9ZY5ozr62fynbfQXd28U4VHiGTejDFiygnbYQRfXqOjiIuTDh99/aP42Osfk+xPszDHETkztrtJo59EI2WSMGW5vkE0FtdD0OKLan1bPoJTQS0dJ62YyPbClDo93pFDYgfKSv0xRVQHv2rsrtJNh5DRdZXiQzZsp5nbHl23QsgT+tUKYOWCIcAHh9Pzc2y3N5L05RwSSOrVRsknkUzbgJvtDWISVksw4+Zmi7PNWZEFKSfcXN7AB6m85YkwxoSYgYwMCp3SEQgCMKWEk80JcpKs+ZeePMHNzc1hgartfgj3zAVDJ9IIgoW2alqeU2+7cx5EyhI3RUxJ0pQTy0QRb7uGWBOB0Wn8hANIvP4ZDmNmxDxJkqHGWXvnxNk6ZdDaY9VvEFYd3nj3LYyDeNmRgT5lRNeBAXzk5cf4l771dWRmXP7qZxHzDSYKiA3WGBLj8vIGv/7rv1EEkSolcETwoQN8QBU2hPX6TBQhPc9S8xRekpqgBADsajWlvuux7jfqzExwLuNqiPjVz3wOw26HnLlkNwr9Q0TmCKJUnMNZeXlMe+j7NTqNWrD76FeEEHxJHvNujb6vhZwZUkSl9J0B1sIkwh/EuEq74qQCEVK+mWvFyiCYkJFY4Ks+9HtQy6FmIYCbTS3mDSI434sFoGOWVCCb/yF4SSQBJLLi8um7mLS+pvCmBPheSO2CFnuRQuiuga0m3YBRci4c1QQ67z2GR5Igd9WxmucoIXAhBLAzYQ8ALDxGTSRVQpTNNDtwrsEF9ozaWG0iEsZEsGyQEcjeiY8K1emceWmVMG6GCb/5z76AYcr4ho+/jvOOcNoHnHSSWHO+0cIVnIUf3YnyM6UkyUdjmlkfExMyurKhIMbGjhTZkLL4LLzN66ZTJVvZxhaMnCttNYwO2ztIhnoC0QQvDCMCqwLwjiA8aQ5EjLVmEQMAhQAxYAndukdQ2LM7ESuIhgHDbgvSOPVJ+d8zy1w6PT0DXMBKo1xYfYOdRvjsxglRFdEQAnaDwC+OtUC9cxhTxu7iGc5WEjp5fb19T4jDftubmRvWBDtfZOyZMPMeUjuzlkJ75+m7GvMa0WlW2G47oK023k5SZiA7AE7gAwXuizMthABPspO/ux2QuwEbIvh+AyTJrOxCD6aE7ALGGPH56y3++s//Gpz32EaAnjzBaSdFnYPSFcftDvFmW7A5H4Sm9dHjx9hsNghdh359Or9vZeRajoUtVmKC03A0730p9m1wR4wJPDEYETENSGmHt9/eomXR9H6QpI0CCwQ4WpU++iCYJ5Gk1DsSf4VwXUicxm6UggdZuViQGc6JoI4xYYojcrLnIJuF8HGrttMRYhjLk0opA3uT1xUTuO97dGEN72t2723zS8aOYawzFj6ZWcIxC6aqJR3NrzGmhGEQAWLVhEzRGIYBwzjAbcWZZnOs757WBC4AzkuleyMAWxvTo3eqsEjCymq1wiuvvFL8SLYuBB4JBTc3R3vm6qDOUFrgLCUrxVqNOv6xpL3bZipcJREh9Li5npASI6ZYcH0AYvXOYLBQwmy/8s67oJTw0bMVTjZSWUnGUzOps4xfjlKoY+UdOgqgvj6vcRyxixFR1zoDyBOBR9QiOBDhKo9D+tEKdwCFSM6c3FQsEhRFgdGWMGRMk0Jj3kKtWTYP1eR3ntB5X8KcvbcMdpU9ziMEIHhg1W/AZ4/gkYoVE2OUzN3CaHmKNNSw6OCl8HjUSBwhRZT7sKpa9vy3261wyDOAmEoE24ciQ1V2UN/sRNKtdtGWSAWWYhWCtxLAATk5kA+F/CpFRkr2XSzOPTsnM8P3HSioxhukus5qfaLUs8Cq63C+EVQuxojLcUBYneL1Vz6Os9Mn8H6FSBE7DeU78xKbH1kSUpz3ivvFch9vf+lNvP25L9aEo2EAhYDXP/YxPHnyBAAw5Uq1wMyYdqmEUwJVUyk4Z1YMUscoM2NskiJCCDjbnMO5DEZUzbcDc410yRTByKqhSBTRmAhIgM8yqTNbGUHRhkRY57IRI3tReyGxvJwZ4KQCKcB1PfxKy9SRoCW9FvvImcEktS3NXwsY1GCL2qy1usnFGDFNVXOze57PrfY9iwaGCj+EXjh2yrhyLr4cUleCTcMQAlaPHyOkSo3hnINXArqSTS2SDavVSrRYF8H6kIIPmn1qTfhOwL4IKKlIVoU7CCWGHlBc3c1pOJnU15SN0z2VDd6EjUw5K+YeEdOk4ycFMUIXYImATn0JWV/3WrwidBIDHmNEAGNAxjAAeXsDwKklNFVNnCAFdUIvjBS2GeWEHAJ6GrDKI0rheueRNHEnpYgpVYd/+auPsFjyTUSaOJtrdbQa6VaFv0D8arkpzs6cdCwENYhEoIae23mlnQ6i/PWdEyZUVAiPKCsJ4gYgwtn5eeNTIvjVCuMwFuv09PQMV9dXdRP3XrJnlUnSkpYKtQIDqyba7uzsDLe1eyLcq+ZU4RgqD8g+N5PSuw6cHSIYjgI261OMkUHeeCsYXXBqOuvK9BXWMM0rgxFWPb7uox/FR157HX23KVhtJktGYVxdXeHzn/8cbrY3oHCOV159CcGvkBEFpwsBa05iescIciLUfM+KGUux24RaJ9E2HOc93njjDTx9+lQ2lfUpJE1ZBFtWbRmYm9eOarajY6u2LpPYqXYmpj/j8vIZgAznM4CEcbhWb78IoxSnkoDFEIhsUo08J1l0cmvCew7OGMYbxFi5SVqu6pgSph0jZyuw4RHTVLIwRYlWaIYlfC8zF2eSaFsM9gpasXldFmGM1IFof2zaeWWYqlg/mAl3gEpegR2TCWDXxPN3XvlVOpydnuJkvcaJ7zRLucN6tcZJEIFdGAB70QTTyYlQ4/oIkDj8sgMCBaz6FVYrK4zsQdzBqg5ZWCHBsPyMm2FbnLghhALnSYCAJM5ktozXBhKiNkKIS1an8w4r9GB2mEapqkVaftE2rlCYB9WiXQlXS2bhbgI5kO/FUktRHbRzayuRpPrtWDRuJqlsBE1P6NyEYKGkAEAJxANckvXUxYye585vA0yS8gJ5TYCqVCWiJBTnu0jeIuxzFgVQzgk4isg5gjgaHlf4i1hZ62JOgonfjMq1xAgua8JTQPCMvqshsQRRGoGGWiVmicQ5PRW++2mE81Je1DnCMI44Pz/HNE1SHFypCEqoKbPUmFZN/0MRLQM4eHdeTEDzhc1235zLAs08YUpzzHHSAtIqCyALF4BpIRaCNeOqYUzXO7zz1lOcrc8RzmWDSepISiTZYierDq8+eQlfngaMN1d468ufg3cBjr04VVICZaFcFdMWEkq26cCUa2TFwDg7e6lwZ6ScJZplO+HmZhAKW7rUBSoUo+M4Sr3VcdQ43kGw1AZDjcbNkqVC/TSlEnst0I0IRxHmGTmPyjZYnXYw38RsQ0XjdPISQaPPAFQXmwTLGAyh4aQWNVF8CyaOrGl4ZeOTSNyY0gBcsOOtn4bGWhtR4lfsY26vgcIoCV3ke8uB5htGhjjgWOfIauXQdxJfPVz0eBY6dM5Xhx+A3neSCCR3jzhpWTXVKMnIQ0srda1EQGUghMp73ncdvG7eYqJnoNPyhZ3WPPBdSTKzUL0QOgTfSb5FcCAvBc1PT3owSzUz07qZGc53AHfIOWAccgm7k2FxmJLD9ZYLnDSyL9auhd3GUa1q38tmo5CSWOGEqNq21yxx43svjmCfMKFmfyKNwHiNrCykrid4Eqe8JAyO6NQyGqeM7fYGNA7qtFbnc+fRKfxDTiiuvUahOSfMoDlVRYC5hyMgatKhwL0SlpySxtBDfBMuOJ23GTFJQIfXKBwaBOoh9ZGtgiSCrVay2ZCOjToQ0PkOvpTCBLpVD0cZKXU4PV1jGibsdmPJkk0p4frmCkZjcf7oQ5DENE0JX37romBs4EPJOShmKDc7sXnAMxHIhTLxjPejRBsssHtAoKD1WrDO3fYGo3ImT5NomVOq5feur6/x2c9+tsSbAgSXW+cnVOvxBbcnxW9N8IIZnFpzUe6lLbKdJqkXWpxaLf0AIJMYlaa2dbbWwanOoP2mTiboj4F6LEMggvanzWtvgpDrl7Yx5PJ41CMp3zbaVn1+8/5WLVqeXd14ZbOzcwKV0tZ+anmdR2AZ/djKhODQnGo2MkCKaniqCVbBMSgn5DRhnEYkHzAsNaYmx0JtTCwPsb7JRlkLURi8Vlg/nXASdV74ZbquE4WBBA+3nOUM1ELqqGMjFrCEwWYvcd0m5HwX0IUApxz3iR3APbpwAqArMd82t1Jeg9xK4YiAsF6hO1nXWsc6xuQ0fNgH+NCj63okDY2NTj5PZfjn5HhJcWwbL+IAwqkOpMR7dSEBLmMaIt5+9xKX73wZz55d4OnTp3h2cYG8uwanCTkrDbbyRlkI58nmBGenpzjVSkybzab4CdaavJVZKHmdhv5OMRX/BCCwjJDYmc8mIcaAOEWNAqqWpdPIn+2QAEq4GZNuaPLXimobWR+rFeudVaYK6FeEk43DowxMo+D425sbdN1Kr59x+ezqwISu7V4Id4bCtd4pP4qHRTcUAUSVG1ze1wXovAM1zhQTyPbP6jW2WLZp01YZXXixs+K4kziTrMCzCmuLa7UNI08iyGrCD7DbNZsJzYnBwPL4izNUE0rac3brNaJSGhSSs2Zz0lQO+T3KGihjIie83cFYR/2Wbw/urrALFS24jauu30OF8fJ3t12LimVm72cC0oyAmTC+o59cutJ0fd4JWlzEkSvzSgSYmvbqKM52YthzlXlZEtJQhXsLAQIo0KPXKJhpmko4m1Uhs+sG74qWDQAvP3oZo9YWTUn5zIcR06T1T72XPAsbQ6Akv0ntVpQooOK8ZAdwAGePaWKM47X4QMwCTmtwUhK8EJA8IQeaOR69Cv6+k6S+vj/R5D7xZWwePcJqJQrUerVCxCckGAAAIABJREFU368Lt0wXOrhuDfJ90fSZxReQs5DJxXGLdPUVXD57hjfffANvvvEGhutLzSK3yCyAs8BVq+ARKCAnxvUw4OLdK6T4ZiHxE7y8RxckyGG9XgnPkpY9PD09lXqrJ+uSOS4bl/Dc2KYmfFWVvjdzQs7jLA/DQl8HLVQCnQdbFfZwXMgRHUmhHsnm4JIlTgBcv8G6W2N9eg4CY1Ba4LvavRDuzjtsHp0VjBhqkoJRtGiLh56mCXEckaZYohemOCE1TsbC9dJAE8tY6OVrZC7kWIBoI04HvsAqpnmpQHBB6AZqmOL8vviIMAPmQqWFQFgdcVY7sfLNK/e6C8iu9tPlefQAQciejkrSJmLk4DgceH+w/87NoZm9y5jjqoFWmo1siY0zCIkrb4loQjWUT+6tWkrL8xzqBxHNUPpDxyw/y5yRpzZ7WCyQVutGed6ErnMIgcqcIwI6Xzcpc5YVRcSJcJQQ3mm2OVqKehcCvGr3FxcXIBDSmOGyh4se0xgFgkgOLnmkyBjHqFFS1doxQWPkbqUoGAwqE6IDUf6d+IrADWo+gdX3gBxF4+Z5va1IhMjATq0KgpMcEOmCwBAkm6ZUOJP6o6ZVU1jBd6sS8uk8wXeEYRjx7rtPwTlivHxHCpOrc9Inrpg7A7wK6NcbfPIbv1HqnXqBYYbdgGkacXV5he3NjURtjRlb1oQtQilsY0apUWH3K4+1RjBtNhucn5/i6voKJycnOD8/R9+tkRJjsznR9Ro0Wqnee+SMGBPQSax7VnfDFGOh5YhTBk9JHfNAUIepdw7eE0S5kHUgG5RD2Jwi3L1E74dwn6YJ//Szvyme/WnCOIwSYtdo2C1MQcxwTIWzncFzPnfTjFG1tqVAWTYDMspvGm247tZ2DXPAVZ5ugRRSc0VBVvcseDm4vis+gioEiST+1uhwC1QFVg2suVcHIGF2ndzewMG2+O4FhTsDYEvsuCPW1kaUNJLJNH5e9JFB8KSRN+UziVsuAjxXWKaGNrbnWg42101GB3qZtr/XW2dsnJq2D+gGro4t5LJpCb6eQE5r8Oici8oeatFbYZwk69OJcDfWxTqnhRNlGCRscHQOvXcFPhnHEU/fereSiEHyGwTjTnDZZq8DNWUcHbtSyQmoygYbnidXV1I6hqfQzCOC0IVaMZwMQIRz+2ztMbD6EYiFLK22KPs6A5hGKUXJAMUOaUc6v83PJtizZPRq7zT6zB5vjgmUBdrplE6bQw8mjy986U2JLIkZnmqtYbEwehjUp6MAsFTeAliKvcuDxfZmhAupKCNSi0GCHET79wh+jb7f4KMf+Si+/nd8PVbrINmwnRRhgYM8b1UIGEDf9QAT+jI9k1pKmn+QxLcw7CbZJh2DWSBggXsznJKy0R3hv8A9Ee7jMOLzn/1cwW+t2pCFMSkQOxO+aYHLkqXi6cTmnJcyS01SfWWYdWkqZdWp6EJA7wP6XgoXMxN4ll5h6AQXQSOJJvWsjklrqJYrwNZV3SZSESDSJKQtRtEEBbahcm8OvoQ+yjXUOdecmO0eDzbG3mN386OXv1wKYuOawW3XIdO261XtOQIozu75M1hAJhABxo31ModRLA6U6r8ZJMUax84VumLX3IsIxPa6dl/ZHPSom1ERYDYK1GDFdceCWUZs8JVWZuJMSBnNb+T3wUv5xXEnVYHYQ3MTErrQCQUsSKK7FL6JcQK8Q9+tEW9u0MHB5QqTZRao05n2DCAvR3shIISf0SxRfU56zyBNuOOsFMIeKcfFc5Zx9q76lIRcRmiphXOHhUYiZR2TXLVdzLmCGgmva00sewqSBeqDF8oOSxiLEStfayW7xi8gydFW21hXYvtMy6al/j6ar5GUGYBHzoRxEvbLmzzh+otbXKYbfPwTn8D540d46eQRuvUGPnTIeSVQroafhk7olYs/B0DwknC53W2RphHBiS9F7jsiTbui4MYUi9VOoFoD4Ei7F8IdQCP5ynyplY0Wm1SGTAKr82k6S9vCnRrl/vUJdXEYNFIEr87wovUslT9a/NX7mOHDTbUj3vuJbRgy/WKJz7d4ZirHLkXgAR/icdl+VBgf/cGit+0nNarkzus0gl0xi7JhH2+qgYZDWoo5seS1+TSW9+EXAmxPa+fDS8B52Ujb37VRRC08N+sxiXAj1HHpFVOtx9DsryOGdxWX70JAUJrYlsDNwiCdc8hO4MyXXnoJb775JsYpI6XqnM+Q5LZZ2O2iuwXqy6YpCzwmVLP7lq4FMvRdj1dffRVf/OLnMU7D/JzLdZFbegdUCuxcKR1qQMJyPKtvibU4uvcS9WJYdVDsHKiMn4es9PbZybW4fN6ORXt8+3f5vDNlZBLKgTe+9EUwA99y8q0YB0aKA0IH8T24DudnEumS3CQFxNVR3sEhOI9uzfCrE5BnhN6pD1DIzqbdFtvtVmgvcgbiKL4fziD3IWCFJCKpO6jNt6Y582y9EhEScwlZah2Vz+NoO+QAJCIN3+NCFtT3vdSdKTG4FWusGLB+zsZfvT+pqBGMVedvv5+3JRQl2tXhe7krzvW3oy2dhO9Hq4kvc0Kp/RFdjAcdX7T1g/l22Tq3l+1QDL29rt9zCTM1ocxcs0nb89j3Zv6X++RaFQioiXc2N+R8GZwkBwMQCCihlpbMeT4XjRKg7W+d23p/ueaJLPsJiOafuWa87oah4MHyAxSitjI2CPNHpGOx3CiPPZ9WUDv1VcBVp7dZGTY/boNf6xgf/PrgPR9iEWWWqCqn1aNyBt558yv4DXwGLz1+STF7IT80X4IPHryWv5EzhjjBZ4/ggmY3C/3DyEDyHagjBOdwevYI/Tjh6uoKT995B7vrm+JMdncosPdCuM8aQ/hhsJxkzaA3i8ayv5w+6GVJvr3TH3G8ceaSiGJMbLPJr9jHMbg2s0JIs4mz1HgPTazbtdf23tv+vp/NhI4Jl6Vj+XnPcdv7Y8+rFYjz37TQSnu++TGc6zPZ33gPad7H78vGYNnn+XlrDPshATZznhffkMzp8l3OUCq4ma/J/rZRYZeXl3JeZRAkIl0T4hxktkCDuEePvFR0cqLZ/N7rsxpHFmVGhMp8qfduM75VgOaRVASkdPR5Hxpbey2Rcb6s/zqWGZVL5nCb34ut4/nmd1s/9s6XWZKyAPX/Md5680t44/Ofw5UmH1ntZO8DVusV3NkJ1icnJQTzJKxwst5gs1mLctt38Ou+PEcPD8/ip1l3j/DkUcDTyeH6SvIApvihgWWq+Yg0d1qh/NXB1srlhscSoA4+avgj2l3YLkJ7i6tqGIDvV8X8ley3ZkNphezB5y0WxmE9cl8bqd8rAigqU+GXqPsCQ4IfbXLKR+XvohdkZ7wTn5lroe2q3r8HOata5cg5Cex1y2I6+s1cJi++W1yZFtryLScsR/FikRLPOzPTzOy72wVNu0nTbIOX12JFNloe1d4yc9XMQWVOAYAV+yZIKnsGg6PM4+C0ZGAWh1vOGb6xAuCkElhK4pzNnOBIs5+b5D/h9Z9KSUfboNuxrsJ3P8rKIEkCmjQ0uW+nTIuGNta1akMgKfz758wKiywmg0JD8v9WABebGYQseLtCsszZ8hIXWvmeTQx9EPr1XEbsRaXtyXq1HhoufkKdW2JAMOABypOWvWzWuvOIX6EijBwRHMsz9VbovFuhW21KYZDN6gQnqxOcnJxUWnDX4eWXX52FyR5r90K4M2dMu11dyBpuxcdM/wzAVWeXYPDzY5fmVVlzRWDbh43+RoykZEvcKN1VwPDeNG1fSTHhVqQzhD63fCBmq56fHCGlGhXELFETYBSzE5DyamaZgGuEQrnych4vxu0uLZl1DGwxyR/bQdpRkyax+i1z5yGL5ojl1J77BS0Q2+tIhScbF3hzH7z3TJZWT9s7e3ObFdKQ99gmZ9NUlQMfZJx8CYGsgn2IE1KO4kxU4VMxbelPCJ2mqJtwyhidJs+QFCmJWjygPLtFfx0cMllMvsV+Z3CeAEQE1whLv3CkGtcN3W1FtmRl+mF7RONX0rGiAwLIYQah8PxpoMxp0s2z2SiZO5X3Ve2wXAPtQjOOs67NXvLiuwK6Fjky3+gkcEI2F3HsEhJbNqoTVzRHKanIEZymWXQXpwmdm2tjlqGdmJF22i/etx6JSEM2Hbzva42JrkLZh9o9Ee5c+LFNo7ot3IeIFjQjdTAK7e0e9ndYliwxupazYS4XZhJVhfg82mPPtFto8m2/mBnDEAstaz2gjscSAjgEGSz7f0hILfu1xJMPYZ4tbLBsh66xPO42Gl5b0PnAuW9rVbhXm2dfuM+Pp8Ul5l0iSBhFm6uw6LdWADJNuO2DxEcTvFdOmSY136rnbLdbYNyBuOZZtOMvWdIbTUGnMv9tflsoX3DzzXBv5Kiem0ipN7KEq3Y8Z1VctiWN9LHWwnOHIKmZj2FBY3GoHXN8tjkpObOUydu7c9G+5Jj5GnoeuDBT7a8dLc+2anXt3HLERSmR+WvjADBHMBPI5RkyUM45v8HyMjX3af2W8V1GDVnGB8FRxLDb4dnFxZ3Q1r0Q7kRu5lC1oMejD6kqlPr7+sHR3/D+QLa/t0nbmjrcfCe/xfy3eW7S7Q32Qri3AlMm/5yLQjSI2wXnbQ4ne3/X5D50jiWePLvP52jPi6EXofDcZ25/XM9hG8RseyWa5QAAaOKa5QT7uHv9bz1N61MQzb1alfVoIgInqQAGzDfN1sGXYgRpCFw7zsxcaAsq6ZXmbSA3fQEQutnGVCBMe08M4eRXbB5cMqzveo7G0/88beYrOKBszCtlHffNLNffofNng6TGWugkxoiUD2Pk7To+9jmzELhZRjtQlQBb7wAjL4RMhjhR6zyozmcicYJ7NO9zBjU5ARWJuHvDa//O7kNhvRJsccczux/CHbVKCoA92HTZltHRZuYCh4W3XaMVKcvBMf6XUkEGddJXzWau4czS/nHggVQsqPmIqsZARrFgfaywTuWwyGWyHGpt/461Y4trqXW9yAL/attc634xWGbpY9jT3IG5cGdhxp39Yk+4z83v/Y10SXiG2fcMCIkY5gLH7rPrOrjYSW3QA2MsDvyAlG7bDGnfwlv0KoMBngcAWJz9XZbdsu/P0553Diydsu3vW0vj0Lnt3o3iNqWE3TBA9Nh5zdelNXRIeatKGUotXlIoyi/W9hK68Y5neQIEoXuw9UkkdQtMQTQLz6YW2bmbTf826/ao1dvg/B8KzR0AiKXkmvR8GbU+X9LyxzRAGcGSlWn3uwjqtQLPZXConsPOxzlLiJOXij9m5jNLFt+q72amsWsLArQOoPZai/swTYzs3LNEBC7CnbzUSCzZjEfGbe48LVvcbNyOidCKdb6gkJ1dQ86wP894Pr7NBnhYez9+j/Uq8qOiAe9p7vPFQvpZ4xqo91yOizg0QoZdM+0Ld0tMs1NYHdea9EQAC42rA5cNp7Bg2p7PDE4ROUppvNpvLicv8ijdHhnByHMISmL1Zl13So3b3Mns1VygLDVDU0pUAz64GdDsJTPUp2Rf0Wz8JddokTHcbuAKf0Cv23VS7DrzPMksxYRocA6bf0qrSBVzW5Qz7yxTmuqYQLjaiWpWsflZirBPDGrmG2fJBzD4VLhlah0FecY1IIScUADLVyJ2M2uipSkJdQjK/WdeqkBcEgDvavdHuDd63DEtksssqTu7mefOqDTJjl3G8OpufIvZA5YsulmKug0uMXKcZhpvWvBMC8zSCI/FtfavaFVO2/5UDaLQ+KKZZM/xUJchbsuRvAvr3sPxm3O1x8ze75W5n+s+e8cfuOZ70b4a6+MYRHCsX4e00eVndS4DSvUGNmtNPyci5JgxpghP85jlAiVoHzJuF+6H7nrZ830Lr90M6/NaarqttclNzP2h1iYSEVTAUAt3oJTzw+y/VemxRCixxOT3kozmsFp55KR0D9mi64SmwwSt7zqQMsK2kKm8n49j0vuwUijtvdr79u8h+go7d0v0J3c4ld865+HdfEw7ZbG1NnMyH0AW5vLr4PDP2r0R7kC7kCRu1Vo7oKYlm4ltpnHGXPi1wp2INErlkNk9v34rRNuJQURzPH5uQOxdE2h24qbvS8FKWOL1c/MyNw/b7uWu1mKoxxb9iwrTO6/Kc+2dj8AZM3x1eYrfooD/anwES3jqEDxxaCNYLrzZd4tjtVJiheKoJt0JLCCRLseuCeBA6ufifg58dtf4Ei2TYPZ9R4cEffvd3jWbeXrod8eSw46el0jrCjcfYWF4Ow9LJmZm4W0PXXOKeZ7G8h6B/YSu5f20fw/1t4VaLaijXevezytntec69L4dLyIqvpkXCUK4F8KdWepR1rYQFAsBJyZjq7lXGtw6OAthmOcwwTEhvxTutwucZfbjXKAtp/5tmvvyqPbe2j4+j2Zpn5vm1/bleQT7bYv2+DHL8LMXh3ru8h0casvneAj3vu0axxbvbedYbgxLq2p5DeNVKccqWmIL18z32cbXPu9qkB604A4511pFwr53C62b95xbhwMDbvPHvMiGCkDZa2Ydna0AxjySRW5+wtHG0Ipp9fknJri8TxVx6LkuS/hZW+Lhh+aJ/T26Npvftpb88ne3taXsMyv+edbKvRDuOfOecAdobyABW1CVVW+puZv5Z0NbtCqeL5CjES54ESHDwMxi2Ice7pr6+8eYiboI63vBRXTb8S96rjt/w/afuoUIlHX8N8fG5kX6dkiotSyVL3KuZb3e286xnJeHtFKqB8/oCExY25Ei3C3PoC7i2RwkFH60pXC39+kQBk7zNdRqp3WdtNv+8Xt+TxpDycNo9tmsqVpe+svCbnlbi4Ti10gpIU4JjGG2KbUbHLBvgdnnNn+WTLBLS6SNfLpLuI+jRlPdskne1paw7vJZHmv3Qrgzcy0aDWDhI4QI8vkO3GodrVgtD3PpBCOuWmvmuqjIHKEEW3LCKc9F+y/nWODKbqG5Z5pflaxzqnkxCHlG8r7MWQT2Jd7SWbksNcd6/taUxV4/uDn5/lKZa1MVF11oWc11lkJoubctwrL3FrFsyIsPqekLAXuMcc2vGUukQvprxVFcEV5aJq2sjHnMA1jocr2X0nmHlClTusT5l8FH+PC989K3nLUMn/QzuF4yElvMXS0dK9QA9a/YrQcLCND7kJq6XOAIgd6c1jrIM+EzM931IRSWRhMwbONYLdoKEeYq9u2ZE0GSCxcTlJo/ckh5GgBKNJgwq0q45mxWLWsggMCLcotQnHweAGDx5jpX9ZYoC3er8XdaeLrh84axGzFyjVabz2uvDtZyNbcU8q5+puPDmjnfaZ0HZ2MZpYoWyCiURcAtN4461ijnLVuvMWOWNbEnJPfacwl3IvpNAJcQLtPIzN9ORC8D+BEAnwTwmwD+CDM/JenxnwPwPQBuAPw7zPypOy/SmjlHD9EFfGAFtprSgQ/nGaxl/NpoizrhTGtuWRzBTZk5fc8830SWMdaz05bN5I6wxcXXzi0N2UXm3B0Y6Ozc+r0HHfzcmqbt7E8+VDy/4IqHiNgAjdyoY+OaxWDf5APPeoZl723RtTHLIgbP0/5z8z2B4UqG8KFzEYRf925tqmhopBsGanRUEWQ2XuBSbpFIeMfbtPVSO4CsIpFD0C5470HM8AuIxBR5ZlZeF4eu67Hb7Uo93py5pKWnnMttle9FJVYtX2Ei7bGIwsrpJJquCkK7dwZ4NlZ1XZX54ezc8v0SPp8wtzCo+a+c0u0t4Vy0JLuqbZxs71THqc+i/U1ZxmykZXYe43M/0I7Q6c5gW+aSSZ6hmbSkPO4AAmG50rD3SbMptwpTgdD0FxU+ukOiN+1FNPd/lZnfat5/P4CfYuY/S0Tfr+//DIA/COBb9N+/DODP69/nakQ0KzCw991tgr1p8/VK2L/VOq1osXsDOi3aiekInhaCduGQckul2rQJuxYt1dl9wbrv5OLFTxwO6d6HzvXcx9B8+zALp53IxyCIQ9CZHbfU3JfWhDv0LJsfeX/4PktfgFn4HzeLyWCyGrZ4ZGyeqyRh83sC9rBp648tQJLkpKBOtM67WWQuoQb7eq2X2gdXSro5Iqw8lc1BWB5j2UhTSjg5OUVKGZeXl9o3QlRK4GEYsB2lMEZKUgAnxoSkTJGUtYKRQj2GbYte3uQfsFgLClA01plZwcLFbhsVg0E5Ng+aUZKx0GjwraBmCxsuH8wiWogA7+o8bI9e+tfsmna1GbyGys/KqAK+xqnP58dBmAuLeURUeKjEIlKFh7KSrC3XRsX3l34be932pYWRLJhjCSfd1n4rsMwfBvDd+vovAfhZiHD/wwD+F5Ye/39E9BIRvc7MX3reE1sokbXlDR16GEsszRCUchzPq/yUaIVyjGllejhooUZTqYxj700Lt2fnG0HM2ocWySFmuIXmMsf3bRNqtfGIuVjcJ3d6HsdMe4m0nBfLDYYhxU/K18cn0rHJlh0wN+EXwh+AW4ZPHurTbddmKGRShX2xw4gUArhjii9CNo93p51zXmFCnikBpiwyZ3B2yKqtk59vdK3mDlTN2EJfvXKUOw2dW61WcA5CZQBgvV6rICG8/PLLUqdzGHF9c1MzXV0ASCAO13l0AfBq+TqnlmmyHACSouz6zC2GfRnxVGApLlgHiLQMGElEWqAlVj0f2xl+XBQIFbm2ccwUBi65IGa5Y2k9NEmM9Rm1a7k+w+J0XyYtLZUTHG6zQIWmn3Kb1YphMCKWG4TspkvfxzG5NkvsbK9F761DlQH8bZIn9ReZ+QcBfKQR2G8A+Ii+/jiAzzW//bx+dlS4Oyf1I63Rgcyt2U5nPSrf3SXcCWAh9C/XhO7m5aEmzEocwYGbaiyM6rTVM8MST2yH9moaWkszOlSAkOGbcMl9oUyqSS6uszfTGgvjgFPxtmbCaHZlmp/TFt3RCX7k83ZZ8eIae2fj/eIRy2scvM4t2j5jsRdwqyMf63RLgHZ3Yz2n9eXoL9lDy/8cuKb+x/qdM2KsWYshBPR9h5RSqeHLLGXaYooIIWC3Eyfdt37rt+LTn/40hmHAOAr7Y5wiBiaMpFw3pEWufWi0Q4euW0PIqDxSjIhTVs1ZhSob46QFLySw+jAEltAqSrqhgQFvmdVWZJvnsJglGpGTGZGSldFTzZ9IIagq7NFo7vJiLvA6gyp0zg4xIS6S/0xrbx6A/uQ4VfBt665YD/YxmyVa/Whp+ZMjbekcPaQw2Tp/kbX+vML9u5j5C0T0dQB+koj+UfslMzMtt+g7GhH9SQB/EgCcD1idPG6+Ve3D/nvX/XB9jBVXni8sNi27cSyBWpdFLYpRjKhGc2dgVjIPsyOLboAWMlmKadh1y9EHcGBT/0oTJ1393f6mt8RBj4pfcxYdELTztzI2xx4oA8iuCbPkQtI578XCaqFGk3FldBqnUXNFguC3bR/24Ua3Zx20lpElwMw2meW9mtV2pO09Dmr8NEfmJYGQSvk/wGcPxyiwHJM4bz1nEJKc08abCFOcsN1OyuvtkFIERYb3HXIEbmKCcz3GYcSnPvUruLq6wm6cMCXhZE/Jq7NaavoJrawrJQTFapDqT+QYXedwfn6GzJ1sMjnDeVkPEpmhGnviGZ9SzAmTOTWbqDT7PucIl8eaGAiGuZPNwTpxRlZmVEckHDy5Un5nZIwsDkmzLJxtHPIfFIoIvXaChEfao2vnFxPgApXvbYN2hXtKj1zUQz5oMes15S8hWQlHwwefQ7s+tLEU6yAvLafahecRts8l3Jn5C/r3y0T0NwB8B4A3DW4hotcBfFkP/wKAr29+/gn9bHnOHwTwgwDQ92tuaxi2XX+enUomzXw3bkVr+4Dp1uoldxH+A+1UmQ3xXDVoznj83RLrPtanss0dGAvm5xwjObAIq9t+kwm4RSeFnqK+rl0s3y1Pb2K80UXkHdWBawVpi5/PznPHvS5HlNuNUS+1t5ncdk4+8PWtXShIK6KmozugRMsADAoeFjfRZqsyGCknxMiYMor57UiLQTsvEVuZME4J4xQxTRN2uxFDjEhswkkocK06oZWjm/lLCKj1Z0UQd6sN+pXwuLR8LcXnkgmcAAvFm1KSMMSi2c8T/1KMoLQTa9YegRgHJYdk7QmRcykPSDmDUmXhzGAQYrOBMChVa0GQm7nm63MqXPnieKdipQsko5u6OpcJrFp3Cw1NM1kgxUBaTVo3C7MuFnTf+mm971vlylEt4ehUex79/U7hTkSnABwzX+rrPwDgvwTw4wD+OIA/q39/TH/y4wD+QyL6YYgj9eJF8HaBIZ7f9NA+7jn8lkYYN8eW44+Y+MectnOBt6/x3WrFPUc7bh7elkSyvI3Dmuht5ueLN4ZfJMAsu3fIqb/ntqUDEQnNIa7QO8jx7XYqWvABc2G2QT+H1ffC7a4TUsGZiwMUCUxUnHdgcRY7YjgnFX1yzqVQjCMJ2Sul9pzDbhoRVvL3ZogYhhFxEgEUU8SUBSEnIoE8iApmb9jtDLpsLLOUEsZxRMaIvl+VjWAf23VgdsXP0es1TWgyz6mqY4xA7kSJ9SJiU8pa5UxT7QkIOSMFdRgzg7VQjmn7nWrmOWuYqRYbKQlrPM86rWn8EkHkSLwv9plZlcwGibBYC6h4vxVWt7YsjMFsWzNrecL9hMR0gAFxvmHw7O+hNU6LY1+kPY/m/hEAf0MvHAD8ZWb+v4jo5wH8VSL6EwA+C+CP6PH/ByQM8jOQUMh/93k60t4YUYVInqctceelgF8q1bcNZnuuGeZVzkHluPkNyH/2dPFWo/0tCNdjv93/+Dgsc2hiHWqqH93SmwyHufe+DUEEWnuj/oJnUT65+dXh5rM4l4uwaPII5LpL05kArlSuosQ2FaO4Bd+sF/lWeb1MfhNQ5jgffisgyjVyEjvFFIvMYPZwTmlimw1aNN4EDnJsCEHYEMcB0zBgOybcjAnTOBXtEQCy1xh0LVxBRCVpZklFvGwi4Ahp2CHGhNPTU4QQ9lhRebYpKAdMOr7KeafZAAAG/UlEQVSWnHNI6newMNEQZAPjmBRqEWjIUYeUkwjfIBo5GSGUwrScGSlFOC1kYhwzsKTGbHH7c8ikPEOu5G6Zmvci5THX3OfCfb22zUKtBXgw13DglCYtjGIDDRzDCNros2PRZiKHUDaMZdLl88iSO4U7M/8TAP/igc/fBvD7D3zOAP7UnVc+0myO3Ka9Hwq7+1purWa03w4l3hwWms+7uczR8YNXBHhUxVi040QmpOQaPmOGb2cKyDPmw4x8qEqPfa8eWVJHGzOQXNVO5aA4096JvToDUWCols+dAOTFEHCLoR+8VxXwdWLKhtAeszf/5HsrBJGU+qIIWVVeuMCHtXVdh816hRUBjx8/Qt/3uLy8xBAn3AwjxuwwTAkx5XJO0dYdyMnGllmuHZpQyudpjkTw7nY7rNfrmaZPzon10d5rowiFEGbzyzmnmrsHA0UrdyGAnIcj1fAzAzlrcW8nc8gJBBNTEl+NCVFiiLdGwi+TJZOZ45cMWlloxCrcmRV+IQIZ82sR7rH5DYMXigMjonUrMgeAQ7m3adohTgsBfUQstbkhrUW9X+Cb4Rp5aHBYe2+3NXrvTPWvvhHRJYBf+6D78RztVQBv3XnUB98e+vnetod+vnftw9BH4MPTz29g5tcOfXEv6AcA/Bozf/sH3Ym7GhH9wkM/37v20M/3tn0Y+vlh6CPw4ennbe3Fafge2kN7aA/tod379iDcH9pDe2gP7Wuw3Rfh/oMfdAeesz30871tD/18b9uHoZ8fhj4CH55+Hm33wqH60B7aQ3toD+29bfdFc39oD+2hPbSH9h62D1y4E9G/QUS/RkSfIaEO/iD78j8R0ZeJ6Feaz14mop8kon+sf5/o50RE/732++8T0be9T338eiL6GSL6h0T0q0T0H93Tfq6J6O8S0S9rP/8L/fwbiejntD8/QkS9fr7S95/R7z/5fvSz6a8nol8kop+4r/0kot8kon9ARL9ERL+gn92r567XfomI/hoR/SMi+jQRfed96ycR/U4dR/v3jIj+9H3r52+ptbwQ7/c/SBLXbwD4JgA9gF8G8Ls+wP78PgDfBuBXms/+GwDfr6+/H8B/ra+/B8D/CcmP+L0Afu596uPrAL5NX58D+HUAv+se9pMAnOnrDsDP6fX/KoDv08//AoB/X1//BwD+gr7+PgA/8j4/+/8YwF8G8BP6/t71E1IU59XFZ/fqueu1/xKAf09f9wBeuo/9bPrrIcy233Cf+/nC9/WBXhz4TgB/q3n/AwB+4APu0ycXwv3XALyur1+HxOQDwF8E8EcPHfc+9/fHAPzr97mfAE4AfArCNfQWgLB8/gD+FoDv1NdBj6P3qX+fAPBTAP41AD+hC/g+9vOQcL9Xzx3AYwD/dDkm962fi779AQD/z33v54v++6BhmWPc7/epvShv/fvWFBL4PRCt+N71U6GOX4Iwhv4kxEp7l4W4Y9mX0k/9/gLAK+9HPwH8dwD+U1TehlfuaT8ZUlfh75FQZgP377l/I4CvAPifFeb6H0kIB+9bP9v2fQD+ir6+z/18ofZBC/cPVWPZsu9FeBERnQH43wH8aWZ+1n53X/rJzImZfzdEM/4OAP/8B9ylvUZE/yaALzPz3/ug+/Ic7buY+dsgpSz/FBH9vvbLe/LcAwTa/PPM/HsAXEPgjdLuST8BAOpL+UMAfnT53X3q51fTPmjh/lzc7x9we5OErx70VfDW/3Y0Iuoggv1/Y+a/fl/7aY2Z3wXwMxB44yWiUuKq7Uvpp37/GMDb70P3/hUAf4ikCPwPQ6CZP3cP+wlu6ioAmNVV0P7ch+f+eQCfZ+af0/d/DSLs71s/rf1BAJ9i5jf1/X3t5wu3D1q4/zyAb9HIhB5iHv34B9ynZTPeemCft/7fVi/678UL89Z/dY2ICMAPAfg0M/+397ifrxHRS/p6A/ELfBoi5L/3SD+t/98L4KdVc/ptbcz8A8z8CWb+JGT+/TQz/7H71k8iOiWic3sNwYl/BffsuTPzGwA+R0S/Uz/6/QD+4X3rZ9P+KCokY/25j/188fZBg/4QL/SvQ/DY/+wD7stfgdR6nSAayJ+A4Kk/BeAfA/g7AF7WYwnA/6D9/gcAvv196uN3QUzFvw/gl/Tf99zDfv4LAH5R+/krAP5z/fybAPxdCN//jwJY6edrff8Z/f6bPoDn/92o0TL3qp/an1/Wf79qa+W+PXe99u8G8Av67P8mgCf3tJ+nEKvrcfPZvevnV/vvIUP1oT20h/bQvgbbBw3LPLSH9tAe2kP7bWgPwv2hPbSH9tC+BtuDcH9oD+2hPbSvwfYg3B/aQ3toD+1rsD0I94f20B7aQ/sabA/C/aE9tIf20L4G24Nwf2gP7aE9tK/B9iDcH9pDe2gP7Wuw/f9DvmFnMkPrLwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyNT0kdA0yiO"
      },
      "source": [
        "image = torch.zeros((1, 3, height, width)).float() #800)).float()\r\n",
        "\r\n",
        "bbox = torch.FloatTensor([[20, 30, 300, 400], [100, 200, 400, 400]]) # [y1, x1, y2, x2] format\r\n",
        "labels = torch.LongTensor([6, 8]) # 0 represents background\r\n",
        "sub_sample = 16##########################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "9393099d07a1430780a8c3aaac1a4cf2",
            "7d84d6ed145846d7b17a9ad71dfb7687",
            "4e8d40d329a948f591547473e7899e57",
            "6dcd36a13d594295948d1efe9850d3d9",
            "00c634d48e07497d8ea69868e36432df",
            "41861cf80b4640528a9e4ac7cea4035a",
            "46a169ce683342c9bdf82e5ac756da7a",
            "78d51fe5653948a4a2a93768e2cb391f"
          ]
        },
        "id": "1J1TTkZL1CHg",
        "outputId": "120a66dc-47c8-4ef4-c8f5-68546d5c3ece"
      },
      "source": [
        "#Create Network Backbone\r\n",
        "req_features, channels = create_fe_extractor()\r\n",
        "faster_rcnn_fe_extractor = nn.Sequential(*req_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9393099d07a1430780a8c3aaac1a4cf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN5YSOD03R9D",
        "outputId": "6a81926f-7465-4dc4-86d6-e93151b94888"
      },
      "source": [
        "print(faster_rcnn_fe_extractor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcdCRyLU3rGL",
        "outputId": "c6326a41-18ce-46de-d916-fadfb662e928"
      },
      "source": [
        "out_map = faster_rcnn_fe_extractor(image)\r\n",
        "print(out_map.size())\r\n",
        "#Out: torch.Size([1, 512, 32, 50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 32, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO7GbIXv4Fh6",
        "outputId": "ef61043c-9122-4a55-d1f3-a75472cac8de"
      },
      "source": [
        "#create anchors\r\n",
        "ratios = [0.5, 1, 2]###########################################################################################################\r\n",
        "anchor_scales = [8, 16, 32]#####################################################################################################\r\n",
        "n_anchor = len(ratios) * len(anchor_scales)\r\n",
        "print(n_anchor)\r\n",
        "\r\n",
        "anchors = calc_anchors()\r\n",
        "print(anchors.shape)\r\n",
        "    #Out: [14400, 4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "(14400, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FiJI-N65WT6",
        "outputId": "2ce20131-0027-4a99-c730-7f8d26480c1e"
      },
      "source": [
        "print(anchors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ -37.254834    -82.50966799   53.254834     98.50966799]\n",
            " [ -82.50966799 -173.01933598   98.50966799  189.01933598]\n",
            " [-173.01933598 -354.03867197  189.01933598  370.03867197]\n",
            " ...\n",
            " [ 413.49033201  746.745166    594.50966799  837.254834  ]\n",
            " [ 322.98066402  701.49033201  685.01933598  882.50966799]\n",
            " [ 141.96132803  610.98066402  866.03867197  973.01933598]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHc-7p2W7E--"
      },
      "source": [
        "label, valid_anchor_boxes, index_inside = calc_valid_anchors_and_labels(anchors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thf8dffR4ssL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYNtpmHn5JfS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sMH_TA8_MjK"
      },
      "source": [
        "ious = calc_ious(valid_anchor_boxes, bbox.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdO6XO1dxcuD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_qNWmlr5fTl",
        "outputId": "8a496881-6ec2-4df9-c2ed-8c44542cb6de"
      },
      "source": [
        "print(ious[:100])\r\n",
        "#Out: [22500, 2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.11717526 0.        ]\n",
            " [0.12205174 0.        ]\n",
            " [0.12205174 0.        ]\n",
            " [0.12205174 0.        ]\n",
            " [0.12205174 0.        ]\n",
            " [0.12205174 0.        ]\n",
            " [0.12205174 0.        ]\n",
            " [0.11717526 0.        ]\n",
            " [0.10536766 0.        ]\n",
            " [0.09380704 0.        ]\n",
            " [0.08248574 0.        ]\n",
            " [0.07139639 0.        ]\n",
            " [0.06053195 0.        ]\n",
            " [0.04988563 0.        ]\n",
            " [0.03945094 0.        ]\n",
            " [0.02922163 0.        ]\n",
            " [0.01919169 0.        ]\n",
            " [0.00935535 0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.1155177  0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.1469349  0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.15321975 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.15321975 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.15321975 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.15321975 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.15321975 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.15321975 0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.1469349  0.        ]\n",
            " [0.12889279 0.        ]\n",
            " [0.13177474 0.        ]\n",
            " [0.1155177  0.        ]\n",
            " [0.11701012 0.        ]\n",
            " [0.0981696  0.        ]\n",
            " [0.10262577 0.        ]\n",
            " [0.08135281 0.        ]\n",
            " [0.08860717 0.        ]\n",
            " [0.0650433  0.        ]\n",
            " [0.07494056 0.        ]\n",
            " [0.04921846 0.        ]\n",
            " [0.06161284 0.        ]\n",
            " [0.033857   0.        ]\n",
            " [0.04861157 0.        ]\n",
            " [0.01893885 0.        ]\n",
            " [0.03592489 0.        ]\n",
            " [0.00444511 0.        ]\n",
            " [0.02354152 0.        ]\n",
            " [0.         0.        ]\n",
            " [0.01145071 0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.13512549 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15318395 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15977333 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15977333 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15977333 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15977333 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15977333 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15977333 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.15318395 0.        ]\n",
            " [0.15107831 0.        ]\n",
            " [0.13730185 0.        ]\n",
            " [0.13512549 0.        ]\n",
            " [0.12185128 0.        ]\n",
            " [0.11453042 0.        ]\n",
            " [0.10681487 0.        ]\n",
            " [0.09466936 0.        ]\n",
            " [0.09217622 0.        ]\n",
            " [0.07550376 0.        ]\n",
            " [0.07791972 0.        ]\n",
            " [0.05699772 0.        ]\n",
            " [0.06403062 0.        ]\n",
            " [0.03911777 0.        ]\n",
            " [0.05049489 0.        ]\n",
            " [0.02183266 0.        ]\n",
            " [0.03729921 0.        ]\n",
            " [0.0051132  0.        ]\n",
            " [0.02443094 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i7-jjY7Mkdd",
        "outputId": "a024d7c9-4a02-4250-f652-f72ac0100658"
      },
      "source": [
        "ious.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7631334336124749"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFb2T72AMmIj"
      },
      "source": [
        "pos_iou_threshold  = 0.7 #########################################################################\r\n",
        "neg_iou_threshold = 0.3 ##########################################################################\r\n",
        "\r\n",
        "n_sample = 256 ###########################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F47P8DdAUjY5"
      },
      "source": [
        "anchor_labels, anchor_locations = calc_anchor_locations_and_labels(valid_anchor_boxes, ious, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzixSF6gU3Y6",
        "outputId": "4eeb753d-87eb-4248-f150-a1dcbfefd00f"
      },
      "source": [
        "anchor_labels.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ppnfLCVWVJq",
        "outputId": "c7b8972b-4bfd-4d20-9369-7628441d78ec"
      },
      "source": [
        "for loc in anchor_locations:\r\n",
        "  print(loc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mDie letzten 5000Â Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            "[-0.3046875  -0.859375    0.15860503 -0.24686008]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -3.36980575  0.43618575  1.40804633]\n",
            "[-0.46403883 -1.68490288 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -1.68490288  1.12933293  0.71489915]\n",
            "[-0.51928154 -0.60766989  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -2.3828125   0.78275934  1.06147274]\n",
            "[-0.3671875  -0.859375    0.15860503 -0.24686008]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -3.36980575  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -1.68490288  1.12933293  0.71489915]\n",
            "[-0.60766989 -0.60766989  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -2.3828125   0.78275934  1.06147274]\n",
            "[-0.4296875  -0.859375    0.15860503 -0.24686008]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -3.36980575  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -1.68490288  1.12933293  0.71489915]\n",
            "[-0.69605824 -0.60766989  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -2.3828125   0.78275934  1.06147274]\n",
            "[-0.4921875  -0.859375    0.15860503 -0.24686008]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -3.36980575  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -1.68490288  1.12933293  0.71489915]\n",
            "[-0.78444659 -0.60766989  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -2.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -3.36980575  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -1.68490288  1.12933293  0.71489915]\n",
            "[-0.87283493 -0.60766989  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -2.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -3.36980575  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -1.68490288  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -2.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -1.68490288  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -2.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -1.68490288  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -1.77329122  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -1.77329122  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -1.77329122  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.30935922 -0.88664561  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.22097087 -0.88664561  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.13258252 -0.88664561  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -2.5078125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.54137863 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -2.5078125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.45299028 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.36460193 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.06629126 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.27621359 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.11048543 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.18782524 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.15467961 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.09943689 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.19887378 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -1.77329122  1.12933293  0.71489915]\n",
            "[ 0.01104854 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.24306796 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.0773398  -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.28726213 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.16572815 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.3314563  -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.2541165  -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.37565048 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.34250485 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -3.54658245  0.43618575  1.40804633]\n",
            "[-0.41984465 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -1.77329122  1.12933293  0.71489915]\n",
            "[-0.43089319 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -3.54658245  0.43618575  1.40804633]\n",
            "[-0.46403883 -1.77329122 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -1.77329122  1.12933293  0.71489915]\n",
            "[-0.51928154 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.60766989 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.69605824 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -2.5078125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.25390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.78444659 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -1.77329122  1.12933293  0.71489915]\n",
            "[-0.87283493 -0.65186406  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -3.54658245  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -1.77329122  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -1.77329122  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -2.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -1.77329122  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -1.86167957  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -1.86167957  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -1.86167957  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.30935922 -0.93083979  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.22097087 -0.93083979  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.13258252 -0.93083979  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -2.6328125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.54137863 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -2.6328125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.45299028 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.36460193 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.06629126 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.27621359 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.11048543 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.18782524 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.15467961 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.09943689 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.19887378 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -1.86167957  1.12933293  0.71489915]\n",
            "[ 0.01104854 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.24306796 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.0773398  -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.28726213 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.16572815 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.3314563  -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.2541165  -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.37565048 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.34250485 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -3.72335914  0.43618575  1.40804633]\n",
            "[-0.41984465 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -1.86167957  1.12933293  0.71489915]\n",
            "[-0.43089319 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -3.72335914  0.43618575  1.40804633]\n",
            "[-0.46403883 -1.86167957 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -1.86167957  1.12933293  0.71489915]\n",
            "[-0.51928154 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.60766989 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.69605824 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -2.6328125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.31640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.78444659 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -1.86167957  1.12933293  0.71489915]\n",
            "[-0.87283493 -0.69605824  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -3.72335914  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -1.86167957  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -1.86167957  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -2.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -1.86167957  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -1.95006792  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -1.95006792  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -1.95006792  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.30935922 -0.97503396  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.22097087 -0.97503396  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.13258252 -0.97503396  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -2.7578125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.54137863 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -2.7578125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.45299028 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.36460193 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.06629126 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.27621359 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.11048543 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.18782524 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.15467961 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.09943689 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.19887378 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -1.95006792  1.12933293  0.71489915]\n",
            "[ 0.01104854 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.24306796 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.0773398  -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.28726213 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.16572815 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.3314563  -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.2541165  -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.37565048 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.34250485 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -3.90013584  0.43618575  1.40804633]\n",
            "[-0.41984465 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -1.95006792  1.12933293  0.71489915]\n",
            "[-0.43089319 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -3.90013584  0.43618575  1.40804633]\n",
            "[-0.46403883 -1.95006792 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -1.95006792  1.12933293  0.71489915]\n",
            "[-0.51928154 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.60766989 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.69605824 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -2.7578125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.37890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.78444659 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -1.95006792  1.12933293  0.71489915]\n",
            "[-0.87283493 -0.74025241  0.50517862 -0.59343367]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -3.90013584  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -1.95006792  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -1.95006792  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -2.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -1.95006792  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.03845627  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.03845627  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.03845627  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.03845627  1.12933293  0.71489915]\n",
            "[ 0.30935922 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.03845627  1.12933293  0.71489915]\n",
            "[ 0.22097087 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.03845627  1.12933293  0.71489915]\n",
            "[ 0.13258252 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -2.8828125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.03845627  1.12933293  0.71489915]\n",
            "[ 0.04419417 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -2.8828125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.04419417 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.13258252 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.22097087 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.30935922 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.39774756 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.48613591 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.57452426 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.66291261 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.75130096 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.03845627  1.12933293  0.71489915]\n",
            "[-0.8396893  -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -4.07691254  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.03845627  1.12933293  0.71489915]\n",
            "[-0.92807765 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -4.07691254  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.03845627 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.03845627  1.12933293  0.71489915]\n",
            "[-1.016466   -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.03845627  1.12933293  0.71489915]\n",
            "[-1.10485435 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.03845627  1.12933293  0.71489915]\n",
            "[-1.19324269 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -2.8828125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.44140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.03845627  1.12933293  0.71489915]\n",
            "[-1.28163104 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.03845627  1.12933293  0.71489915]\n",
            "[-1.37001939 -1.01922813  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -4.07691254  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.03845627  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.03845627  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -2.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.03845627  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.12684462  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.12684462  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.12684462  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.12684462  1.12933293  0.71489915]\n",
            "[ 0.30935922 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.12684462  1.12933293  0.71489915]\n",
            "[ 0.22097087 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.12684462  1.12933293  0.71489915]\n",
            "[ 0.13258252 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.0078125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.12684462  1.12933293  0.71489915]\n",
            "[ 0.04419417 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.0078125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.04419417 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.13258252 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.22097087 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.30935922 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.39774756 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.48613591 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.57452426 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.66291261 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.75130096 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.12684462  1.12933293  0.71489915]\n",
            "[-0.8396893  -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -4.25368923  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.12684462  1.12933293  0.71489915]\n",
            "[-0.92807765 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -4.25368923  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.12684462 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.12684462  1.12933293  0.71489915]\n",
            "[-1.016466   -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.12684462  1.12933293  0.71489915]\n",
            "[-1.10485435 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.12684462  1.12933293  0.71489915]\n",
            "[-1.19324269 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.0078125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.50390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.12684462  1.12933293  0.71489915]\n",
            "[-1.28163104 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.12684462  1.12933293  0.71489915]\n",
            "[-1.37001939 -1.06342231  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -4.25368923  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.12684462  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.12684462  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.12684462  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.21523296  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.21523296  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.21523296  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.21523296  1.12933293  0.71489915]\n",
            "[ 0.30935922 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.21523296  1.12933293  0.71489915]\n",
            "[ 0.22097087 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.21523296  1.12933293  0.71489915]\n",
            "[ 0.13258252 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.1328125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.21523296  1.12933293  0.71489915]\n",
            "[ 0.04419417 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.1328125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.04419417 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.13258252 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.22097087 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.30935922 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.39774756 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.48613591 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.57452426 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.66291261 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.75130096 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.21523296  1.12933293  0.71489915]\n",
            "[-0.8396893  -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -4.43046593  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.21523296  1.12933293  0.71489915]\n",
            "[-0.92807765 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -4.43046593  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.21523296 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.21523296  1.12933293  0.71489915]\n",
            "[-1.016466   -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.21523296  1.12933293  0.71489915]\n",
            "[-1.10485435 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.21523296  1.12933293  0.71489915]\n",
            "[-1.19324269 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.1328125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.56640625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.21523296  1.12933293  0.71489915]\n",
            "[-1.28163104 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.21523296  1.12933293  0.71489915]\n",
            "[-1.37001939 -1.10761648  0.43618575  0.02175197]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -4.43046593  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.21523296  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.21523296  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.1328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.21523296  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.2578125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.2578125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -4.60724262  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -4.60724262  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.30362131 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.2578125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.62890625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -4.60724262  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.2578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.30362131  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.3828125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.3828125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -4.78401932  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -4.78401932  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.39200966 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.3828125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.69140625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -4.78401932  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.3828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.39200966  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.5078125   0.78275934  1.06147274]\n",
            "[ 0.09375    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.5078125   0.78275934  1.06147274]\n",
            "[ 0.03125    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.03125    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.09375    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.15625    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.21875    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.28125    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.34375    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.40625    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.46875    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.53125    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.59375    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -4.96079601  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.65625    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -4.96079601  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.48039801 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.71875    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.78125    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.5078125   0.78275934  1.06147274]\n",
            "[-0.84375    -1.75390625  0.08961216  0.36832556]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -4.96079601  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.5078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.48039801  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.06629126 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.11048543 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.15467961 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.19887378 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.24306796 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.28726213 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.3314563  -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.37565048 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -5.13757271  0.43618575  1.40804633]\n",
            "[-0.41984465 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -5.13757271  0.43618575  1.40804633]\n",
            "[-0.46403883 -2.56878635 -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -5.13757271  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.6328125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.56878635  1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 1.14904852 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.97227182 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.79549513 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.61871843 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.44194174 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.26516504 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.08838835 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.08838835 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.26516504 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.06629126 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.44194174 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.11048543 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.61871843 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.15467961 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.79549513 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.19887378 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.97227182 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.24306796 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.14904852 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.28726213 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.32582521 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.3314563  -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.50260191 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.37565048 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.67937861 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -5.3143494   0.43618575  1.40804633]\n",
            "[-0.41984465 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8561553  -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -5.3143494   0.43618575  1.40804633]\n",
            "[-0.46403883 -2.6571747  -0.25696143  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.032932   -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.20970869 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.38648539 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.56326208 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.74003878 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -5.3143494   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.91681547 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.09359217 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.7578125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-3.27036886 -2.6571747   1.12933293  0.71489915]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -5.4911261   0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -3.8828125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.6875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.5625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.4375     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.3125     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.1875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.0625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.0625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.1875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.3125     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.4375     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.5625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.6875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8125     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.9375     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.0625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.1875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.3125     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.4375     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.5625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.6875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.8125     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.9375     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -5.66790279  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.0625     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-2.1875     -4.0078125   0.78275934  1.06147274]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.30935922 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.22097087 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.13258252 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[ 0.04419417 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.04419417 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.13258252 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.22097087 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.30935922 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.39774756 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.48613591 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.57452426 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.66291261 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.75130096 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.8396893  -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-0.92807765 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.016466   -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.10485435 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.19324269 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.28163104 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[-1.37001939 -5.84467949  0.43618575  1.40804633]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsf5K4CrWbSm"
      },
      "source": [
        "rpn = RPN(channels, n_anchor) ############auÃŸerhalb der schleife packen !!!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Cvg0Cgk2Fo"
      },
      "source": [
        "pred_anchor_locs, pred_cls_scores = rpn(out_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfHJ9gSglIw8",
        "outputId": "8d930c19-0891-4fc5-c38c-e27706a16ec5"
      },
      "source": [
        "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\r\n",
        "print(pred_anchor_locs.shape)\r\n",
        "#Out: torch.Size([1, 22500, 4])\r\n",
        "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\r\n",
        "print(pred_cls_scores.shape)\r\n",
        "#Out torch.Size([1, 50, 50, 18])\r\n",
        "objectness_score = pred_cls_scores.view(out_map.shape[0], out_map.shape[2], out_map.shape[3], 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\r\n",
        "print(objectness_score.shape)\r\n",
        "#Out torch.Size([1, 22500])\r\n",
        "pred_cls_scores  = pred_cls_scores.view(out_map.shape[0], -1, 2)\r\n",
        "print(pred_cls_scores.shape)\r\n",
        "# Out torch.size([1, 22500, 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 14400, 4])\n",
            "torch.Size([1, 32, 50, 18])\n",
            "torch.Size([1, 14400])\n",
            "torch.Size([1, 14400, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhkUEX1FluEX"
      },
      "source": [
        "#is_training = True #!!!!!!!!?????????!!!!!!!!!???????????!!!!!!!!!!!??????????!!!!!!!!!?????????????????????????\r\n",
        "\r\n",
        "nms_thresh = 0.7 ###############################################################################################\r\n",
        "n_train_pre_nms = 12000 ########################################################################################\r\n",
        "n_train_post_nms = 2000 ########################################################################################\r\n",
        "n_test_pre_nms = 6000 ##########################################################################################\r\n",
        "n_test_post_nms = 300 ##########################################################################################\r\n",
        "min_size = 16 ##################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbsnhMm4l0w5"
      },
      "source": [
        "roi = generate_regions_of_interest(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbCloONSsW6Z"
      },
      "source": [
        "#Proposal targets\r\n",
        "n_sample = 128 #Number of samples to sample from roi, The default value is 128.\r\n",
        "pos_ratio = 0.25 #the number of positive examples out of the n_samples. The default values is 0.25.\r\n",
        "pos_iou_thresh = 0.5 #the number of positive examples out of the n_samples. The default values is 0.25.\r\n",
        "neg_iou_thresh_hi = 0.5 #The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\r\n",
        "neg_iou_thresh_lo = 0.0 #The overlap value bounding required to consider a region proposal as negitive [background object]."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVxG-sOauSy4"
      },
      "source": [
        "ious = calc_ious(roi,  bbox.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZuKPNbjvw2r",
        "outputId": "686ed623-2d3d-4db2-c3d2-c334eb604c41"
      },
      "source": [
        "print(ious.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1700, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bFJZhFbvdDA"
      },
      "source": [
        "sample_roi, gt_roi_labels, gt_roi_locs = create_region_proposals()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQWoo-uyvfxz",
        "outputId": "1a287386-03b2-487d-d535-0dd258cae64c"
      },
      "source": [
        "sample_roi.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2qqDQkhGBzU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XBsRGWMIh7j"
      },
      "source": [
        "fast_rcnn = Fast_RCNN()\r\n",
        "\r\n",
        "roi_pool_out = roi_pooling(sample_roi)\r\n",
        "roi_cls_loc, roi_cls_score = fast_rcnn(roi_pool_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeJ9x6INLf5l",
        "outputId": "28d152d2-ea24-4842-9f02-04b1cded1eac"
      },
      "source": [
        "#calc loss\r\n",
        "print(pred_anchor_locs.shape)\r\n",
        "print(pred_cls_scores.shape)\r\n",
        "print(anchor_locations.shape)\r\n",
        "print(anchor_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 14400, 4])\n",
            "torch.Size([1, 14400, 2])\n",
            "(14400, 4)\n",
            "(14400,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsztMIUmL1rM"
      },
      "source": [
        "rpn_lambda = 10#############################################################################\r\n",
        "rpn_loss = calc_rpn_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mZ0JfHEQzOv",
        "outputId": "76a2718e-81e5-4985-e10b-f67e7106a04c"
      },
      "source": [
        "print(roi_cls_loc.shape)\r\n",
        "print(roi_cls_score.shape)\r\n",
        "print(gt_roi_locs.shape)\r\n",
        "print(gt_roi_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 56])\n",
            "torch.Size([128, 14])\n",
            "(128, 4)\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uctr6KwXPUUt"
      },
      "source": [
        "roi_lambda = 10#############################################################\r\n",
        "roi_loss = calc_fast_rcnn_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH1MbGF0QwyX",
        "outputId": "cbd6b319-8dbc-42b1-ca4c-a1484c906e5a"
      },
      "source": [
        "#total_loss\r\n",
        "total_loss = rpn_loss + roi_loss\r\n",
        "print(total_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(128.1377, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lTXJShMOeh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY25Hiy2Njqn"
      },
      "source": [
        "############################################################################################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRR9rrQZOrmd"
      },
      "source": [
        "#set configs\r\n",
        "height = 512 ##########################################################################################################################\r\n",
        "width = 800 ##########################################################################################################################\r\n",
        "sub_sample = 16##########################################################################################\r\n",
        "ratios = [0.5, 1, 2]###########################################################################################################\r\n",
        "anchor_scales = [8, 16, 32]#####################################################################################################\r\n",
        "pos_iou_threshold  = 0.7 #########################################################################\r\n",
        "neg_iou_threshold = 0.3 ##########################################################################\r\n",
        "n_sample = 256 ###########################################################################################################\r\n",
        "#is_training = True #!!!!!!!!?????????!!!!!!!!!???????????!!!!!!!!!!!??????????!!!!!!!!!?????????????????????????\r\n",
        "\r\n",
        "nms_thresh = 0.7 ###############################################################################################\r\n",
        "n_train_pre_nms = 12000 ########################################################################################\r\n",
        "n_train_post_nms = 2000 ########################################################################################\r\n",
        "n_test_pre_nms = 6000 ##########################################################################################\r\n",
        "n_test_post_nms = 300 ##########################################################################################\r\n",
        "min_size = 16 ##################################################################################################\r\n",
        "\r\n",
        "#Proposal targets\r\n",
        "n_sample = 128 #Number of samples to sample from roi, The default value is 128.\r\n",
        "pos_ratio = 0.25 #the number of positive examples out of the n_samples. The default values is 0.25.\r\n",
        "pos_iou_thresh = 0.5 #the number of positive examples out of the n_samples. The default values is 0.25.\r\n",
        "neg_iou_thresh_hi = 0.5 #The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\r\n",
        "neg_iou_thresh_lo = 0.0 #The overlap value bounding required to consider a region proposal as negitive [background object].\r\n",
        "\r\n",
        "rpn_lambda = 10#############################################################################\r\n",
        "roi_lambda = 10#############################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvUzV10cOsO6"
      },
      "source": [
        "#prepare data\r\n",
        "input = Image.open(\"/content/drive/MyDrive/Computer Vision/0000f77c-6257be58.jpg\") #800x512\r\n",
        "img_size = (height, width)\r\n",
        "n_classes = 13\r\n",
        "transform = transforms.Compose([transforms.Resize((height, width)),\r\n",
        "                                     transforms.ToTensor()])\r\n",
        "input_tensor = transform(input)\r\n",
        "image = torch.zeros((1, 3, height, width)).float() #800)).float()\r\n",
        "\r\n",
        "bbox = torch.FloatTensor([[20, 30, 300, 400], [100, 200, 400, 400]]) # [y1, x1, y2, x2] format\r\n",
        "labels = torch.LongTensor([6, 8]) # 0 represents background"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhRcwL5r9KoI",
        "outputId": "0e62b315-e65c-4ced-9199-20d49aaad7fd"
      },
      "source": [
        "#create anchors\r\n",
        "n_anchor = len(ratios) * len(anchor_scales)\r\n",
        "print(n_anchor)\r\n",
        "\r\n",
        "anchors = calc_anchors()\r\n",
        "print(anchors.shape)\r\n",
        "    #Out: [14400, 4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "(14400, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654,
          "referenced_widgets": [
            "85b41ad3b8f345bfba92929265da5003",
            "cea653080c0649da82722c239aa3c810",
            "195a8d76326040299576f4acd06a4c85",
            "694e425b3597404c8858bba91cff889e",
            "9af17beca2424fe79fec666cff92fb11",
            "fd51a2bfa2d346ae9e1c99b39e5be5f8",
            "379371b9f84544ccbdab51b92b2fa092",
            "a92bb52577344380bd46dba72f5b45db"
          ]
        },
        "id": "q76GrLJc5SMB",
        "outputId": "c3c6bc19-5ce7-4994-fa3d-3f86988d3a50"
      },
      "source": [
        "#set network optimizer etc.\r\n",
        "# set a boolean flag that indicates whether a cuda capable GPU is available\r\n",
        "# we will need this for transferring our tensors to the device and\r\n",
        "# for persistent memory in the data loader\r\n",
        "is_gpu = torch.cuda.is_available()\r\n",
        "print(\"GPU is available:\", is_gpu)\r\n",
        "print(\"If you are receiving False, try setting your runtime to GPU\")\r\n",
        "\r\n",
        "# set the device to cuda if a GPU is available\r\n",
        "device = torch.device(\"cuda\" if is_gpu else \"cpu\")\r\n",
        "\r\n",
        "#create Network Backbone\r\n",
        "req_features, channels = create_fe_extractor()\r\n",
        "faster_rcnn_fe_extractor = nn.Sequential(*req_features).to(device)\r\n",
        "print(faster_rcnn_fe_extractor)\r\n",
        "\r\n",
        "#create RPN\r\n",
        "rpn = RPN(channels, n_anchor).to(device) \r\n",
        "\r\n",
        "#create Fast-RCNN\r\n",
        "fast_rcnn = Fast_RCNN().to(device)\r\n",
        "params = list(faster_rcnn_fe_extractor.parameters()) + list(rpn.parameters()) + list(fast_rcnn.parameters())\r\n",
        "#model.load_state_dict(torch.load(\"/content/drive/My Drive/weights15.pt\"))#####################################################################\r\n",
        "\r\n",
        "#set optimizer for backpropagation\r\n",
        "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available: False\n",
            "If you are receiving False, try setting your runtime to GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85b41ad3b8f345bfba92929265da5003",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQSt1Pxf5jsQ",
        "outputId": "228f2d8e-0bb9-449d-eeaf-cb63eb54b54d"
      },
      "source": [
        "for i in range(1000):\r\n",
        "    print(\"step\",i)\r\n",
        "    #train loop (faster_rcnn_fe_extractor, anchors, )\r\n",
        "    label, valid_anchor_boxes, index_inside = calc_valid_anchors_and_labels(anchors)\r\n",
        "\r\n",
        "    ious = calc_ious(valid_anchor_boxes, bbox.numpy())\r\n",
        "    n_sample = 256 ###########################################################################################################\r\n",
        "    anchor_labels, anchor_locations = calc_anchor_locations_and_labels(valid_anchor_boxes, ious, label)\r\n",
        "\r\n",
        "    image.to(device)###########?!?!??!?!?!????????????????????!!!!!!!!!!!!!!?????????!!!!!!!!???????!!!!!!!!??????!\r\n",
        "    out_map = faster_rcnn_fe_extractor(image).to(device)\r\n",
        "\r\n",
        "    pred_anchor_locs, pred_cls_scores = rpn(out_map)\r\n",
        "\r\n",
        "    pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\r\n",
        "    #print(pred_anchor_locs.shape)\r\n",
        "    #Out: torch.Size([1, 22500, 4])\r\n",
        "    pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\r\n",
        "    #print(pred_cls_scores.shape)\r\n",
        "    #Out torch.Size([1, 50, 50, 18])\r\n",
        "    objectness_score = pred_cls_scores.view(out_map.shape[0], out_map.shape[2], out_map.shape[3], 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\r\n",
        "    #print(objectness_score.shape)\r\n",
        "    #Out torch.Size([1, 22500])\r\n",
        "    pred_cls_scores  = pred_cls_scores.view(out_map.shape[0], -1, 2)\r\n",
        "    #print(pred_cls_scores.shape)\r\n",
        "    # Out torch.size([1, 22500, 2])\r\n",
        "\r\n",
        "    roi = generate_regions_of_interest(True) ############################!!!!!!!!!!!!!!!!!???????????????????!!!!!!!!!!!!!!!??????????????!!!!!!!!!!!!???????????!!!!!!\r\n",
        "\r\n",
        "    n_sample = 128 #Number of samples to sample from roi, The default value is 128.\r\n",
        "    ious = calc_ious(roi,  bbox.numpy())\r\n",
        "    sample_roi, gt_roi_labels, gt_roi_locs = create_region_proposals()\r\n",
        "\r\n",
        "    roi_pool_out = roi_pooling(sample_roi)\r\n",
        "    roi_cls_loc, roi_cls_score = fast_rcnn(roi_pool_out)\r\n",
        "\r\n",
        "    rpn_loss = calc_rpn_loss()\r\n",
        "    print(rpn_loss)\r\n",
        "\r\n",
        "    roi_loss = calc_fast_rcnn_loss()\r\n",
        "    print(roi_loss)\r\n",
        "    #total_loss\r\n",
        "    total_loss = rpn_loss + roi_loss\r\n",
        "    print(total_loss)\r\n",
        "\r\n",
        "    # compute gradient and do the SGD step\r\n",
        "    # we reset the optimizer with zero_grad to \"flush\" former gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    total_loss.backward()\r\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0\n",
            "tensor(1.2669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(181.8224, grad_fn=<AddBackward0>)\n",
            "tensor(183.0893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 1\n",
            "tensor(1.2668, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(180.6567, grad_fn=<AddBackward0>)\n",
            "tensor(181.9235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 2\n",
            "tensor(1.2658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(170.8002, grad_fn=<AddBackward0>)\n",
            "tensor(172.0659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 3\n",
            "tensor(1.2654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(172.4782, grad_fn=<AddBackward0>)\n",
            "tensor(173.7436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 4\n",
            "tensor(1.2654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(163.9523, grad_fn=<AddBackward0>)\n",
            "tensor(165.2177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 5\n",
            "tensor(1.2655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(156.3661, grad_fn=<AddBackward0>)\n",
            "tensor(157.6316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 6\n",
            "tensor(1.2664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(168.7479, grad_fn=<AddBackward0>)\n",
            "tensor(170.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 7\n",
            "tensor(1.2661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(147.5982, grad_fn=<AddBackward0>)\n",
            "tensor(148.8644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 8\n",
            "tensor(1.2658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(172.8744, grad_fn=<AddBackward0>)\n",
            "tensor(174.1402, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 9\n",
            "tensor(1.2653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(170.5537, grad_fn=<AddBackward0>)\n",
            "tensor(171.8191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 10\n",
            "tensor(1.2698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(134.5055, grad_fn=<AddBackward0>)\n",
            "tensor(135.7753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 11\n",
            "tensor(1.2745, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(128.6827, grad_fn=<AddBackward0>)\n",
            "tensor(129.9572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 12\n",
            "tensor(1.2780, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(162.1394, grad_fn=<AddBackward0>)\n",
            "tensor(163.4174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 13\n",
            "tensor(1.2831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(130.6948, grad_fn=<AddBackward0>)\n",
            "tensor(131.9779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 14\n",
            "tensor(1.2879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(106.8755, grad_fn=<AddBackward0>)\n",
            "tensor(108.1634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 15\n",
            "tensor(1.2924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(121.9517, grad_fn=<AddBackward0>)\n",
            "tensor(123.2441, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 16\n",
            "tensor(1.2940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(142.6882, grad_fn=<AddBackward0>)\n",
            "tensor(143.9822, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 17\n",
            "tensor(1.2927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(134.0119, grad_fn=<AddBackward0>)\n",
            "tensor(135.3046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 18\n",
            "tensor(1.2894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(141.2988, grad_fn=<AddBackward0>)\n",
            "tensor(142.5882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 19\n",
            "tensor(1.2846, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(120.9525, grad_fn=<AddBackward0>)\n",
            "tensor(122.2371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 20\n",
            "tensor(1.2744, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(116.4226, grad_fn=<AddBackward0>)\n",
            "tensor(117.6970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 21\n",
            "tensor(1.2638, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(141.3408, grad_fn=<AddBackward0>)\n",
            "tensor(142.6046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 22\n",
            "tensor(1.2567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(118.4073, grad_fn=<AddBackward0>)\n",
            "tensor(119.6640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 23\n",
            "tensor(1.2534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(122.5361, grad_fn=<AddBackward0>)\n",
            "tensor(123.7894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 24\n",
            "tensor(1.2562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(106.6204, grad_fn=<AddBackward0>)\n",
            "tensor(107.8766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 25\n",
            "tensor(1.2590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(110.5481, grad_fn=<AddBackward0>)\n",
            "tensor(111.8071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 26\n",
            "tensor(1.2623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(121.5818, grad_fn=<AddBackward0>)\n",
            "tensor(122.8440, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 27\n",
            "tensor(1.2640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(120.7361, grad_fn=<AddBackward0>)\n",
            "tensor(122.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 28\n",
            "tensor(1.2644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(119.0337, grad_fn=<AddBackward0>)\n",
            "tensor(120.2981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 29\n",
            "tensor(1.2631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(111.2244, grad_fn=<AddBackward0>)\n",
            "tensor(112.4875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 30\n",
            "tensor(1.2545, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(113.9942, grad_fn=<AddBackward0>)\n",
            "tensor(115.2487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 31\n",
            "tensor(1.2431, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(112.8105, grad_fn=<AddBackward0>)\n",
            "tensor(114.0536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 32\n",
            "tensor(1.2331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(123.5079, grad_fn=<AddBackward0>)\n",
            "tensor(124.7410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 33\n",
            "tensor(1.2263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(120.0275, grad_fn=<AddBackward0>)\n",
            "tensor(121.2537, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 34\n",
            "tensor(1.2218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(122.3324, grad_fn=<AddBackward0>)\n",
            "tensor(123.5542, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 35\n",
            "tensor(1.2171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(101.3195, grad_fn=<AddBackward0>)\n",
            "tensor(102.5366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 36\n",
            "tensor(1.2127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(107.6129, grad_fn=<AddBackward0>)\n",
            "tensor(108.8256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 37\n",
            "tensor(1.2104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(91.3760, grad_fn=<AddBackward0>)\n",
            "tensor(92.5864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 38\n",
            "tensor(1.2085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(127.0502, grad_fn=<AddBackward0>)\n",
            "tensor(128.2586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 39\n",
            "tensor(1.2063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(124.9392, grad_fn=<AddBackward0>)\n",
            "tensor(126.1455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 40\n",
            "tensor(1.2043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(127.0334, grad_fn=<AddBackward0>)\n",
            "tensor(128.2377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 41\n",
            "tensor(1.2067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(110.8868, grad_fn=<AddBackward0>)\n",
            "tensor(112.0935, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 42\n",
            "tensor(1.2059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(117.4471, grad_fn=<AddBackward0>)\n",
            "tensor(118.6529, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 43\n",
            "tensor(1.2078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(105.5895, grad_fn=<AddBackward0>)\n",
            "tensor(106.7973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 44\n",
            "tensor(1.2054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(108.8536, grad_fn=<AddBackward0>)\n",
            "tensor(110.0590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 45\n",
            "tensor(1.2020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(115.0132, grad_fn=<AddBackward0>)\n",
            "tensor(116.2152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 46\n",
            "tensor(1.1970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(106.5872, grad_fn=<AddBackward0>)\n",
            "tensor(107.7842, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 47\n",
            "tensor(1.1931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(106.6351, grad_fn=<AddBackward0>)\n",
            "tensor(107.8282, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 48\n",
            "tensor(1.1895, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(115.3735, grad_fn=<AddBackward0>)\n",
            "tensor(116.5630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 49\n",
            "tensor(1.1838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(109.8931, grad_fn=<AddBackward0>)\n",
            "tensor(111.0769, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 50\n",
            "tensor(1.1703, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(100.5133, grad_fn=<AddBackward0>)\n",
            "tensor(101.6836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 51\n",
            "tensor(1.1499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(109.3543, grad_fn=<AddBackward0>)\n",
            "tensor(110.5042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 52\n",
            "tensor(1.1350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(117.5947, grad_fn=<AddBackward0>)\n",
            "tensor(118.7297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 53\n",
            "tensor(1.1358, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(112.8477, grad_fn=<AddBackward0>)\n",
            "tensor(113.9835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 54\n",
            "tensor(1.1729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(88.0287, grad_fn=<AddBackward0>)\n",
            "tensor(89.2015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 55\n",
            "tensor(1.1842, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(90.9461, grad_fn=<AddBackward0>)\n",
            "tensor(92.1303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 56\n",
            "tensor(1.1503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(99.5035, grad_fn=<AddBackward0>)\n",
            "tensor(100.6538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 57\n",
            "tensor(1.0898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(97.5335, grad_fn=<AddBackward0>)\n",
            "tensor(98.6233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 58\n",
            "tensor(1.0646, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(98.2155, grad_fn=<AddBackward0>)\n",
            "tensor(99.2801, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 59\n",
            "tensor(1.0331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(99.4315, grad_fn=<AddBackward0>)\n",
            "tensor(100.4646, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 60\n",
            "tensor(1.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(86.9607, grad_fn=<AddBackward0>)\n",
            "tensor(87.9625, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 61\n",
            "tensor(0.9858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(92.8165, grad_fn=<AddBackward0>)\n",
            "tensor(93.8023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 62\n",
            "tensor(0.9752, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(83.7461, grad_fn=<AddBackward0>)\n",
            "tensor(84.7213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 63\n",
            "tensor(0.9620, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(83.2697, grad_fn=<AddBackward0>)\n",
            "tensor(84.2317, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 64\n",
            "tensor(0.9560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(84.5063, grad_fn=<AddBackward0>)\n",
            "tensor(85.4623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 65\n",
            "tensor(0.9596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(87.6623, grad_fn=<AddBackward0>)\n",
            "tensor(88.6220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 66\n",
            "tensor(0.9647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(75.7467, grad_fn=<AddBackward0>)\n",
            "tensor(76.7114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 67\n",
            "tensor(0.9538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(92.9418, grad_fn=<AddBackward0>)\n",
            "tensor(93.8956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 68\n",
            "tensor(0.9160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(84.5218, grad_fn=<AddBackward0>)\n",
            "tensor(85.4378, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 69\n",
            "tensor(0.8937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(79.8896, grad_fn=<AddBackward0>)\n",
            "tensor(80.7834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 70\n",
            "tensor(0.8848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(80.0303, grad_fn=<AddBackward0>)\n",
            "tensor(80.9151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 71\n",
            "tensor(0.8791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(86.2686, grad_fn=<AddBackward0>)\n",
            "tensor(87.1477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 72\n",
            "tensor(0.8776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(87.0913, grad_fn=<AddBackward0>)\n",
            "tensor(87.9689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 73\n",
            "tensor(0.8807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(83.2103, grad_fn=<AddBackward0>)\n",
            "tensor(84.0910, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 74\n",
            "tensor(0.8701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(69.6625, grad_fn=<AddBackward0>)\n",
            "tensor(70.5326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 75\n",
            "tensor(0.8653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(69.1442, grad_fn=<AddBackward0>)\n",
            "tensor(70.0094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 76\n",
            "tensor(0.8575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(72.6464, grad_fn=<AddBackward0>)\n",
            "tensor(73.5039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 77\n",
            "tensor(0.8455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(79.5491, grad_fn=<AddBackward0>)\n",
            "tensor(80.3947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 78\n",
            "tensor(0.8355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(79.7019, grad_fn=<AddBackward0>)\n",
            "tensor(80.5374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 79\n",
            "tensor(0.8275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(80.2641, grad_fn=<AddBackward0>)\n",
            "tensor(81.0916, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 80\n",
            "tensor(0.8255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(84.4011, grad_fn=<AddBackward0>)\n",
            "tensor(85.2266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 81\n",
            "tensor(0.8167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(78.2659, grad_fn=<AddBackward0>)\n",
            "tensor(79.0826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 82\n",
            "tensor(0.8007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(76.8488, grad_fn=<AddBackward0>)\n",
            "tensor(77.6495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 83\n",
            "tensor(0.7811, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(67.3772, grad_fn=<AddBackward0>)\n",
            "tensor(68.1583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 84\n",
            "tensor(0.7653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(71.1009, grad_fn=<AddBackward0>)\n",
            "tensor(71.8662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 85\n",
            "tensor(0.7529, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(64.8409, grad_fn=<AddBackward0>)\n",
            "tensor(65.5938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 86\n",
            "tensor(0.7439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(70.9772, grad_fn=<AddBackward0>)\n",
            "tensor(71.7211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 87\n",
            "tensor(0.7479, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(74.3241, grad_fn=<AddBackward0>)\n",
            "tensor(75.0720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 88\n",
            "tensor(0.7561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(70.6233, grad_fn=<AddBackward0>)\n",
            "tensor(71.3794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 89\n",
            "tensor(0.7573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(64.2780, grad_fn=<AddBackward0>)\n",
            "tensor(65.0353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 90\n",
            "tensor(0.7503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(68.8075, grad_fn=<AddBackward0>)\n",
            "tensor(69.5578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 91\n",
            "tensor(0.7397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(66.7927, grad_fn=<AddBackward0>)\n",
            "tensor(67.5324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 92\n",
            "tensor(0.7301, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(62.9565, grad_fn=<AddBackward0>)\n",
            "tensor(63.6866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 93\n",
            "tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(57.6261, grad_fn=<AddBackward0>)\n",
            "tensor(58.3518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 94\n",
            "tensor(0.7265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(67.9774, grad_fn=<AddBackward0>)\n",
            "tensor(68.7039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 95\n",
            "tensor(0.7305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(60.8876, grad_fn=<AddBackward0>)\n",
            "tensor(61.6181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 96\n",
            "tensor(0.7292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(70.6280, grad_fn=<AddBackward0>)\n",
            "tensor(71.3572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 97\n",
            "tensor(0.7264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(63.3724, grad_fn=<AddBackward0>)\n",
            "tensor(64.0988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 98\n",
            "tensor(0.7198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(65.0645, grad_fn=<AddBackward0>)\n",
            "tensor(65.7843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 99\n",
            "tensor(0.7093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(63.4545, grad_fn=<AddBackward0>)\n",
            "tensor(64.1638, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 100\n",
            "tensor(0.7064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(65.9135, grad_fn=<AddBackward0>)\n",
            "tensor(66.6199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 101\n",
            "tensor(0.7053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(65.7014, grad_fn=<AddBackward0>)\n",
            "tensor(66.4067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 102\n",
            "tensor(0.7068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(66.5426, grad_fn=<AddBackward0>)\n",
            "tensor(67.2494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 103\n",
            "tensor(0.7066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(62.3244, grad_fn=<AddBackward0>)\n",
            "tensor(63.0310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 104\n",
            "tensor(0.7069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(58.8316, grad_fn=<AddBackward0>)\n",
            "tensor(59.5384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 105\n",
            "tensor(0.7075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(67.2025, grad_fn=<AddBackward0>)\n",
            "tensor(67.9099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 106\n",
            "tensor(0.7076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(60.0709, grad_fn=<AddBackward0>)\n",
            "tensor(60.7785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 107\n",
            "tensor(0.7059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.1215, grad_fn=<AddBackward0>)\n",
            "tensor(52.8273, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 108\n",
            "tensor(0.7090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.8208, grad_fn=<AddBackward0>)\n",
            "tensor(55.5299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 109\n",
            "tensor(0.7120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(68.4441, grad_fn=<AddBackward0>)\n",
            "tensor(69.1561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 110\n",
            "tensor(0.7148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(59.9893, grad_fn=<AddBackward0>)\n",
            "tensor(60.7041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 111\n",
            "tensor(0.7189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.8974, grad_fn=<AddBackward0>)\n",
            "tensor(55.6163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 112\n",
            "tensor(0.7219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(61.5968, grad_fn=<AddBackward0>)\n",
            "tensor(62.3188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 113\n",
            "tensor(0.7128, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(61.3328, grad_fn=<AddBackward0>)\n",
            "tensor(62.0456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 114\n",
            "tensor(0.7062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(64.4361, grad_fn=<AddBackward0>)\n",
            "tensor(65.1424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 115\n",
            "tensor(0.7000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3621, grad_fn=<AddBackward0>)\n",
            "tensor(44.0621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 116\n",
            "tensor(0.6976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(66.9372, grad_fn=<AddBackward0>)\n",
            "tensor(67.6347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 117\n",
            "tensor(0.6933, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(56.2156, grad_fn=<AddBackward0>)\n",
            "tensor(56.9089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 118\n",
            "tensor(0.6927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(64.7828, grad_fn=<AddBackward0>)\n",
            "tensor(65.4755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 119\n",
            "tensor(0.6924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(70.5769, grad_fn=<AddBackward0>)\n",
            "tensor(71.2693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 120\n",
            "tensor(0.6885, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(64.8848, grad_fn=<AddBackward0>)\n",
            "tensor(65.5733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 121\n",
            "tensor(0.6847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(58.8068, grad_fn=<AddBackward0>)\n",
            "tensor(59.4915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 122\n",
            "tensor(0.6847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(59.3111, grad_fn=<AddBackward0>)\n",
            "tensor(59.9958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 123\n",
            "tensor(0.6840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(57.4290, grad_fn=<AddBackward0>)\n",
            "tensor(58.1130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 124\n",
            "tensor(0.6844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(57.5242, grad_fn=<AddBackward0>)\n",
            "tensor(58.2086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 125\n",
            "tensor(0.6847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.5486, grad_fn=<AddBackward0>)\n",
            "tensor(50.2333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 126\n",
            "tensor(0.6851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.8261, grad_fn=<AddBackward0>)\n",
            "tensor(47.5112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 127\n",
            "tensor(0.6862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(61.3946, grad_fn=<AddBackward0>)\n",
            "tensor(62.0808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 128\n",
            "tensor(0.6864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(58.3700, grad_fn=<AddBackward0>)\n",
            "tensor(59.0564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 129\n",
            "tensor(0.6847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.5009, grad_fn=<AddBackward0>)\n",
            "tensor(53.1857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 130\n",
            "tensor(0.6803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.6926, grad_fn=<AddBackward0>)\n",
            "tensor(56.3729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 131\n",
            "tensor(0.6817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.1156, grad_fn=<AddBackward0>)\n",
            "tensor(47.7973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 132\n",
            "tensor(0.6860, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(53.3526, grad_fn=<AddBackward0>)\n",
            "tensor(54.0386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 133\n",
            "tensor(0.6887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.4909, grad_fn=<AddBackward0>)\n",
            "tensor(56.1796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 134\n",
            "tensor(0.6842, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.9346, grad_fn=<AddBackward0>)\n",
            "tensor(51.6188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 135\n",
            "tensor(0.6768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.6717, grad_fn=<AddBackward0>)\n",
            "tensor(51.3486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 136\n",
            "tensor(0.6726, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.6575, grad_fn=<AddBackward0>)\n",
            "tensor(56.3300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 137\n",
            "tensor(0.6712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.6657, grad_fn=<AddBackward0>)\n",
            "tensor(56.3369, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 138\n",
            "tensor(0.6690, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.3204, grad_fn=<AddBackward0>)\n",
            "tensor(50.9894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 139\n",
            "tensor(0.6681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(56.8100, grad_fn=<AddBackward0>)\n",
            "tensor(57.4781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 140\n",
            "tensor(0.6686, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(58.3721, grad_fn=<AddBackward0>)\n",
            "tensor(59.0407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 141\n",
            "tensor(0.6673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.7209, grad_fn=<AddBackward0>)\n",
            "tensor(56.3882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 142\n",
            "tensor(0.6661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(53.7644, grad_fn=<AddBackward0>)\n",
            "tensor(54.4305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 143\n",
            "tensor(0.6653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(56.8612, grad_fn=<AddBackward0>)\n",
            "tensor(57.5265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 144\n",
            "tensor(0.6651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.1009, grad_fn=<AddBackward0>)\n",
            "tensor(50.7660, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 145\n",
            "tensor(0.6672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.6039, grad_fn=<AddBackward0>)\n",
            "tensor(51.2711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 146\n",
            "tensor(0.6666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.9221, grad_fn=<AddBackward0>)\n",
            "tensor(50.5887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 147\n",
            "tensor(0.6676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.6878, grad_fn=<AddBackward0>)\n",
            "tensor(49.3553, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 148\n",
            "tensor(0.6698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(53.4706, grad_fn=<AddBackward0>)\n",
            "tensor(54.1404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 149\n",
            "tensor(0.6679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(57.2572, grad_fn=<AddBackward0>)\n",
            "tensor(57.9251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 150\n",
            "tensor(0.6624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.1070, grad_fn=<AddBackward0>)\n",
            "tensor(52.7694, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 151\n",
            "tensor(0.6589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.8859, grad_fn=<AddBackward0>)\n",
            "tensor(55.5448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 152\n",
            "tensor(0.6573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.2432, grad_fn=<AddBackward0>)\n",
            "tensor(55.9005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 153\n",
            "tensor(0.6567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.5304, grad_fn=<AddBackward0>)\n",
            "tensor(51.1872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 154\n",
            "tensor(0.6547, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.0473, grad_fn=<AddBackward0>)\n",
            "tensor(54.7020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 155\n",
            "tensor(0.6551, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.1258, grad_fn=<AddBackward0>)\n",
            "tensor(48.7808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 156\n",
            "tensor(0.6559, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.7715, grad_fn=<AddBackward0>)\n",
            "tensor(55.4274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 157\n",
            "tensor(0.6536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(55.4282, grad_fn=<AddBackward0>)\n",
            "tensor(56.0818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 158\n",
            "tensor(0.6518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(59.1200, grad_fn=<AddBackward0>)\n",
            "tensor(59.7718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 159\n",
            "tensor(0.6496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.5430, grad_fn=<AddBackward0>)\n",
            "tensor(52.1926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 160\n",
            "tensor(0.6496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.2352, grad_fn=<AddBackward0>)\n",
            "tensor(50.8848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 161\n",
            "tensor(0.6492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.7170, grad_fn=<AddBackward0>)\n",
            "tensor(50.3662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 162\n",
            "tensor(0.6483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(53.5417, grad_fn=<AddBackward0>)\n",
            "tensor(54.1900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 163\n",
            "tensor(0.6477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(58.3261, grad_fn=<AddBackward0>)\n",
            "tensor(58.9738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 164\n",
            "tensor(0.6474, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.3619, grad_fn=<AddBackward0>)\n",
            "tensor(53.0093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 165\n",
            "tensor(0.6519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.6258, grad_fn=<AddBackward0>)\n",
            "tensor(52.2777, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 166\n",
            "tensor(0.6632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(62.2461, grad_fn=<AddBackward0>)\n",
            "tensor(62.9093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 167\n",
            "tensor(0.6741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(56.0928, grad_fn=<AddBackward0>)\n",
            "tensor(56.7669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 168\n",
            "tensor(0.6823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.7404, grad_fn=<AddBackward0>)\n",
            "tensor(47.4228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 169\n",
            "tensor(0.6836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.5565, grad_fn=<AddBackward0>)\n",
            "tensor(49.2400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 170\n",
            "tensor(0.6833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.7642, grad_fn=<AddBackward0>)\n",
            "tensor(52.4475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 171\n",
            "tensor(0.6788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.0686, grad_fn=<AddBackward0>)\n",
            "tensor(48.7475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 172\n",
            "tensor(0.6732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.8298, grad_fn=<AddBackward0>)\n",
            "tensor(52.5030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 173\n",
            "tensor(0.6695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.4417, grad_fn=<AddBackward0>)\n",
            "tensor(48.1112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 174\n",
            "tensor(0.6687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.0227, grad_fn=<AddBackward0>)\n",
            "tensor(47.6914, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 175\n",
            "tensor(0.6684, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6351, grad_fn=<AddBackward0>)\n",
            "tensor(48.3034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 176\n",
            "tensor(0.6666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.0427, grad_fn=<AddBackward0>)\n",
            "tensor(52.7093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 177\n",
            "tensor(0.6642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.9116, grad_fn=<AddBackward0>)\n",
            "tensor(52.5759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 178\n",
            "tensor(0.6625, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.2621, grad_fn=<AddBackward0>)\n",
            "tensor(45.9246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 179\n",
            "tensor(0.6590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.3280, grad_fn=<AddBackward0>)\n",
            "tensor(47.9870, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 180\n",
            "tensor(0.6568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.4849, grad_fn=<AddBackward0>)\n",
            "tensor(51.1417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 181\n",
            "tensor(0.6583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(53.1581, grad_fn=<AddBackward0>)\n",
            "tensor(53.8163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 182\n",
            "tensor(0.6633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.0605, grad_fn=<AddBackward0>)\n",
            "tensor(44.7238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 183\n",
            "tensor(0.6678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.4160, grad_fn=<AddBackward0>)\n",
            "tensor(53.0838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 184\n",
            "tensor(0.6653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.7526, grad_fn=<AddBackward0>)\n",
            "tensor(44.4179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 185\n",
            "tensor(0.6592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.9244, grad_fn=<AddBackward0>)\n",
            "tensor(49.5836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 186\n",
            "tensor(0.6539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.5091, grad_fn=<AddBackward0>)\n",
            "tensor(50.1630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 187\n",
            "tensor(0.6566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.6009, grad_fn=<AddBackward0>)\n",
            "tensor(53.2575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 188\n",
            "tensor(0.6589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.7604, grad_fn=<AddBackward0>)\n",
            "tensor(44.4193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 189\n",
            "tensor(0.6611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.8426, grad_fn=<AddBackward0>)\n",
            "tensor(47.5037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 190\n",
            "tensor(0.6592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6520, grad_fn=<AddBackward0>)\n",
            "tensor(45.3112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 191\n",
            "tensor(0.6596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.6001, grad_fn=<AddBackward0>)\n",
            "tensor(47.2597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 192\n",
            "tensor(0.6571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.5382, grad_fn=<AddBackward0>)\n",
            "tensor(46.1953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 193\n",
            "tensor(0.6588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.5431, grad_fn=<AddBackward0>)\n",
            "tensor(46.2020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 194\n",
            "tensor(0.6606, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.4737, grad_fn=<AddBackward0>)\n",
            "tensor(46.1343, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 195\n",
            "tensor(0.6622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.2338, grad_fn=<AddBackward0>)\n",
            "tensor(42.8960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 196\n",
            "tensor(0.6618, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.3096, grad_fn=<AddBackward0>)\n",
            "tensor(50.9714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 197\n",
            "tensor(0.6651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.9137, grad_fn=<AddBackward0>)\n",
            "tensor(47.5788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 198\n",
            "tensor(0.6734, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.0961, grad_fn=<AddBackward0>)\n",
            "tensor(40.7694, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 199\n",
            "tensor(0.6703, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.8589, grad_fn=<AddBackward0>)\n",
            "tensor(52.5291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 200\n",
            "tensor(0.6695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.1343, grad_fn=<AddBackward0>)\n",
            "tensor(46.8038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 201\n",
            "tensor(0.6671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.2148, grad_fn=<AddBackward0>)\n",
            "tensor(46.8819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 202\n",
            "tensor(0.6613, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2562, grad_fn=<AddBackward0>)\n",
            "tensor(44.9175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 203\n",
            "tensor(0.6549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.7823, grad_fn=<AddBackward0>)\n",
            "tensor(45.4372, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 204\n",
            "tensor(0.6529, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.5131, grad_fn=<AddBackward0>)\n",
            "tensor(52.1659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 205\n",
            "tensor(0.6482, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8894, grad_fn=<AddBackward0>)\n",
            "tensor(44.5376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 206\n",
            "tensor(0.6446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.5999, grad_fn=<AddBackward0>)\n",
            "tensor(50.2445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 207\n",
            "tensor(0.6432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.5563, grad_fn=<AddBackward0>)\n",
            "tensor(50.1995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 208\n",
            "tensor(0.6459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6134, grad_fn=<AddBackward0>)\n",
            "tensor(48.2593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 209\n",
            "tensor(0.6516, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(53.4706, grad_fn=<AddBackward0>)\n",
            "tensor(54.1222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 210\n",
            "tensor(0.6542, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.9760, grad_fn=<AddBackward0>)\n",
            "tensor(52.6302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 211\n",
            "tensor(0.6541, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.3045, grad_fn=<AddBackward0>)\n",
            "tensor(50.9586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 212\n",
            "tensor(0.6559, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0043, grad_fn=<AddBackward0>)\n",
            "tensor(42.6602, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 213\n",
            "tensor(0.6564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0841, grad_fn=<AddBackward0>)\n",
            "tensor(42.7405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 214\n",
            "tensor(0.6585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1491, grad_fn=<AddBackward0>)\n",
            "tensor(45.8076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 215\n",
            "tensor(0.6645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1021, grad_fn=<AddBackward0>)\n",
            "tensor(36.7666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 216\n",
            "tensor(0.6679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9035, grad_fn=<AddBackward0>)\n",
            "tensor(40.5714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 217\n",
            "tensor(0.6676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.7614, grad_fn=<AddBackward0>)\n",
            "tensor(48.4291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 218\n",
            "tensor(0.6650, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.7481, grad_fn=<AddBackward0>)\n",
            "tensor(40.4131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 219\n",
            "tensor(0.6586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.8423, grad_fn=<AddBackward0>)\n",
            "tensor(55.5010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 220\n",
            "tensor(0.6544, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8091, grad_fn=<AddBackward0>)\n",
            "tensor(40.4635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 221\n",
            "tensor(0.6503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.9789, grad_fn=<AddBackward0>)\n",
            "tensor(46.6292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 222\n",
            "tensor(0.6455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.7502, grad_fn=<AddBackward0>)\n",
            "tensor(34.3957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 223\n",
            "tensor(0.6417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4503, grad_fn=<AddBackward0>)\n",
            "tensor(45.0920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 224\n",
            "tensor(0.6406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.3271, grad_fn=<AddBackward0>)\n",
            "tensor(44.9677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 225\n",
            "tensor(0.6414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1143, grad_fn=<AddBackward0>)\n",
            "tensor(45.7557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 226\n",
            "tensor(0.6408, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.2913, grad_fn=<AddBackward0>)\n",
            "tensor(49.9320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 227\n",
            "tensor(0.6369, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.8067, grad_fn=<AddBackward0>)\n",
            "tensor(50.4436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 228\n",
            "tensor(0.6325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.0872, grad_fn=<AddBackward0>)\n",
            "tensor(44.7197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 229\n",
            "tensor(0.6301, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8403, grad_fn=<AddBackward0>)\n",
            "tensor(42.4704, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 230\n",
            "tensor(0.6288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(58.1109, grad_fn=<AddBackward0>)\n",
            "tensor(58.7397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 231\n",
            "tensor(0.6288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.1410, grad_fn=<AddBackward0>)\n",
            "tensor(46.7698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 232\n",
            "tensor(0.6282, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.1943, grad_fn=<AddBackward0>)\n",
            "tensor(54.8225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 233\n",
            "tensor(0.6313, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6401, grad_fn=<AddBackward0>)\n",
            "tensor(44.2714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 234\n",
            "tensor(0.6366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.6482, grad_fn=<AddBackward0>)\n",
            "tensor(50.2848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 235\n",
            "tensor(0.6393, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1648, grad_fn=<AddBackward0>)\n",
            "tensor(45.8041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 236\n",
            "tensor(0.6374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.8773, grad_fn=<AddBackward0>)\n",
            "tensor(39.5147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 237\n",
            "tensor(0.6339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9394, grad_fn=<AddBackward0>)\n",
            "tensor(41.5733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 238\n",
            "tensor(0.6320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3904, grad_fn=<AddBackward0>)\n",
            "tensor(44.0224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 239\n",
            "tensor(0.6314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4653, grad_fn=<AddBackward0>)\n",
            "tensor(45.0968, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 240\n",
            "tensor(0.6318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8544, grad_fn=<AddBackward0>)\n",
            "tensor(44.4863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 241\n",
            "tensor(0.6336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1069, grad_fn=<AddBackward0>)\n",
            "tensor(41.7405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 242\n",
            "tensor(0.6375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2494, grad_fn=<AddBackward0>)\n",
            "tensor(44.8870, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 243\n",
            "tensor(0.6393, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.4623, grad_fn=<AddBackward0>)\n",
            "tensor(47.1015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 244\n",
            "tensor(0.6367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8724, grad_fn=<AddBackward0>)\n",
            "tensor(40.5091, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 245\n",
            "tensor(0.6314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.0690, grad_fn=<AddBackward0>)\n",
            "tensor(43.7003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 246\n",
            "tensor(0.6275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6189, grad_fn=<AddBackward0>)\n",
            "tensor(48.2464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 247\n",
            "tensor(0.6265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.2686, grad_fn=<AddBackward0>)\n",
            "tensor(43.8951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 248\n",
            "tensor(0.6273, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9607, grad_fn=<AddBackward0>)\n",
            "tensor(44.5879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 249\n",
            "tensor(0.6294, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.3026, grad_fn=<AddBackward0>)\n",
            "tensor(40.9320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 250\n",
            "tensor(0.6299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.1365, grad_fn=<AddBackward0>)\n",
            "tensor(43.7663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 251\n",
            "tensor(0.6278, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.8110, grad_fn=<AddBackward0>)\n",
            "tensor(45.4388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 252\n",
            "tensor(0.6258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.1692, grad_fn=<AddBackward0>)\n",
            "tensor(43.7950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 253\n",
            "tensor(0.6226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9875, grad_fn=<AddBackward0>)\n",
            "tensor(44.6102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 254\n",
            "tensor(0.6245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4605, grad_fn=<AddBackward0>)\n",
            "tensor(45.0850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 255\n",
            "tensor(0.6296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3238, grad_fn=<AddBackward0>)\n",
            "tensor(39.9535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 256\n",
            "tensor(0.6350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.3821, grad_fn=<AddBackward0>)\n",
            "tensor(45.0171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 257\n",
            "tensor(0.6364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2988, grad_fn=<AddBackward0>)\n",
            "tensor(44.9352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 258\n",
            "tensor(0.6344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.3596, grad_fn=<AddBackward0>)\n",
            "tensor(42.9940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 259\n",
            "tensor(0.6318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8519, grad_fn=<AddBackward0>)\n",
            "tensor(42.4837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 260\n",
            "tensor(0.6299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.7827, grad_fn=<AddBackward0>)\n",
            "tensor(52.4126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 261\n",
            "tensor(0.6285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6211, grad_fn=<AddBackward0>)\n",
            "tensor(41.2497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 262\n",
            "tensor(0.6250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.5904, grad_fn=<AddBackward0>)\n",
            "tensor(49.2154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 263\n",
            "tensor(0.6217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.9730, grad_fn=<AddBackward0>)\n",
            "tensor(45.5947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 264\n",
            "tensor(0.6199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6200, grad_fn=<AddBackward0>)\n",
            "tensor(44.2399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 265\n",
            "tensor(0.6198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.9807, grad_fn=<AddBackward0>)\n",
            "tensor(45.6005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 266\n",
            "tensor(0.6202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3199, grad_fn=<AddBackward0>)\n",
            "tensor(43.9400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 267\n",
            "tensor(0.6209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4399, grad_fn=<AddBackward0>)\n",
            "tensor(45.0608, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 268\n",
            "tensor(0.6226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.3519, grad_fn=<AddBackward0>)\n",
            "tensor(49.9745, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 269\n",
            "tensor(0.6224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8529, grad_fn=<AddBackward0>)\n",
            "tensor(43.4753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 270\n",
            "tensor(0.6226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.9523, grad_fn=<AddBackward0>)\n",
            "tensor(47.5749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 271\n",
            "tensor(0.6215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9272, grad_fn=<AddBackward0>)\n",
            "tensor(44.5487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 272\n",
            "tensor(0.6205, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9691, grad_fn=<AddBackward0>)\n",
            "tensor(40.5896, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 273\n",
            "tensor(0.6186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6393, grad_fn=<AddBackward0>)\n",
            "tensor(44.2579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 274\n",
            "tensor(0.6155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1209, grad_fn=<AddBackward0>)\n",
            "tensor(41.7365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 275\n",
            "tensor(0.6135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.2331, grad_fn=<AddBackward0>)\n",
            "tensor(48.8466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 276\n",
            "tensor(0.6120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.0196, grad_fn=<AddBackward0>)\n",
            "tensor(38.6316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 277\n",
            "tensor(0.6111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.2706, grad_fn=<AddBackward0>)\n",
            "tensor(40.8817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 278\n",
            "tensor(0.6126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.4228, grad_fn=<AddBackward0>)\n",
            "tensor(43.0355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 279\n",
            "tensor(0.6139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.4370, grad_fn=<AddBackward0>)\n",
            "tensor(47.0509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 280\n",
            "tensor(0.6136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.1083, grad_fn=<AddBackward0>)\n",
            "tensor(47.7220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 281\n",
            "tensor(0.6133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.2874, grad_fn=<AddBackward0>)\n",
            "tensor(38.9007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 282\n",
            "tensor(0.6151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0609, grad_fn=<AddBackward0>)\n",
            "tensor(42.6760, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 283\n",
            "tensor(0.6200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.4749, grad_fn=<AddBackward0>)\n",
            "tensor(46.0949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 284\n",
            "tensor(0.6238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2874, grad_fn=<AddBackward0>)\n",
            "tensor(41.9112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 285\n",
            "tensor(0.6233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.8607, grad_fn=<AddBackward0>)\n",
            "tensor(37.4840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 286\n",
            "tensor(0.6202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.1546, grad_fn=<AddBackward0>)\n",
            "tensor(50.7748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 287\n",
            "tensor(0.6162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.7961, grad_fn=<AddBackward0>)\n",
            "tensor(41.4123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 288\n",
            "tensor(0.6123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.3030, grad_fn=<AddBackward0>)\n",
            "tensor(51.9152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 289\n",
            "tensor(0.6095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.4513, grad_fn=<AddBackward0>)\n",
            "tensor(47.0608, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 290\n",
            "tensor(0.6094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1345, grad_fn=<AddBackward0>)\n",
            "tensor(45.7439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 291\n",
            "tensor(0.6098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.0558, grad_fn=<AddBackward0>)\n",
            "tensor(44.6656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 292\n",
            "tensor(0.6097, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9877, grad_fn=<AddBackward0>)\n",
            "tensor(44.5974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 293\n",
            "tensor(0.6088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8691, grad_fn=<AddBackward0>)\n",
            "tensor(43.4779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 294\n",
            "tensor(0.6065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9493, grad_fn=<AddBackward0>)\n",
            "tensor(40.5558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 295\n",
            "tensor(0.6045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.8283, grad_fn=<AddBackward0>)\n",
            "tensor(45.4328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 296\n",
            "tensor(0.6039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1751, grad_fn=<AddBackward0>)\n",
            "tensor(38.7790, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 297\n",
            "tensor(0.6043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8349, grad_fn=<AddBackward0>)\n",
            "tensor(42.4392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 298\n",
            "tensor(0.6036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.6167, grad_fn=<AddBackward0>)\n",
            "tensor(46.2202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 299\n",
            "tensor(0.6012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.0383, grad_fn=<AddBackward0>)\n",
            "tensor(48.6395, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 300\n",
            "tensor(0.5993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.1001, grad_fn=<AddBackward0>)\n",
            "tensor(37.6994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 301\n",
            "tensor(0.5993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0254, grad_fn=<AddBackward0>)\n",
            "tensor(42.6248, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 302\n",
            "tensor(0.6001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.5175, grad_fn=<AddBackward0>)\n",
            "tensor(38.1176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 303\n",
            "tensor(0.6001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4940, grad_fn=<AddBackward0>)\n",
            "tensor(40.0941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 304\n",
            "tensor(0.5990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.3264, grad_fn=<AddBackward0>)\n",
            "tensor(33.9254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 305\n",
            "tensor(0.5992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.4419, grad_fn=<AddBackward0>)\n",
            "tensor(49.0410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 306\n",
            "tensor(0.5986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.0190, grad_fn=<AddBackward0>)\n",
            "tensor(48.6176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 307\n",
            "tensor(0.6000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9048, grad_fn=<AddBackward0>)\n",
            "tensor(40.5048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 308\n",
            "tensor(0.6031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8166, grad_fn=<AddBackward0>)\n",
            "tensor(42.4197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 309\n",
            "tensor(0.6048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4711, grad_fn=<AddBackward0>)\n",
            "tensor(40.0759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 310\n",
            "tensor(0.6031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.8053, grad_fn=<AddBackward0>)\n",
            "tensor(48.4084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 311\n",
            "tensor(0.6005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3225, grad_fn=<AddBackward0>)\n",
            "tensor(39.9230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 312\n",
            "tensor(0.5986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1221, grad_fn=<AddBackward0>)\n",
            "tensor(41.7208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 313\n",
            "tensor(0.5988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.7804, grad_fn=<AddBackward0>)\n",
            "tensor(51.3793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 314\n",
            "tensor(0.5988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.8357, grad_fn=<AddBackward0>)\n",
            "tensor(50.4345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 315\n",
            "tensor(0.5993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.9541, grad_fn=<AddBackward0>)\n",
            "tensor(43.5533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 316\n",
            "tensor(0.5990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.1901, grad_fn=<AddBackward0>)\n",
            "tensor(46.7892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 317\n",
            "tensor(0.5978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.7222, grad_fn=<AddBackward0>)\n",
            "tensor(48.3199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 318\n",
            "tensor(0.5985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.4483, grad_fn=<AddBackward0>)\n",
            "tensor(51.0469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 319\n",
            "tensor(0.5981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.4635, grad_fn=<AddBackward0>)\n",
            "tensor(48.0615, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 320\n",
            "tensor(0.5977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6696, grad_fn=<AddBackward0>)\n",
            "tensor(44.2673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 321\n",
            "tensor(0.5991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6147, grad_fn=<AddBackward0>)\n",
            "tensor(39.2138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 322\n",
            "tensor(0.6010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.6826, grad_fn=<AddBackward0>)\n",
            "tensor(38.2835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 323\n",
            "tensor(0.6033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3656, grad_fn=<AddBackward0>)\n",
            "tensor(37.9689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 324\n",
            "tensor(0.6037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2434, grad_fn=<AddBackward0>)\n",
            "tensor(44.8472, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 325\n",
            "tensor(0.6010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.7159, grad_fn=<AddBackward0>)\n",
            "tensor(49.3169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 326\n",
            "tensor(0.5986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0120, grad_fn=<AddBackward0>)\n",
            "tensor(42.6105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 327\n",
            "tensor(0.5961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.9363, grad_fn=<AddBackward0>)\n",
            "tensor(36.5324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 328\n",
            "tensor(0.5942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.7515, grad_fn=<AddBackward0>)\n",
            "tensor(49.3457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 329\n",
            "tensor(0.5927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(54.4210, grad_fn=<AddBackward0>)\n",
            "tensor(55.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 330\n",
            "tensor(0.5920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.5376, grad_fn=<AddBackward0>)\n",
            "tensor(44.1296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 331\n",
            "tensor(0.5921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5971, grad_fn=<AddBackward0>)\n",
            "tensor(43.1892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 332\n",
            "tensor(0.5938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.9663, grad_fn=<AddBackward0>)\n",
            "tensor(45.5601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 333\n",
            "tensor(0.5981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.1801, grad_fn=<AddBackward0>)\n",
            "tensor(49.7783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 334\n",
            "tensor(0.6031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.2642, grad_fn=<AddBackward0>)\n",
            "tensor(47.8673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 335\n",
            "tensor(0.6041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.8037, grad_fn=<AddBackward0>)\n",
            "tensor(45.4078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 336\n",
            "tensor(0.6043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9410, grad_fn=<AddBackward0>)\n",
            "tensor(44.5453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 337\n",
            "tensor(0.6037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1711, grad_fn=<AddBackward0>)\n",
            "tensor(41.7748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 338\n",
            "tensor(0.6015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4524, grad_fn=<AddBackward0>)\n",
            "tensor(42.0539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 339\n",
            "tensor(0.5985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5655, grad_fn=<AddBackward0>)\n",
            "tensor(37.1640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 340\n",
            "tensor(0.5972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.8358, grad_fn=<AddBackward0>)\n",
            "tensor(41.4330, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 341\n",
            "tensor(0.5928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3803, grad_fn=<AddBackward0>)\n",
            "tensor(39.9731, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 342\n",
            "tensor(0.5902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.1009, grad_fn=<AddBackward0>)\n",
            "tensor(40.6911, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 343\n",
            "tensor(0.5891, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.9876, grad_fn=<AddBackward0>)\n",
            "tensor(51.5767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 344\n",
            "tensor(0.5899, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.7009, grad_fn=<AddBackward0>)\n",
            "tensor(43.2908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 345\n",
            "tensor(0.5921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2724, grad_fn=<AddBackward0>)\n",
            "tensor(41.8645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 346\n",
            "tensor(0.5949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.0773, grad_fn=<AddBackward0>)\n",
            "tensor(45.6722, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 347\n",
            "tensor(0.5989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.0482, grad_fn=<AddBackward0>)\n",
            "tensor(43.6471, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 348\n",
            "tensor(0.5995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.6885, grad_fn=<AddBackward0>)\n",
            "tensor(40.2880, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 349\n",
            "tensor(0.5990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4013, grad_fn=<AddBackward0>)\n",
            "tensor(42.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 350\n",
            "tensor(0.5981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3376, grad_fn=<AddBackward0>)\n",
            "tensor(39.9357, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 351\n",
            "tensor(0.5965, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.6526, grad_fn=<AddBackward0>)\n",
            "tensor(47.2490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 352\n",
            "tensor(0.5962, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.7145, grad_fn=<AddBackward0>)\n",
            "tensor(43.3107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 353\n",
            "tensor(0.5964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.3337, grad_fn=<AddBackward0>)\n",
            "tensor(46.9300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 354\n",
            "tensor(0.5952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8992, grad_fn=<AddBackward0>)\n",
            "tensor(40.4944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 355\n",
            "tensor(0.5919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.2628, grad_fn=<AddBackward0>)\n",
            "tensor(38.8547, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 356\n",
            "tensor(0.5884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.9044, grad_fn=<AddBackward0>)\n",
            "tensor(36.4928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 357\n",
            "tensor(0.5859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3061, grad_fn=<AddBackward0>)\n",
            "tensor(39.8921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 358\n",
            "tensor(0.5859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7363, grad_fn=<AddBackward0>)\n",
            "tensor(37.3222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 359\n",
            "tensor(0.5883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5920, grad_fn=<AddBackward0>)\n",
            "tensor(43.1803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 360\n",
            "tensor(0.5922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3107, grad_fn=<AddBackward0>)\n",
            "tensor(43.9029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 361\n",
            "tensor(0.5954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.2211, grad_fn=<AddBackward0>)\n",
            "tensor(43.8165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 362\n",
            "tensor(0.5975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5635, grad_fn=<AddBackward0>)\n",
            "tensor(43.1610, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 363\n",
            "tensor(0.6005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(52.7134, grad_fn=<AddBackward0>)\n",
            "tensor(53.3138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 364\n",
            "tensor(0.5989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.3029, grad_fn=<AddBackward0>)\n",
            "tensor(45.9018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 365\n",
            "tensor(0.5939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.9014, grad_fn=<AddBackward0>)\n",
            "tensor(52.4953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 366\n",
            "tensor(0.5872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2148, grad_fn=<AddBackward0>)\n",
            "tensor(44.8020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 367\n",
            "tensor(0.5832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.2576, grad_fn=<AddBackward0>)\n",
            "tensor(33.8407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 368\n",
            "tensor(0.5835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.3925, grad_fn=<AddBackward0>)\n",
            "tensor(49.9760, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 369\n",
            "tensor(0.5849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4674, grad_fn=<AddBackward0>)\n",
            "tensor(42.0523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 370\n",
            "tensor(0.5863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.7378, grad_fn=<AddBackward0>)\n",
            "tensor(42.3241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 371\n",
            "tensor(0.5853, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.0746, grad_fn=<AddBackward0>)\n",
            "tensor(44.6599, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 372\n",
            "tensor(0.5834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1135, grad_fn=<AddBackward0>)\n",
            "tensor(45.6969, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 373\n",
            "tensor(0.5832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.2837, grad_fn=<AddBackward0>)\n",
            "tensor(43.8669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 374\n",
            "tensor(0.5864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1109, grad_fn=<AddBackward0>)\n",
            "tensor(41.6973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 375\n",
            "tensor(0.5900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.6212, grad_fn=<AddBackward0>)\n",
            "tensor(47.2112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 376\n",
            "tensor(0.5928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.0790, grad_fn=<AddBackward0>)\n",
            "tensor(43.6718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 377\n",
            "tensor(0.5934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.3586, grad_fn=<AddBackward0>)\n",
            "tensor(45.9520, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 378\n",
            "tensor(0.5918, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.1798, grad_fn=<AddBackward0>)\n",
            "tensor(40.7716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 379\n",
            "tensor(0.5891, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.2742, grad_fn=<AddBackward0>)\n",
            "tensor(48.8633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 380\n",
            "tensor(0.5854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5144, grad_fn=<AddBackward0>)\n",
            "tensor(43.0998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 381\n",
            "tensor(0.5818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6736, grad_fn=<AddBackward0>)\n",
            "tensor(41.2554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 382\n",
            "tensor(0.5770, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6191, grad_fn=<AddBackward0>)\n",
            "tensor(48.1961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 383\n",
            "tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.3710, grad_fn=<AddBackward0>)\n",
            "tensor(45.9466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 384\n",
            "tensor(0.5754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.7294, grad_fn=<AddBackward0>)\n",
            "tensor(46.3047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 385\n",
            "tensor(0.5767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1819, grad_fn=<AddBackward0>)\n",
            "tensor(42.7586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 386\n",
            "tensor(0.5817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.2167, grad_fn=<AddBackward0>)\n",
            "tensor(49.7984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 387\n",
            "tensor(0.5831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.1347, grad_fn=<AddBackward0>)\n",
            "tensor(39.7179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 388\n",
            "tensor(0.5826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.3071, grad_fn=<AddBackward0>)\n",
            "tensor(41.8897, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 389\n",
            "tensor(0.5832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.8127, grad_fn=<AddBackward0>)\n",
            "tensor(45.3959, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 390\n",
            "tensor(0.5847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1739, grad_fn=<AddBackward0>)\n",
            "tensor(41.7586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 391\n",
            "tensor(0.5851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.3527, grad_fn=<AddBackward0>)\n",
            "tensor(36.9378, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 392\n",
            "tensor(0.5858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.6623, grad_fn=<AddBackward0>)\n",
            "tensor(49.2481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 393\n",
            "tensor(0.5879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9606, grad_fn=<AddBackward0>)\n",
            "tensor(44.5485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 394\n",
            "tensor(0.5907, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.5893, grad_fn=<AddBackward0>)\n",
            "tensor(41.1800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 395\n",
            "tensor(0.5946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.3721, grad_fn=<AddBackward0>)\n",
            "tensor(41.9667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 396\n",
            "tensor(0.5956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.8582, grad_fn=<AddBackward0>)\n",
            "tensor(32.4538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 397\n",
            "tensor(0.5968, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.2462, grad_fn=<AddBackward0>)\n",
            "tensor(36.8430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 398\n",
            "tensor(0.5927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6343, grad_fn=<AddBackward0>)\n",
            "tensor(44.2270, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 399\n",
            "tensor(0.5848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.5473, grad_fn=<AddBackward0>)\n",
            "tensor(41.1321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 400\n",
            "tensor(0.5786, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2524, grad_fn=<AddBackward0>)\n",
            "tensor(39.8310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 401\n",
            "tensor(0.5771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9389, grad_fn=<AddBackward0>)\n",
            "tensor(41.5160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 402\n",
            "tensor(0.5785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6306, grad_fn=<AddBackward0>)\n",
            "tensor(45.2090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 403\n",
            "tensor(0.5807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2315, grad_fn=<AddBackward0>)\n",
            "tensor(39.8121, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 404\n",
            "tensor(0.5780, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.4338, grad_fn=<AddBackward0>)\n",
            "tensor(50.0118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 405\n",
            "tensor(0.5763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.1858, grad_fn=<AddBackward0>)\n",
            "tensor(43.7622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 406\n",
            "tensor(0.5748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7623, grad_fn=<AddBackward0>)\n",
            "tensor(39.3372, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 407\n",
            "tensor(0.5739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.3097, grad_fn=<AddBackward0>)\n",
            "tensor(44.8836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 408\n",
            "tensor(0.5711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.4896, grad_fn=<AddBackward0>)\n",
            "tensor(41.0607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 409\n",
            "tensor(0.5677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.1002, grad_fn=<AddBackward0>)\n",
            "tensor(40.6678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 410\n",
            "tensor(0.5655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.9803, grad_fn=<AddBackward0>)\n",
            "tensor(47.5458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 411\n",
            "tensor(0.5638, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4892, grad_fn=<AddBackward0>)\n",
            "tensor(39.0530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 412\n",
            "tensor(0.5621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4875, grad_fn=<AddBackward0>)\n",
            "tensor(45.0495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 413\n",
            "tensor(0.5606, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.6405, grad_fn=<AddBackward0>)\n",
            "tensor(43.2011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 414\n",
            "tensor(0.5599, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.1483, grad_fn=<AddBackward0>)\n",
            "tensor(44.7081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 415\n",
            "tensor(0.5593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.4339, grad_fn=<AddBackward0>)\n",
            "tensor(37.9932, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 416\n",
            "tensor(0.5601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.5983, grad_fn=<AddBackward0>)\n",
            "tensor(47.1584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 417\n",
            "tensor(0.5598, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6420, grad_fn=<AddBackward0>)\n",
            "tensor(45.2018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 418\n",
            "tensor(0.5609, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.4490, grad_fn=<AddBackward0>)\n",
            "tensor(33.0099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 419\n",
            "tensor(0.5609, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.7292, grad_fn=<AddBackward0>)\n",
            "tensor(43.2901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 420\n",
            "tensor(0.5617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2339, grad_fn=<AddBackward0>)\n",
            "tensor(39.7956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 421\n",
            "tensor(0.5648, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9724, grad_fn=<AddBackward0>)\n",
            "tensor(41.5372, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 422\n",
            "tensor(0.5687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.8398, grad_fn=<AddBackward0>)\n",
            "tensor(39.4086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 423\n",
            "tensor(0.5701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8654, grad_fn=<AddBackward0>)\n",
            "tensor(44.4355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 424\n",
            "tensor(0.5659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.9634, grad_fn=<AddBackward0>)\n",
            "tensor(42.5293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 425\n",
            "tensor(0.5619, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.3981, grad_fn=<AddBackward0>)\n",
            "tensor(38.9600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 426\n",
            "tensor(0.5605, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.5525, grad_fn=<AddBackward0>)\n",
            "tensor(47.1130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 427\n",
            "tensor(0.5629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8369, grad_fn=<AddBackward0>)\n",
            "tensor(42.3998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 428\n",
            "tensor(0.5662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.0984, grad_fn=<AddBackward0>)\n",
            "tensor(47.6646, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 429\n",
            "tensor(0.5685, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6997, grad_fn=<AddBackward0>)\n",
            "tensor(41.2682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 430\n",
            "tensor(0.5709, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.6060, grad_fn=<AddBackward0>)\n",
            "tensor(42.1769, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 431\n",
            "tensor(0.5716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4206, grad_fn=<AddBackward0>)\n",
            "tensor(35.9922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 432\n",
            "tensor(0.5728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.0979, grad_fn=<AddBackward0>)\n",
            "tensor(40.6707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 433\n",
            "tensor(0.5737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0088, grad_fn=<AddBackward0>)\n",
            "tensor(42.5825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 434\n",
            "tensor(0.5736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.5836, grad_fn=<AddBackward0>)\n",
            "tensor(39.1572, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 435\n",
            "tensor(0.5730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.7402, grad_fn=<AddBackward0>)\n",
            "tensor(36.3131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 436\n",
            "tensor(0.5712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.0010, grad_fn=<AddBackward0>)\n",
            "tensor(48.5722, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 437\n",
            "tensor(0.5688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.0623, grad_fn=<AddBackward0>)\n",
            "tensor(45.6311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 438\n",
            "tensor(0.5674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.4231, grad_fn=<AddBackward0>)\n",
            "tensor(40.9905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 439\n",
            "tensor(0.5662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.5100, grad_fn=<AddBackward0>)\n",
            "tensor(33.0762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 440\n",
            "tensor(0.5665, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0204, grad_fn=<AddBackward0>)\n",
            "tensor(35.5869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 441\n",
            "tensor(0.5678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1049, grad_fn=<AddBackward0>)\n",
            "tensor(45.6727, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 442\n",
            "tensor(0.5699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.7356, grad_fn=<AddBackward0>)\n",
            "tensor(46.3055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 443\n",
            "tensor(0.5714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8992, grad_fn=<AddBackward0>)\n",
            "tensor(43.4706, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 444\n",
            "tensor(0.5746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.2661, grad_fn=<AddBackward0>)\n",
            "tensor(50.8407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 445\n",
            "tensor(0.5827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.5382, grad_fn=<AddBackward0>)\n",
            "tensor(47.1209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 446\n",
            "tensor(0.5946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.4894, grad_fn=<AddBackward0>)\n",
            "tensor(51.0841, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 447\n",
            "tensor(0.5936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(51.1288, grad_fn=<AddBackward0>)\n",
            "tensor(51.7224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 448\n",
            "tensor(0.5845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0759, grad_fn=<AddBackward0>)\n",
            "tensor(41.6604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 449\n",
            "tensor(0.5746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.7896, grad_fn=<AddBackward0>)\n",
            "tensor(42.3642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 450\n",
            "tensor(0.5691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1640, grad_fn=<AddBackward0>)\n",
            "tensor(42.7331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 451\n",
            "tensor(0.5669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4646, grad_fn=<AddBackward0>)\n",
            "tensor(40.0315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 452\n",
            "tensor(0.5683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.5350, grad_fn=<AddBackward0>)\n",
            "tensor(45.1033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 453\n",
            "tensor(0.5714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.8861, grad_fn=<AddBackward0>)\n",
            "tensor(34.4575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 454\n",
            "tensor(0.5750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.2532, grad_fn=<AddBackward0>)\n",
            "tensor(42.8282, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 455\n",
            "tensor(0.5777, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7866, grad_fn=<AddBackward0>)\n",
            "tensor(37.3642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 456\n",
            "tensor(0.5789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4946, grad_fn=<AddBackward0>)\n",
            "tensor(42.0735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 457\n",
            "tensor(0.5785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.4461, grad_fn=<AddBackward0>)\n",
            "tensor(32.0246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 458\n",
            "tensor(0.5810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8165, grad_fn=<AddBackward0>)\n",
            "tensor(40.3974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 459\n",
            "tensor(0.5824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.0139, grad_fn=<AddBackward0>)\n",
            "tensor(39.5963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 460\n",
            "tensor(0.5817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.9095, grad_fn=<AddBackward0>)\n",
            "tensor(46.4912, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 461\n",
            "tensor(0.5776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.5876, grad_fn=<AddBackward0>)\n",
            "tensor(44.1652, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 462\n",
            "tensor(0.5737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.5969, grad_fn=<AddBackward0>)\n",
            "tensor(38.1705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 463\n",
            "tensor(0.5680, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8113, grad_fn=<AddBackward0>)\n",
            "tensor(44.3792, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 464\n",
            "tensor(0.5635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.2494, grad_fn=<AddBackward0>)\n",
            "tensor(33.8129, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 465\n",
            "tensor(0.5584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(30.6646, grad_fn=<AddBackward0>)\n",
            "tensor(31.2231, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 466\n",
            "tensor(0.5555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4120, grad_fn=<AddBackward0>)\n",
            "tensor(41.9675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 467\n",
            "tensor(0.5534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.6467, grad_fn=<AddBackward0>)\n",
            "tensor(38.2001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 468\n",
            "tensor(0.5538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.9353, grad_fn=<AddBackward0>)\n",
            "tensor(33.4891, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 469\n",
            "tensor(0.5551, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.9718, grad_fn=<AddBackward0>)\n",
            "tensor(43.5269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 470\n",
            "tensor(0.5575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.3428, grad_fn=<AddBackward0>)\n",
            "tensor(33.9003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 471\n",
            "tensor(0.5582, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4589, grad_fn=<AddBackward0>)\n",
            "tensor(36.0171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 472\n",
            "tensor(0.5563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8113, grad_fn=<AddBackward0>)\n",
            "tensor(40.3676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 473\n",
            "tensor(0.5539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.7596, grad_fn=<AddBackward0>)\n",
            "tensor(44.3135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 474\n",
            "tensor(0.5506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.9109, grad_fn=<AddBackward0>)\n",
            "tensor(43.4615, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 475\n",
            "tensor(0.5500, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.5946, grad_fn=<AddBackward0>)\n",
            "tensor(50.1446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 476\n",
            "tensor(0.5497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0186, grad_fn=<AddBackward0>)\n",
            "tensor(42.5682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 477\n",
            "tensor(0.5480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.7284, grad_fn=<AddBackward0>)\n",
            "tensor(42.2764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 478\n",
            "tensor(0.5448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.3587, grad_fn=<AddBackward0>)\n",
            "tensor(38.9034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 479\n",
            "tensor(0.5418, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.9882, grad_fn=<AddBackward0>)\n",
            "tensor(38.5300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 480\n",
            "tensor(0.5400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6296, grad_fn=<AddBackward0>)\n",
            "tensor(41.1696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 481\n",
            "tensor(0.5396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1199, grad_fn=<AddBackward0>)\n",
            "tensor(42.6595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 482\n",
            "tensor(0.5401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3231, grad_fn=<AddBackward0>)\n",
            "tensor(37.8632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 483\n",
            "tensor(0.5406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.9745, grad_fn=<AddBackward0>)\n",
            "tensor(43.5152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 484\n",
            "tensor(0.5404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6101, grad_fn=<AddBackward0>)\n",
            "tensor(44.1505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 485\n",
            "tensor(0.5403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1025, grad_fn=<AddBackward0>)\n",
            "tensor(42.6428, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 486\n",
            "tensor(0.5396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.5189, grad_fn=<AddBackward0>)\n",
            "tensor(40.0584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 487\n",
            "tensor(0.5386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.4853, grad_fn=<AddBackward0>)\n",
            "tensor(44.0239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 488\n",
            "tensor(0.5382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9013, grad_fn=<AddBackward0>)\n",
            "tensor(40.4394, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 489\n",
            "tensor(0.5394, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3075, grad_fn=<AddBackward0>)\n",
            "tensor(37.8469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 490\n",
            "tensor(0.5407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(30.4747, grad_fn=<AddBackward0>)\n",
            "tensor(31.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 491\n",
            "tensor(0.5425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.8849, grad_fn=<AddBackward0>)\n",
            "tensor(45.4275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 492\n",
            "tensor(0.5456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.2077, grad_fn=<AddBackward0>)\n",
            "tensor(42.7533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 493\n",
            "tensor(0.5489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.3814, grad_fn=<AddBackward0>)\n",
            "tensor(46.9303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 494\n",
            "tensor(0.5566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.5556, grad_fn=<AddBackward0>)\n",
            "tensor(46.1122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 495\n",
            "tensor(0.5654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2439, grad_fn=<AddBackward0>)\n",
            "tensor(41.8093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 496\n",
            "tensor(0.5705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.5951, grad_fn=<AddBackward0>)\n",
            "tensor(50.1656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 497\n",
            "tensor(0.5694, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2113, grad_fn=<AddBackward0>)\n",
            "tensor(39.7807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 498\n",
            "tensor(0.5658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.0366, grad_fn=<AddBackward0>)\n",
            "tensor(45.6023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 499\n",
            "tensor(0.5597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9920, grad_fn=<AddBackward0>)\n",
            "tensor(40.5517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 500\n",
            "tensor(0.5578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6524, grad_fn=<AddBackward0>)\n",
            "tensor(44.2101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 501\n",
            "tensor(0.5586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.7217, grad_fn=<AddBackward0>)\n",
            "tensor(35.2803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 502\n",
            "tensor(0.5596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.4678, grad_fn=<AddBackward0>)\n",
            "tensor(43.0274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 503\n",
            "tensor(0.5601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2482, grad_fn=<AddBackward0>)\n",
            "tensor(41.8083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 504\n",
            "tensor(0.5596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.4608, grad_fn=<AddBackward0>)\n",
            "tensor(47.0204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 505\n",
            "tensor(0.5565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.1100, grad_fn=<AddBackward0>)\n",
            "tensor(33.6665, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 506\n",
            "tensor(0.5558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.0671, grad_fn=<AddBackward0>)\n",
            "tensor(47.6229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 507\n",
            "tensor(0.5588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.8628, grad_fn=<AddBackward0>)\n",
            "tensor(36.4216, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 508\n",
            "tensor(0.5636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8425, grad_fn=<AddBackward0>)\n",
            "tensor(38.4061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 509\n",
            "tensor(0.5659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4680, grad_fn=<AddBackward0>)\n",
            "tensor(42.0339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 510\n",
            "tensor(0.5665, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.9271, grad_fn=<AddBackward0>)\n",
            "tensor(39.4936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 511\n",
            "tensor(0.5653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.9858, grad_fn=<AddBackward0>)\n",
            "tensor(34.5511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 512\n",
            "tensor(0.5652, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.4089, grad_fn=<AddBackward0>)\n",
            "tensor(43.9742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 513\n",
            "tensor(0.5682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.4000, grad_fn=<AddBackward0>)\n",
            "tensor(31.9683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 514\n",
            "tensor(0.5729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.4293, grad_fn=<AddBackward0>)\n",
            "tensor(44.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 515\n",
            "tensor(0.5754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.3259, grad_fn=<AddBackward0>)\n",
            "tensor(42.9013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 516\n",
            "tensor(0.5748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2080, grad_fn=<AddBackward0>)\n",
            "tensor(44.7828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 517\n",
            "tensor(0.5753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8837, grad_fn=<AddBackward0>)\n",
            "tensor(40.4590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 518\n",
            "tensor(0.5744, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7956, grad_fn=<AddBackward0>)\n",
            "tensor(39.3699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 519\n",
            "tensor(0.5714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6088, grad_fn=<AddBackward0>)\n",
            "tensor(45.1802, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 520\n",
            "tensor(0.5687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.3573, grad_fn=<AddBackward0>)\n",
            "tensor(46.9260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 521\n",
            "tensor(0.5682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.5028, grad_fn=<AddBackward0>)\n",
            "tensor(36.0710, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 522\n",
            "tensor(0.5688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.8325, grad_fn=<AddBackward0>)\n",
            "tensor(47.4014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 523\n",
            "tensor(0.5706, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9837, grad_fn=<AddBackward0>)\n",
            "tensor(35.5544, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 524\n",
            "tensor(0.5741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3643, grad_fn=<AddBackward0>)\n",
            "tensor(39.9384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 525\n",
            "tensor(0.5763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6718, grad_fn=<AddBackward0>)\n",
            "tensor(37.2480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 526\n",
            "tensor(0.5757, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.9737, grad_fn=<AddBackward0>)\n",
            "tensor(36.5494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 527\n",
            "tensor(0.5726, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5475, grad_fn=<AddBackward0>)\n",
            "tensor(42.1201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 528\n",
            "tensor(0.5712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.2606, grad_fn=<AddBackward0>)\n",
            "tensor(45.8318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 529\n",
            "tensor(0.5710, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.4719, grad_fn=<AddBackward0>)\n",
            "tensor(35.0429, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 530\n",
            "tensor(0.5717, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.3650, grad_fn=<AddBackward0>)\n",
            "tensor(34.9367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 531\n",
            "tensor(0.5741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.0589, grad_fn=<AddBackward0>)\n",
            "tensor(37.6330, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 532\n",
            "tensor(0.5750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.4289, grad_fn=<AddBackward0>)\n",
            "tensor(41.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 533\n",
            "tensor(0.5758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.7613, grad_fn=<AddBackward0>)\n",
            "tensor(42.3372, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 534\n",
            "tensor(0.5781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9246, grad_fn=<AddBackward0>)\n",
            "tensor(35.5027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 535\n",
            "tensor(0.5788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.0951, grad_fn=<AddBackward0>)\n",
            "tensor(33.6739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 536\n",
            "tensor(0.5772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6961, grad_fn=<AddBackward0>)\n",
            "tensor(45.2733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 537\n",
            "tensor(0.5750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.0003, grad_fn=<AddBackward0>)\n",
            "tensor(38.5753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 538\n",
            "tensor(0.5730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2769, grad_fn=<AddBackward0>)\n",
            "tensor(39.8499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 539\n",
            "tensor(0.5684, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.4641, grad_fn=<AddBackward0>)\n",
            "tensor(37.0325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 540\n",
            "tensor(0.5631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0873, grad_fn=<AddBackward0>)\n",
            "tensor(35.6503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 541\n",
            "tensor(0.5598, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0045, grad_fn=<AddBackward0>)\n",
            "tensor(42.5644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 542\n",
            "tensor(0.5560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.3692, grad_fn=<AddBackward0>)\n",
            "tensor(38.9252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 543\n",
            "tensor(0.5527, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.4002, grad_fn=<AddBackward0>)\n",
            "tensor(43.9528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 544\n",
            "tensor(0.5482, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.2249, grad_fn=<AddBackward0>)\n",
            "tensor(34.7732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 545\n",
            "tensor(0.5456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.8361, grad_fn=<AddBackward0>)\n",
            "tensor(37.3816, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 546\n",
            "tensor(0.5441, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.3197, grad_fn=<AddBackward0>)\n",
            "tensor(44.8639, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 547\n",
            "tensor(0.5451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0120, grad_fn=<AddBackward0>)\n",
            "tensor(35.5571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 548\n",
            "tensor(0.5460, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5377, grad_fn=<AddBackward0>)\n",
            "tensor(43.0837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 549\n",
            "tensor(0.5452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3111, grad_fn=<AddBackward0>)\n",
            "tensor(39.8563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 550\n",
            "tensor(0.5436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.0322, grad_fn=<AddBackward0>)\n",
            "tensor(47.5758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 551\n",
            "tensor(0.5425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8414, grad_fn=<AddBackward0>)\n",
            "tensor(42.3839, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 552\n",
            "tensor(0.5430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.5571, grad_fn=<AddBackward0>)\n",
            "tensor(45.1001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 553\n",
            "tensor(0.5448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0353, grad_fn=<AddBackward0>)\n",
            "tensor(41.5800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 554\n",
            "tensor(0.5450, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5949, grad_fn=<AddBackward0>)\n",
            "tensor(37.1398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 555\n",
            "tensor(0.5443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.0276, grad_fn=<AddBackward0>)\n",
            "tensor(34.5718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 556\n",
            "tensor(0.5425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.7768, grad_fn=<AddBackward0>)\n",
            "tensor(34.3192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 557\n",
            "tensor(0.5411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.3132, grad_fn=<AddBackward0>)\n",
            "tensor(35.8543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 558\n",
            "tensor(0.5402, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.2411, grad_fn=<AddBackward0>)\n",
            "tensor(37.7813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 559\n",
            "tensor(0.5393, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8074, grad_fn=<AddBackward0>)\n",
            "tensor(42.3467, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 560\n",
            "tensor(0.5396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.7507, grad_fn=<AddBackward0>)\n",
            "tensor(38.2903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 561\n",
            "tensor(0.5422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.8899, grad_fn=<AddBackward0>)\n",
            "tensor(39.4321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 562\n",
            "tensor(0.5449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3758, grad_fn=<AddBackward0>)\n",
            "tensor(37.9207, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 563\n",
            "tensor(0.5453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.6884, grad_fn=<AddBackward0>)\n",
            "tensor(36.2337, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 564\n",
            "tensor(0.5429, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.1759, grad_fn=<AddBackward0>)\n",
            "tensor(39.7188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 565\n",
            "tensor(0.5389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.7862, grad_fn=<AddBackward0>)\n",
            "tensor(44.3251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 566\n",
            "tensor(0.5353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.3202, grad_fn=<AddBackward0>)\n",
            "tensor(41.8554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 567\n",
            "tensor(0.5331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.6053, grad_fn=<AddBackward0>)\n",
            "tensor(36.1385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 568\n",
            "tensor(0.5303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4432, grad_fn=<AddBackward0>)\n",
            "tensor(35.9735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 569\n",
            "tensor(0.5299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0749, grad_fn=<AddBackward0>)\n",
            "tensor(35.6048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 570\n",
            "tensor(0.5307, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.1835, grad_fn=<AddBackward0>)\n",
            "tensor(44.7141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 571\n",
            "tensor(0.5312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.4353, grad_fn=<AddBackward0>)\n",
            "tensor(42.9666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 572\n",
            "tensor(0.5303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.0722, grad_fn=<AddBackward0>)\n",
            "tensor(36.6025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 573\n",
            "tensor(0.5290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8172, grad_fn=<AddBackward0>)\n",
            "tensor(43.3462, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 574\n",
            "tensor(0.5295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.3882, grad_fn=<AddBackward0>)\n",
            "tensor(38.9177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 575\n",
            "tensor(0.5325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7350, grad_fn=<AddBackward0>)\n",
            "tensor(37.2674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 576\n",
            "tensor(0.5379, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.3775, grad_fn=<AddBackward0>)\n",
            "tensor(35.9154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 577\n",
            "tensor(0.5422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3888, grad_fn=<AddBackward0>)\n",
            "tensor(37.9311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 578\n",
            "tensor(0.5430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8439, grad_fn=<AddBackward0>)\n",
            "tensor(43.3870, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 579\n",
            "tensor(0.5411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6940, grad_fn=<AddBackward0>)\n",
            "tensor(48.2351, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 580\n",
            "tensor(0.5377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.0068, grad_fn=<AddBackward0>)\n",
            "tensor(40.5446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 581\n",
            "tensor(0.5345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1047, grad_fn=<AddBackward0>)\n",
            "tensor(41.6392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 582\n",
            "tensor(0.5332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.9193, grad_fn=<AddBackward0>)\n",
            "tensor(42.4525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 583\n",
            "tensor(0.5324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.3143, grad_fn=<AddBackward0>)\n",
            "tensor(40.8467, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 584\n",
            "tensor(0.5311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5221, grad_fn=<AddBackward0>)\n",
            "tensor(43.0532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 585\n",
            "tensor(0.5305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5437, grad_fn=<AddBackward0>)\n",
            "tensor(37.0742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 586\n",
            "tensor(0.5308, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.6093, grad_fn=<AddBackward0>)\n",
            "tensor(51.1401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 587\n",
            "tensor(0.5285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5840, grad_fn=<AddBackward0>)\n",
            "tensor(37.1125, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 588\n",
            "tensor(0.5257, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2673, grad_fn=<AddBackward0>)\n",
            "tensor(44.7930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 589\n",
            "tensor(0.5235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1142, grad_fn=<AddBackward0>)\n",
            "tensor(36.6376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 590\n",
            "tensor(0.5224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.1838, grad_fn=<AddBackward0>)\n",
            "tensor(48.7062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 591\n",
            "tensor(0.5240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.3642, grad_fn=<AddBackward0>)\n",
            "tensor(38.8881, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 592\n",
            "tensor(0.5270, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.8723, grad_fn=<AddBackward0>)\n",
            "tensor(45.3992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 593\n",
            "tensor(0.5290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.4452, grad_fn=<AddBackward0>)\n",
            "tensor(49.9742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 594\n",
            "tensor(0.5299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5319, grad_fn=<AddBackward0>)\n",
            "tensor(42.0618, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 595\n",
            "tensor(0.5286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1733, grad_fn=<AddBackward0>)\n",
            "tensor(45.7019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 596\n",
            "tensor(0.5255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7486, grad_fn=<AddBackward0>)\n",
            "tensor(37.2741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 597\n",
            "tensor(0.5249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.1061, grad_fn=<AddBackward0>)\n",
            "tensor(43.6310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 598\n",
            "tensor(0.5249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4535, grad_fn=<AddBackward0>)\n",
            "tensor(44.9784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 599\n",
            "tensor(0.5242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9827, grad_fn=<AddBackward0>)\n",
            "tensor(44.5069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 600\n",
            "tensor(0.5232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.0236, grad_fn=<AddBackward0>)\n",
            "tensor(36.5467, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 601\n",
            "tensor(0.5192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.9603, grad_fn=<AddBackward0>)\n",
            "tensor(46.4795, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 602\n",
            "tensor(0.5140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.1453, grad_fn=<AddBackward0>)\n",
            "tensor(35.6593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 603\n",
            "tensor(0.5119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9479, grad_fn=<AddBackward0>)\n",
            "tensor(40.4599, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 604\n",
            "tensor(0.5115, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.7782, grad_fn=<AddBackward0>)\n",
            "tensor(45.2897, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 605\n",
            "tensor(0.5109, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4451, grad_fn=<AddBackward0>)\n",
            "tensor(38.9560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 606\n",
            "tensor(0.5132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.8713, grad_fn=<AddBackward0>)\n",
            "tensor(36.3846, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 607\n",
            "tensor(0.5168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3217, grad_fn=<AddBackward0>)\n",
            "tensor(43.8385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 608\n",
            "tensor(0.5199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8428, grad_fn=<AddBackward0>)\n",
            "tensor(43.3627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 609\n",
            "tensor(0.5199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(30.4689, grad_fn=<AddBackward0>)\n",
            "tensor(30.9887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 610\n",
            "tensor(0.5198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9494, grad_fn=<AddBackward0>)\n",
            "tensor(41.4692, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 611\n",
            "tensor(0.5191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.7414, grad_fn=<AddBackward0>)\n",
            "tensor(40.2605, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 612\n",
            "tensor(0.5186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.4060, grad_fn=<AddBackward0>)\n",
            "tensor(37.9245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 613\n",
            "tensor(0.5179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6463, grad_fn=<AddBackward0>)\n",
            "tensor(41.1641, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 614\n",
            "tensor(0.5189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.6179, grad_fn=<AddBackward0>)\n",
            "tensor(38.1368, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 615\n",
            "tensor(0.5175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.2222, grad_fn=<AddBackward0>)\n",
            "tensor(34.7397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 616\n",
            "tensor(0.5153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.9571, grad_fn=<AddBackward0>)\n",
            "tensor(37.4723, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 617\n",
            "tensor(0.5157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.0706, grad_fn=<AddBackward0>)\n",
            "tensor(43.5863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 618\n",
            "tensor(0.5160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.7318, grad_fn=<AddBackward0>)\n",
            "tensor(42.2477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 619\n",
            "tensor(0.5172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.7917, grad_fn=<AddBackward0>)\n",
            "tensor(35.3090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 620\n",
            "tensor(0.5199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.5577, grad_fn=<AddBackward0>)\n",
            "tensor(39.0776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 621\n",
            "tensor(0.5219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.5532, grad_fn=<AddBackward0>)\n",
            "tensor(48.0751, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 622\n",
            "tensor(0.5203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8838, grad_fn=<AddBackward0>)\n",
            "tensor(40.4040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 623\n",
            "tensor(0.5166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4884, grad_fn=<AddBackward0>)\n",
            "tensor(40.0050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 624\n",
            "tensor(0.5137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6604, grad_fn=<AddBackward0>)\n",
            "tensor(39.1741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 625\n",
            "tensor(0.5127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.1561, grad_fn=<AddBackward0>)\n",
            "tensor(32.6688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 626\n",
            "tensor(0.5138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8190, grad_fn=<AddBackward0>)\n",
            "tensor(38.3328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 627\n",
            "tensor(0.5161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8406, grad_fn=<AddBackward0>)\n",
            "tensor(40.3566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 628\n",
            "tensor(0.5157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.6565, grad_fn=<AddBackward0>)\n",
            "tensor(36.1722, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 629\n",
            "tensor(0.5143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.6756, grad_fn=<AddBackward0>)\n",
            "tensor(40.1899, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 630\n",
            "tensor(0.5118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.8645, grad_fn=<AddBackward0>)\n",
            "tensor(37.3763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 631\n",
            "tensor(0.5111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(50.7437, grad_fn=<AddBackward0>)\n",
            "tensor(51.2548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 632\n",
            "tensor(0.5103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2419, grad_fn=<AddBackward0>)\n",
            "tensor(41.7522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 633\n",
            "tensor(0.5113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7405, grad_fn=<AddBackward0>)\n",
            "tensor(39.2517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 634\n",
            "tensor(0.5115, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8027, grad_fn=<AddBackward0>)\n",
            "tensor(44.3142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 635\n",
            "tensor(0.5104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2785, grad_fn=<AddBackward0>)\n",
            "tensor(39.7889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 636\n",
            "tensor(0.5092, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.6590, grad_fn=<AddBackward0>)\n",
            "tensor(35.1682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 637\n",
            "tensor(0.5084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.5117, grad_fn=<AddBackward0>)\n",
            "tensor(41.0201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 638\n",
            "tensor(0.5070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.8267, grad_fn=<AddBackward0>)\n",
            "tensor(42.3337, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 639\n",
            "tensor(0.5068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.8804, grad_fn=<AddBackward0>)\n",
            "tensor(36.3872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 640\n",
            "tensor(0.5067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.9757, grad_fn=<AddBackward0>)\n",
            "tensor(38.4823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 641\n",
            "tensor(0.5066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6488, grad_fn=<AddBackward0>)\n",
            "tensor(37.1554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 642\n",
            "tensor(0.5046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.8855, grad_fn=<AddBackward0>)\n",
            "tensor(36.3901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 643\n",
            "tensor(0.5026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.4373, grad_fn=<AddBackward0>)\n",
            "tensor(42.9399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 644\n",
            "tensor(0.5011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.9473, grad_fn=<AddBackward0>)\n",
            "tensor(46.4485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 645\n",
            "tensor(0.5016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6002, grad_fn=<AddBackward0>)\n",
            "tensor(45.1018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 646\n",
            "tensor(0.5051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5582, grad_fn=<AddBackward0>)\n",
            "tensor(42.0633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 647\n",
            "tensor(0.5084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.0122, grad_fn=<AddBackward0>)\n",
            "tensor(36.5206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 648\n",
            "tensor(0.5090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.4653, grad_fn=<AddBackward0>)\n",
            "tensor(36.9743, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 649\n",
            "tensor(0.5079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.9549, grad_fn=<AddBackward0>)\n",
            "tensor(42.4628, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 650\n",
            "tensor(0.5061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5627, grad_fn=<AddBackward0>)\n",
            "tensor(43.0689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 651\n",
            "tensor(0.5059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.8234, grad_fn=<AddBackward0>)\n",
            "tensor(39.3293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 652\n",
            "tensor(0.5071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.6157, grad_fn=<AddBackward0>)\n",
            "tensor(42.1228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 653\n",
            "tensor(0.5102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3368, grad_fn=<AddBackward0>)\n",
            "tensor(37.8469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 654\n",
            "tensor(0.5165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0351, grad_fn=<AddBackward0>)\n",
            "tensor(41.5517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 655\n",
            "tensor(0.5202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.8134, grad_fn=<AddBackward0>)\n",
            "tensor(41.3336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 656\n",
            "tensor(0.5213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.5979, grad_fn=<AddBackward0>)\n",
            "tensor(45.1192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 657\n",
            "tensor(0.5203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9793, grad_fn=<AddBackward0>)\n",
            "tensor(41.4995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 658\n",
            "tensor(0.5120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.0003, grad_fn=<AddBackward0>)\n",
            "tensor(40.5123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 659\n",
            "tensor(0.5074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.7839, grad_fn=<AddBackward0>)\n",
            "tensor(48.2913, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 660\n",
            "tensor(0.5066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5740, grad_fn=<AddBackward0>)\n",
            "tensor(43.0806, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 661\n",
            "tensor(0.5074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.0769, grad_fn=<AddBackward0>)\n",
            "tensor(38.5843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 662\n",
            "tensor(0.5102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6368, grad_fn=<AddBackward0>)\n",
            "tensor(39.1470, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 663\n",
            "tensor(0.5145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4462, grad_fn=<AddBackward0>)\n",
            "tensor(41.9606, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 664\n",
            "tensor(0.5177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.7616, grad_fn=<AddBackward0>)\n",
            "tensor(41.2794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 665\n",
            "tensor(0.5168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.8164, grad_fn=<AddBackward0>)\n",
            "tensor(41.3333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 666\n",
            "tensor(0.5143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8467, grad_fn=<AddBackward0>)\n",
            "tensor(44.3609, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 667\n",
            "tensor(0.5108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0212, grad_fn=<AddBackward0>)\n",
            "tensor(42.5320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 668\n",
            "tensor(0.5088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3460, grad_fn=<AddBackward0>)\n",
            "tensor(37.8548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 669\n",
            "tensor(0.5077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2884, grad_fn=<AddBackward0>)\n",
            "tensor(41.7960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 670\n",
            "tensor(0.5068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.3031, grad_fn=<AddBackward0>)\n",
            "tensor(46.8098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 671\n",
            "tensor(0.5074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.5500, grad_fn=<AddBackward0>)\n",
            "tensor(38.0573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 672\n",
            "tensor(0.5073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.5487, grad_fn=<AddBackward0>)\n",
            "tensor(34.0560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 673\n",
            "tensor(0.5061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.7701, grad_fn=<AddBackward0>)\n",
            "tensor(45.2762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 674\n",
            "tensor(0.5035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.1004, grad_fn=<AddBackward0>)\n",
            "tensor(39.6039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 675\n",
            "tensor(0.5016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.2179, grad_fn=<AddBackward0>)\n",
            "tensor(44.7195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 676\n",
            "tensor(0.5007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3531, grad_fn=<AddBackward0>)\n",
            "tensor(39.8538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 677\n",
            "tensor(0.5002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3354, grad_fn=<AddBackward0>)\n",
            "tensor(37.8356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 678\n",
            "tensor(0.4995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.5324, grad_fn=<AddBackward0>)\n",
            "tensor(40.0319, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 679\n",
            "tensor(0.4991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(29.8193, grad_fn=<AddBackward0>)\n",
            "tensor(30.3184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 680\n",
            "tensor(0.4996, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.6166, grad_fn=<AddBackward0>)\n",
            "tensor(45.1161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 681\n",
            "tensor(0.4991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.5622, grad_fn=<AddBackward0>)\n",
            "tensor(34.0613, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 682\n",
            "tensor(0.4971, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3736, grad_fn=<AddBackward0>)\n",
            "tensor(39.8707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 683\n",
            "tensor(0.4957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6477, grad_fn=<AddBackward0>)\n",
            "tensor(37.1434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 684\n",
            "tensor(0.4962, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.5806, grad_fn=<AddBackward0>)\n",
            "tensor(34.0767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 685\n",
            "tensor(0.4992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.1716, grad_fn=<AddBackward0>)\n",
            "tensor(37.6707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 686\n",
            "tensor(0.5015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1568, grad_fn=<AddBackward0>)\n",
            "tensor(41.6583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 687\n",
            "tensor(0.5008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.1362, grad_fn=<AddBackward0>)\n",
            "tensor(31.6370, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 688\n",
            "tensor(0.5003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.6585, grad_fn=<AddBackward0>)\n",
            "tensor(33.1587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 689\n",
            "tensor(0.4998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.0703, grad_fn=<AddBackward0>)\n",
            "tensor(37.5701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 690\n",
            "tensor(0.5008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1606, grad_fn=<AddBackward0>)\n",
            "tensor(42.6614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 691\n",
            "tensor(0.5037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3943, grad_fn=<AddBackward0>)\n",
            "tensor(37.8980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 692\n",
            "tensor(0.5082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.9331, grad_fn=<AddBackward0>)\n",
            "tensor(39.4413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 693\n",
            "tensor(0.5125, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9760, grad_fn=<AddBackward0>)\n",
            "tensor(41.4886, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 694\n",
            "tensor(0.5151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1123, grad_fn=<AddBackward0>)\n",
            "tensor(38.6274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 695\n",
            "tensor(0.5150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.8698, grad_fn=<AddBackward0>)\n",
            "tensor(35.3847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 696\n",
            "tensor(0.5111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4170, grad_fn=<AddBackward0>)\n",
            "tensor(41.9281, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 697\n",
            "tensor(0.5043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.1198, grad_fn=<AddBackward0>)\n",
            "tensor(44.6241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 698\n",
            "tensor(0.4972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0164, grad_fn=<AddBackward0>)\n",
            "tensor(41.5136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 699\n",
            "tensor(0.4920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.5901, grad_fn=<AddBackward0>)\n",
            "tensor(40.0821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 700\n",
            "tensor(0.4908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.7742, grad_fn=<AddBackward0>)\n",
            "tensor(46.2649, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 701\n",
            "tensor(0.4917, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6048, grad_fn=<AddBackward0>)\n",
            "tensor(48.0966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 702\n",
            "tensor(0.4935, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.3287, grad_fn=<AddBackward0>)\n",
            "tensor(34.8222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 703\n",
            "tensor(0.4977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.7973, grad_fn=<AddBackward0>)\n",
            "tensor(35.2950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 704\n",
            "tensor(0.5026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5452, grad_fn=<AddBackward0>)\n",
            "tensor(37.0477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 705\n",
            "tensor(0.4994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.9048, grad_fn=<AddBackward0>)\n",
            "tensor(37.4042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 706\n",
            "tensor(0.4942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.3255, grad_fn=<AddBackward0>)\n",
            "tensor(32.8197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 707\n",
            "tensor(0.4936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.0619, grad_fn=<AddBackward0>)\n",
            "tensor(39.5555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 708\n",
            "tensor(0.4967, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5766, grad_fn=<AddBackward0>)\n",
            "tensor(43.0733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 709\n",
            "tensor(0.4971, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8354, grad_fn=<AddBackward0>)\n",
            "tensor(38.3324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 710\n",
            "tensor(0.4958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7638, grad_fn=<AddBackward0>)\n",
            "tensor(39.2595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 711\n",
            "tensor(0.4928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.1502, grad_fn=<AddBackward0>)\n",
            "tensor(32.6430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 712\n",
            "tensor(0.4892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.4514, grad_fn=<AddBackward0>)\n",
            "tensor(41.9407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 713\n",
            "tensor(0.4864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.2428, grad_fn=<AddBackward0>)\n",
            "tensor(43.7293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 714\n",
            "tensor(0.4847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.5735, grad_fn=<AddBackward0>)\n",
            "tensor(44.0582, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 715\n",
            "tensor(0.4833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1175, grad_fn=<AddBackward0>)\n",
            "tensor(38.6009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 716\n",
            "tensor(0.4827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3398, grad_fn=<AddBackward0>)\n",
            "tensor(39.8225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 717\n",
            "tensor(0.4825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.0163, grad_fn=<AddBackward0>)\n",
            "tensor(43.4988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 718\n",
            "tensor(0.4815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.4862, grad_fn=<AddBackward0>)\n",
            "tensor(40.9678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 719\n",
            "tensor(0.4798, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.6128, grad_fn=<AddBackward0>)\n",
            "tensor(36.0926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 720\n",
            "tensor(0.4787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8638, grad_fn=<AddBackward0>)\n",
            "tensor(43.3425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 721\n",
            "tensor(0.4789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.0131, grad_fn=<AddBackward0>)\n",
            "tensor(44.4921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 722\n",
            "tensor(0.4807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.9435, grad_fn=<AddBackward0>)\n",
            "tensor(32.4241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 723\n",
            "tensor(0.4824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.3095, grad_fn=<AddBackward0>)\n",
            "tensor(40.7918, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 724\n",
            "tensor(0.4827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.5301, grad_fn=<AddBackward0>)\n",
            "tensor(38.0128, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 725\n",
            "tensor(0.4821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.4578, grad_fn=<AddBackward0>)\n",
            "tensor(32.9400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 726\n",
            "tensor(0.4831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1828, grad_fn=<AddBackward0>)\n",
            "tensor(41.6659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 727\n",
            "tensor(0.4851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1388, grad_fn=<AddBackward0>)\n",
            "tensor(36.6239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 728\n",
            "tensor(0.4869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5519, grad_fn=<AddBackward0>)\n",
            "tensor(42.0388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 729\n",
            "tensor(0.4857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.6197, grad_fn=<AddBackward0>)\n",
            "tensor(43.1055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 730\n",
            "tensor(0.4834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8526, grad_fn=<AddBackward0>)\n",
            "tensor(38.3360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 731\n",
            "tensor(0.4820, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.3649, grad_fn=<AddBackward0>)\n",
            "tensor(42.8469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 732\n",
            "tensor(0.4823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1074, grad_fn=<AddBackward0>)\n",
            "tensor(41.5898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 733\n",
            "tensor(0.4819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.7797, grad_fn=<AddBackward0>)\n",
            "tensor(40.2616, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 734\n",
            "tensor(0.4807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3678, grad_fn=<AddBackward0>)\n",
            "tensor(37.8486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 735\n",
            "tensor(0.4785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.5862, grad_fn=<AddBackward0>)\n",
            "tensor(44.0647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 736\n",
            "tensor(0.4762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.4639, grad_fn=<AddBackward0>)\n",
            "tensor(37.9401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 737\n",
            "tensor(0.4759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1137, grad_fn=<AddBackward0>)\n",
            "tensor(38.5896, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 738\n",
            "tensor(0.4774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0730, grad_fn=<AddBackward0>)\n",
            "tensor(41.5504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 739\n",
            "tensor(0.4783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.5614, grad_fn=<AddBackward0>)\n",
            "tensor(35.0397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 740\n",
            "tensor(0.4791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.6082, grad_fn=<AddBackward0>)\n",
            "tensor(33.0873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 741\n",
            "tensor(0.4783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1929, grad_fn=<AddBackward0>)\n",
            "tensor(41.6711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 742\n",
            "tensor(0.4778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.3711, grad_fn=<AddBackward0>)\n",
            "tensor(34.8489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 743\n",
            "tensor(0.4771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.4387, grad_fn=<AddBackward0>)\n",
            "tensor(43.9158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 744\n",
            "tensor(0.4780, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4509, grad_fn=<AddBackward0>)\n",
            "tensor(35.9289, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 745\n",
            "tensor(0.4794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0636, grad_fn=<AddBackward0>)\n",
            "tensor(42.5430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 746\n",
            "tensor(0.4804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7367, grad_fn=<AddBackward0>)\n",
            "tensor(39.2171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 747\n",
            "tensor(0.4801, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0410, grad_fn=<AddBackward0>)\n",
            "tensor(35.5210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 748\n",
            "tensor(0.4800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1706, grad_fn=<AddBackward0>)\n",
            "tensor(38.6506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 749\n",
            "tensor(0.4793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0998, grad_fn=<AddBackward0>)\n",
            "tensor(41.5791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 750\n",
            "tensor(0.4775, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6475, grad_fn=<AddBackward0>)\n",
            "tensor(39.1251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 751\n",
            "tensor(0.4770, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.9011, grad_fn=<AddBackward0>)\n",
            "tensor(38.3782, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 752\n",
            "tensor(0.4783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.1509, grad_fn=<AddBackward0>)\n",
            "tensor(40.6293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 753\n",
            "tensor(0.4781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(46.4305, grad_fn=<AddBackward0>)\n",
            "tensor(46.9086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 754\n",
            "tensor(0.4776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.2812, grad_fn=<AddBackward0>)\n",
            "tensor(40.7588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 755\n",
            "tensor(0.4767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.9440, grad_fn=<AddBackward0>)\n",
            "tensor(37.4206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 756\n",
            "tensor(0.4753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.0889, grad_fn=<AddBackward0>)\n",
            "tensor(39.5642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 757\n",
            "tensor(0.4740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9992, grad_fn=<AddBackward0>)\n",
            "tensor(35.4732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 758\n",
            "tensor(0.4735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5430, grad_fn=<AddBackward0>)\n",
            "tensor(42.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 759\n",
            "tensor(0.4733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.9636, grad_fn=<AddBackward0>)\n",
            "tensor(37.4369, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 760\n",
            "tensor(0.4732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.6633, grad_fn=<AddBackward0>)\n",
            "tensor(33.1365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 761\n",
            "tensor(0.4736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.6760, grad_fn=<AddBackward0>)\n",
            "tensor(33.1496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 762\n",
            "tensor(0.4738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.8019, grad_fn=<AddBackward0>)\n",
            "tensor(35.2757, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 763\n",
            "tensor(0.4735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.4201, grad_fn=<AddBackward0>)\n",
            "tensor(34.8937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 764\n",
            "tensor(0.4740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5622, grad_fn=<AddBackward0>)\n",
            "tensor(37.0362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 765\n",
            "tensor(0.4740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.9183, grad_fn=<AddBackward0>)\n",
            "tensor(36.3923, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 766\n",
            "tensor(0.4746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.0756, grad_fn=<AddBackward0>)\n",
            "tensor(32.5502, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 767\n",
            "tensor(0.4756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6936, grad_fn=<AddBackward0>)\n",
            "tensor(39.1693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 768\n",
            "tensor(0.4755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.7889, grad_fn=<AddBackward0>)\n",
            "tensor(38.2643, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 769\n",
            "tensor(0.4763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.9749, grad_fn=<AddBackward0>)\n",
            "tensor(32.4512, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 770\n",
            "tensor(0.4762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.3641, grad_fn=<AddBackward0>)\n",
            "tensor(31.8403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 771\n",
            "tensor(0.4774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.2618, grad_fn=<AddBackward0>)\n",
            "tensor(36.7392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 772\n",
            "tensor(0.4792, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.5218, grad_fn=<AddBackward0>)\n",
            "tensor(35.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 773\n",
            "tensor(0.4784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.2726, grad_fn=<AddBackward0>)\n",
            "tensor(41.7510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 774\n",
            "tensor(0.4779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7951, grad_fn=<AddBackward0>)\n",
            "tensor(37.2730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 775\n",
            "tensor(0.4780, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.9022, grad_fn=<AddBackward0>)\n",
            "tensor(36.3802, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 776\n",
            "tensor(0.4754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.3764, grad_fn=<AddBackward0>)\n",
            "tensor(45.8517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 777\n",
            "tensor(0.4720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4639, grad_fn=<AddBackward0>)\n",
            "tensor(38.9359, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 778\n",
            "tensor(0.4678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9569, grad_fn=<AddBackward0>)\n",
            "tensor(35.4248, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 779\n",
            "tensor(0.4633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.7981, grad_fn=<AddBackward0>)\n",
            "tensor(38.2614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 780\n",
            "tensor(0.4592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4980, grad_fn=<AddBackward0>)\n",
            "tensor(44.9571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 781\n",
            "tensor(0.4569, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.0318, grad_fn=<AddBackward0>)\n",
            "tensor(36.4887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 782\n",
            "tensor(0.4560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.9764, grad_fn=<AddBackward0>)\n",
            "tensor(43.4324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 783\n",
            "tensor(0.4574, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.7824, grad_fn=<AddBackward0>)\n",
            "tensor(36.2398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 784\n",
            "tensor(0.4605, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1443, grad_fn=<AddBackward0>)\n",
            "tensor(36.6048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 785\n",
            "tensor(0.4630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3325, grad_fn=<AddBackward0>)\n",
            "tensor(37.7955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 786\n",
            "tensor(0.4647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.6935, grad_fn=<AddBackward0>)\n",
            "tensor(36.1581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 787\n",
            "tensor(0.4669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2601, grad_fn=<AddBackward0>)\n",
            "tensor(39.7269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 788\n",
            "tensor(0.4688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6874, grad_fn=<AddBackward0>)\n",
            "tensor(37.1561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 789\n",
            "tensor(0.4686, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.7207, grad_fn=<AddBackward0>)\n",
            "tensor(45.1893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 790\n",
            "tensor(0.4674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(47.6351, grad_fn=<AddBackward0>)\n",
            "tensor(48.1025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 791\n",
            "tensor(0.4645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.8544, grad_fn=<AddBackward0>)\n",
            "tensor(44.3189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 792\n",
            "tensor(0.4613, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.6598, grad_fn=<AddBackward0>)\n",
            "tensor(33.1210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 793\n",
            "tensor(0.4608, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4497, grad_fn=<AddBackward0>)\n",
            "tensor(35.9105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 794\n",
            "tensor(0.4636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.9481, grad_fn=<AddBackward0>)\n",
            "tensor(45.4117, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 795\n",
            "tensor(0.4671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.5955, grad_fn=<AddBackward0>)\n",
            "tensor(44.0627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 796\n",
            "tensor(0.4679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2371, grad_fn=<AddBackward0>)\n",
            "tensor(39.7050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 797\n",
            "tensor(0.4688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.7650, grad_fn=<AddBackward0>)\n",
            "tensor(41.2338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 798\n",
            "tensor(0.4675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.1444, grad_fn=<AddBackward0>)\n",
            "tensor(43.6118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 799\n",
            "tensor(0.4661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.5594, grad_fn=<AddBackward0>)\n",
            "tensor(44.0255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 800\n",
            "tensor(0.4650, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0272, grad_fn=<AddBackward0>)\n",
            "tensor(42.4922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 801\n",
            "tensor(0.4640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.5212, grad_fn=<AddBackward0>)\n",
            "tensor(37.9852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 802\n",
            "tensor(0.4648, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6545, grad_fn=<AddBackward0>)\n",
            "tensor(41.1193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 803\n",
            "tensor(0.4664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.0134, grad_fn=<AddBackward0>)\n",
            "tensor(37.4797, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 804\n",
            "tensor(0.4685, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(49.2932, grad_fn=<AddBackward0>)\n",
            "tensor(49.7617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 805\n",
            "tensor(0.4717, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.3746, grad_fn=<AddBackward0>)\n",
            "tensor(40.8463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 806\n",
            "tensor(0.4755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0607, grad_fn=<AddBackward0>)\n",
            "tensor(42.5362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 807\n",
            "tensor(0.4794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.7220, grad_fn=<AddBackward0>)\n",
            "tensor(36.2013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 808\n",
            "tensor(0.4790, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.3165, grad_fn=<AddBackward0>)\n",
            "tensor(42.7955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 809\n",
            "tensor(0.4774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.5568, grad_fn=<AddBackward0>)\n",
            "tensor(36.0342, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 810\n",
            "tensor(0.4760, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4701, grad_fn=<AddBackward0>)\n",
            "tensor(38.9461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 811\n",
            "tensor(0.4747, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.7843, grad_fn=<AddBackward0>)\n",
            "tensor(36.2591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 812\n",
            "tensor(0.4719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8848, grad_fn=<AddBackward0>)\n",
            "tensor(43.3567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 813\n",
            "tensor(0.4667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4856, grad_fn=<AddBackward0>)\n",
            "tensor(38.9523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 814\n",
            "tensor(0.4616, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.9997, grad_fn=<AddBackward0>)\n",
            "tensor(34.4612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 815\n",
            "tensor(0.4623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2677, grad_fn=<AddBackward0>)\n",
            "tensor(39.7300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 816\n",
            "tensor(0.4649, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.3489, grad_fn=<AddBackward0>)\n",
            "tensor(31.8139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 817\n",
            "tensor(0.4675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.8482, grad_fn=<AddBackward0>)\n",
            "tensor(36.3157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 818\n",
            "tensor(0.4686, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1166, grad_fn=<AddBackward0>)\n",
            "tensor(42.5852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 819\n",
            "tensor(0.4655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1705, grad_fn=<AddBackward0>)\n",
            "tensor(36.6360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 820\n",
            "tensor(0.4612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.0376, grad_fn=<AddBackward0>)\n",
            "tensor(31.4988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 821\n",
            "tensor(0.4569, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3098, grad_fn=<AddBackward0>)\n",
            "tensor(43.7667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 822\n",
            "tensor(0.4549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.6512, grad_fn=<AddBackward0>)\n",
            "tensor(40.1061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 823\n",
            "tensor(0.4560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.0943, grad_fn=<AddBackward0>)\n",
            "tensor(37.5503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 824\n",
            "tensor(0.4564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(29.7836, grad_fn=<AddBackward0>)\n",
            "tensor(30.2400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 825\n",
            "tensor(0.4588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1519, grad_fn=<AddBackward0>)\n",
            "tensor(36.6106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 826\n",
            "tensor(0.4632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1559, grad_fn=<AddBackward0>)\n",
            "tensor(38.6191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 827\n",
            "tensor(0.4661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.4111, grad_fn=<AddBackward0>)\n",
            "tensor(37.8772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 828\n",
            "tensor(0.4677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0152, grad_fn=<AddBackward0>)\n",
            "tensor(35.4829, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 829\n",
            "tensor(0.4690, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.7528, grad_fn=<AddBackward0>)\n",
            "tensor(33.2219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 830\n",
            "tensor(0.4708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.2465, grad_fn=<AddBackward0>)\n",
            "tensor(42.7174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 831\n",
            "tensor(0.4696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.4098, grad_fn=<AddBackward0>)\n",
            "tensor(44.8794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 832\n",
            "tensor(0.4663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4067, grad_fn=<AddBackward0>)\n",
            "tensor(35.8730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 833\n",
            "tensor(0.4660, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.9082, grad_fn=<AddBackward0>)\n",
            "tensor(33.3743, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 834\n",
            "tensor(0.4662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3212, grad_fn=<AddBackward0>)\n",
            "tensor(43.7875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 835\n",
            "tensor(0.4674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.1100, grad_fn=<AddBackward0>)\n",
            "tensor(40.5774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 836\n",
            "tensor(0.4695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6643, grad_fn=<AddBackward0>)\n",
            "tensor(37.1339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 837\n",
            "tensor(0.4699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.1239, grad_fn=<AddBackward0>)\n",
            "tensor(31.5938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 838\n",
            "tensor(0.4683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.8091, grad_fn=<AddBackward0>)\n",
            "tensor(41.2774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 839\n",
            "tensor(0.4664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.9349, grad_fn=<AddBackward0>)\n",
            "tensor(44.4014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 840\n",
            "tensor(0.4642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5401, grad_fn=<AddBackward0>)\n",
            "tensor(42.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 841\n",
            "tensor(0.4613, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7249, grad_fn=<AddBackward0>)\n",
            "tensor(37.1862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 842\n",
            "tensor(0.4589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.6347, grad_fn=<AddBackward0>)\n",
            "tensor(38.0937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 843\n",
            "tensor(0.4583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(29.4109, grad_fn=<AddBackward0>)\n",
            "tensor(29.8692, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 844\n",
            "tensor(0.4588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9638, grad_fn=<AddBackward0>)\n",
            "tensor(35.4226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 845\n",
            "tensor(0.4583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9858, grad_fn=<AddBackward0>)\n",
            "tensor(35.4440, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 846\n",
            "tensor(0.4564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.1828, grad_fn=<AddBackward0>)\n",
            "tensor(33.6391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 847\n",
            "tensor(0.4543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2063, grad_fn=<AddBackward0>)\n",
            "tensor(39.6605, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 848\n",
            "tensor(0.4532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.1556, grad_fn=<AddBackward0>)\n",
            "tensor(36.6088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 849\n",
            "tensor(0.4557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.2542, grad_fn=<AddBackward0>)\n",
            "tensor(35.7099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 850\n",
            "tensor(0.4583, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.5210, grad_fn=<AddBackward0>)\n",
            "tensor(35.9793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 851\n",
            "tensor(0.4616, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4586, grad_fn=<AddBackward0>)\n",
            "tensor(38.9202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 852\n",
            "tensor(0.4604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.5111, grad_fn=<AddBackward0>)\n",
            "tensor(38.9715, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 853\n",
            "tensor(0.4574, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8677, grad_fn=<AddBackward0>)\n",
            "tensor(40.3251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 854\n",
            "tensor(0.4533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.5433, grad_fn=<AddBackward0>)\n",
            "tensor(38.9966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 855\n",
            "tensor(0.4506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.7222, grad_fn=<AddBackward0>)\n",
            "tensor(42.1728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 856\n",
            "tensor(0.4506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.9226, grad_fn=<AddBackward0>)\n",
            "tensor(40.3732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 857\n",
            "tensor(0.4522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.0131, grad_fn=<AddBackward0>)\n",
            "tensor(37.4653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 858\n",
            "tensor(0.4546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.7902, grad_fn=<AddBackward0>)\n",
            "tensor(43.2449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 859\n",
            "tensor(0.4552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.1160, grad_fn=<AddBackward0>)\n",
            "tensor(35.5712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 860\n",
            "tensor(0.4564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.8701, grad_fn=<AddBackward0>)\n",
            "tensor(37.3265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 861\n",
            "tensor(0.4581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.5489, grad_fn=<AddBackward0>)\n",
            "tensor(42.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 862\n",
            "tensor(0.4590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.7908, grad_fn=<AddBackward0>)\n",
            "tensor(35.2497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 863\n",
            "tensor(0.4595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.3554, grad_fn=<AddBackward0>)\n",
            "tensor(38.8149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 864\n",
            "tensor(0.4614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.9570, grad_fn=<AddBackward0>)\n",
            "tensor(37.4185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 865\n",
            "tensor(0.4642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.8198, grad_fn=<AddBackward0>)\n",
            "tensor(37.2841, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 866\n",
            "tensor(0.4640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.9319, grad_fn=<AddBackward0>)\n",
            "tensor(35.3958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 867\n",
            "tensor(0.4632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6305, grad_fn=<AddBackward0>)\n",
            "tensor(37.0938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 868\n",
            "tensor(0.4617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.9781, grad_fn=<AddBackward0>)\n",
            "tensor(37.4397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 869\n",
            "tensor(0.4591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.1951, grad_fn=<AddBackward0>)\n",
            "tensor(37.6542, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 870\n",
            "tensor(0.4576, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.4843, grad_fn=<AddBackward0>)\n",
            "tensor(37.9418, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 871\n",
            "tensor(0.4581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.6465, grad_fn=<AddBackward0>)\n",
            "tensor(34.1046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 872\n",
            "tensor(0.4586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.6590, grad_fn=<AddBackward0>)\n",
            "tensor(33.1176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 873\n",
            "tensor(0.4568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(44.1842, grad_fn=<AddBackward0>)\n",
            "tensor(44.6410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 874\n",
            "tensor(0.4532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8733, grad_fn=<AddBackward0>)\n",
            "tensor(43.3264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 875\n",
            "tensor(0.4495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.3364, grad_fn=<AddBackward0>)\n",
            "tensor(40.7859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 876\n",
            "tensor(0.4465, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.2540, grad_fn=<AddBackward0>)\n",
            "tensor(37.7005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 877\n",
            "tensor(0.4457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.5611, grad_fn=<AddBackward0>)\n",
            "tensor(32.0068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 878\n",
            "tensor(0.4453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.0018, grad_fn=<AddBackward0>)\n",
            "tensor(34.4471, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 879\n",
            "tensor(0.4446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.0553, grad_fn=<AddBackward0>)\n",
            "tensor(38.4999, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 880\n",
            "tensor(0.4432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8730, grad_fn=<AddBackward0>)\n",
            "tensor(38.3162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 881\n",
            "tensor(0.4422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.1159, grad_fn=<AddBackward0>)\n",
            "tensor(41.5581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 882\n",
            "tensor(0.4414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6159, grad_fn=<AddBackward0>)\n",
            "tensor(37.0573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 883\n",
            "tensor(0.4451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.7011, grad_fn=<AddBackward0>)\n",
            "tensor(35.1462, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 884\n",
            "tensor(0.4533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.6065, grad_fn=<AddBackward0>)\n",
            "tensor(43.0598, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 885\n",
            "tensor(0.4606, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4486, grad_fn=<AddBackward0>)\n",
            "tensor(39.9092, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 886\n",
            "tensor(0.4574, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0230, grad_fn=<AddBackward0>)\n",
            "tensor(42.4804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 887\n",
            "tensor(0.4566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.7947, grad_fn=<AddBackward0>)\n",
            "tensor(34.2513, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 888\n",
            "tensor(0.4557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.9475, grad_fn=<AddBackward0>)\n",
            "tensor(39.4032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 889\n",
            "tensor(0.4553, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.1769, grad_fn=<AddBackward0>)\n",
            "tensor(33.6322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 890\n",
            "tensor(0.4563, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.5789, grad_fn=<AddBackward0>)\n",
            "tensor(41.0352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 891\n",
            "tensor(0.4575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(28.8250, grad_fn=<AddBackward0>)\n",
            "tensor(29.2826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 892\n",
            "tensor(0.4602, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.1971, grad_fn=<AddBackward0>)\n",
            "tensor(39.6573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 893\n",
            "tensor(0.4638, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.4346, grad_fn=<AddBackward0>)\n",
            "tensor(42.8984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 894\n",
            "tensor(0.4662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(30.7442, grad_fn=<AddBackward0>)\n",
            "tensor(31.2104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 895\n",
            "tensor(0.4673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4492, grad_fn=<AddBackward0>)\n",
            "tensor(35.9165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 896\n",
            "tensor(0.4660, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0903, grad_fn=<AddBackward0>)\n",
            "tensor(35.5564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 897\n",
            "tensor(0.4634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.8156, grad_fn=<AddBackward0>)\n",
            "tensor(43.2790, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 898\n",
            "tensor(0.4603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0408, grad_fn=<AddBackward0>)\n",
            "tensor(35.5011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 899\n",
            "tensor(0.4588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.2450, grad_fn=<AddBackward0>)\n",
            "tensor(33.7038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 900\n",
            "tensor(0.4559, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.3290, grad_fn=<AddBackward0>)\n",
            "tensor(36.7849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 901\n",
            "tensor(0.4525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8915, grad_fn=<AddBackward0>)\n",
            "tensor(38.3440, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 902\n",
            "tensor(0.4497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5422, grad_fn=<AddBackward0>)\n",
            "tensor(36.9919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 903\n",
            "tensor(0.4478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.7070, grad_fn=<AddBackward0>)\n",
            "tensor(36.1548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 904\n",
            "tensor(0.4481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8104, grad_fn=<AddBackward0>)\n",
            "tensor(40.2585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 905\n",
            "tensor(0.4497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.3287, grad_fn=<AddBackward0>)\n",
            "tensor(35.7784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 906\n",
            "tensor(0.4508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.3109, grad_fn=<AddBackward0>)\n",
            "tensor(40.7616, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 907\n",
            "tensor(0.4516, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.7508, grad_fn=<AddBackward0>)\n",
            "tensor(41.2023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 908\n",
            "tensor(0.4521, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(30.2821, grad_fn=<AddBackward0>)\n",
            "tensor(30.7342, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 909\n",
            "tensor(0.4528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.8033, grad_fn=<AddBackward0>)\n",
            "tensor(33.2562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 910\n",
            "tensor(0.4534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.1939, grad_fn=<AddBackward0>)\n",
            "tensor(40.6472, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 911\n",
            "tensor(0.4521, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7785, grad_fn=<AddBackward0>)\n",
            "tensor(37.2306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 912\n",
            "tensor(0.4484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(48.5842, grad_fn=<AddBackward0>)\n",
            "tensor(49.0327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 913\n",
            "tensor(0.4437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.7453, grad_fn=<AddBackward0>)\n",
            "tensor(32.1890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 914\n",
            "tensor(0.4401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7381, grad_fn=<AddBackward0>)\n",
            "tensor(39.1782, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 915\n",
            "tensor(0.4404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7091, grad_fn=<AddBackward0>)\n",
            "tensor(37.1495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 916\n",
            "tensor(0.4413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.5742, grad_fn=<AddBackward0>)\n",
            "tensor(38.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 917\n",
            "tensor(0.4413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.0928, grad_fn=<AddBackward0>)\n",
            "tensor(42.5341, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 918\n",
            "tensor(0.4408, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4611, grad_fn=<AddBackward0>)\n",
            "tensor(39.9019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 919\n",
            "tensor(0.4380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.9376, grad_fn=<AddBackward0>)\n",
            "tensor(38.3756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 920\n",
            "tensor(0.4383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.1842, grad_fn=<AddBackward0>)\n",
            "tensor(37.6225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 921\n",
            "tensor(0.4386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.6942, grad_fn=<AddBackward0>)\n",
            "tensor(40.1329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 922\n",
            "tensor(0.4385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.0756, grad_fn=<AddBackward0>)\n",
            "tensor(37.5140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 923\n",
            "tensor(0.4371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.9250, grad_fn=<AddBackward0>)\n",
            "tensor(43.3621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 924\n",
            "tensor(0.4361, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.0840, grad_fn=<AddBackward0>)\n",
            "tensor(33.5200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 925\n",
            "tensor(0.4367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.8144, grad_fn=<AddBackward0>)\n",
            "tensor(34.2511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 926\n",
            "tensor(0.4383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.4289, grad_fn=<AddBackward0>)\n",
            "tensor(31.8672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 927\n",
            "tensor(0.4409, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8943, grad_fn=<AddBackward0>)\n",
            "tensor(38.3352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 928\n",
            "tensor(0.4437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.6047, grad_fn=<AddBackward0>)\n",
            "tensor(38.0483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 929\n",
            "tensor(0.4450, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.7020, grad_fn=<AddBackward0>)\n",
            "tensor(38.1470, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 930\n",
            "tensor(0.4480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.7112, grad_fn=<AddBackward0>)\n",
            "tensor(39.1592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 931\n",
            "tensor(0.4482, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.9373, grad_fn=<AddBackward0>)\n",
            "tensor(39.3855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 932\n",
            "tensor(0.4469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.2815, grad_fn=<AddBackward0>)\n",
            "tensor(35.7283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 933\n",
            "tensor(0.4451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6037, grad_fn=<AddBackward0>)\n",
            "tensor(44.0488, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 934\n",
            "tensor(0.4448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.2896, grad_fn=<AddBackward0>)\n",
            "tensor(37.7344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 935\n",
            "tensor(0.4439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.1535, grad_fn=<AddBackward0>)\n",
            "tensor(38.5974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 936\n",
            "tensor(0.4401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.6582, grad_fn=<AddBackward0>)\n",
            "tensor(37.0983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 937\n",
            "tensor(0.4398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.9728, grad_fn=<AddBackward0>)\n",
            "tensor(38.4126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 938\n",
            "tensor(0.4395, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.3585, grad_fn=<AddBackward0>)\n",
            "tensor(41.7980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 939\n",
            "tensor(0.4388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.9953, grad_fn=<AddBackward0>)\n",
            "tensor(32.4341, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 940\n",
            "tensor(0.4391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.6164, grad_fn=<AddBackward0>)\n",
            "tensor(41.0555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 941\n",
            "tensor(0.4452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.3649, grad_fn=<AddBackward0>)\n",
            "tensor(39.8100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 942\n",
            "tensor(0.4408, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.5379, grad_fn=<AddBackward0>)\n",
            "tensor(42.9787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 943\n",
            "tensor(0.4396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.5971, grad_fn=<AddBackward0>)\n",
            "tensor(33.0367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 944\n",
            "tensor(0.4391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.3130, grad_fn=<AddBackward0>)\n",
            "tensor(32.7521, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 945\n",
            "tensor(0.4381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5128, grad_fn=<AddBackward0>)\n",
            "tensor(36.9509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 946\n",
            "tensor(0.4361, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.2038, grad_fn=<AddBackward0>)\n",
            "tensor(40.6399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 947\n",
            "tensor(0.4348, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9802, grad_fn=<AddBackward0>)\n",
            "tensor(41.4150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 948\n",
            "tensor(0.4352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.7007, grad_fn=<AddBackward0>)\n",
            "tensor(33.1359, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 949\n",
            "tensor(0.4365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(25.2682, grad_fn=<AddBackward0>)\n",
            "tensor(25.7047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 950\n",
            "tensor(0.4389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.5947, grad_fn=<AddBackward0>)\n",
            "tensor(37.0336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 951\n",
            "tensor(0.4393, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.9101, grad_fn=<AddBackward0>)\n",
            "tensor(38.3493, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 952\n",
            "tensor(0.4382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.2046, grad_fn=<AddBackward0>)\n",
            "tensor(39.6427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 953\n",
            "tensor(0.4347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.6653, grad_fn=<AddBackward0>)\n",
            "tensor(34.1000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 954\n",
            "tensor(0.4320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.1417, grad_fn=<AddBackward0>)\n",
            "tensor(45.5737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 955\n",
            "tensor(0.4306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.5522, grad_fn=<AddBackward0>)\n",
            "tensor(39.9828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 956\n",
            "tensor(0.4307, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6022, grad_fn=<AddBackward0>)\n",
            "tensor(39.0330, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 957\n",
            "tensor(0.4315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(41.0419, grad_fn=<AddBackward0>)\n",
            "tensor(41.4734, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 958\n",
            "tensor(0.4325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.2424, grad_fn=<AddBackward0>)\n",
            "tensor(42.6749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 959\n",
            "tensor(0.4327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.4170, grad_fn=<AddBackward0>)\n",
            "tensor(36.8498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 960\n",
            "tensor(0.4320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.1010, grad_fn=<AddBackward0>)\n",
            "tensor(37.5330, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 961\n",
            "tensor(0.4305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.3920, grad_fn=<AddBackward0>)\n",
            "tensor(37.8224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 962\n",
            "tensor(0.4298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(33.4722, grad_fn=<AddBackward0>)\n",
            "tensor(33.9020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 963\n",
            "tensor(0.4319, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(30.3629, grad_fn=<AddBackward0>)\n",
            "tensor(30.7947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 964\n",
            "tensor(0.4365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.6810, grad_fn=<AddBackward0>)\n",
            "tensor(44.1175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 965\n",
            "tensor(0.4422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.7765, grad_fn=<AddBackward0>)\n",
            "tensor(37.2187, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 966\n",
            "tensor(0.4458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.0870, grad_fn=<AddBackward0>)\n",
            "tensor(35.5328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 967\n",
            "tensor(0.4469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.1391, grad_fn=<AddBackward0>)\n",
            "tensor(42.5861, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 968\n",
            "tensor(0.4468, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.8020, grad_fn=<AddBackward0>)\n",
            "tensor(35.2487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 969\n",
            "tensor(0.4468, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.0547, grad_fn=<AddBackward0>)\n",
            "tensor(32.5014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 970\n",
            "tensor(0.4479, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.7709, grad_fn=<AddBackward0>)\n",
            "tensor(36.2188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 971\n",
            "tensor(0.4486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(40.9760, grad_fn=<AddBackward0>)\n",
            "tensor(41.4246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 972\n",
            "tensor(0.4494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.5226, grad_fn=<AddBackward0>)\n",
            "tensor(32.9721, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 973\n",
            "tensor(0.4499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(43.3739, grad_fn=<AddBackward0>)\n",
            "tensor(43.8237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 974\n",
            "tensor(0.4487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.2017, grad_fn=<AddBackward0>)\n",
            "tensor(38.6503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 975\n",
            "tensor(0.4480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.7583, grad_fn=<AddBackward0>)\n",
            "tensor(35.2062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 976\n",
            "tensor(0.4473, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4320, grad_fn=<AddBackward0>)\n",
            "tensor(35.8792, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 977\n",
            "tensor(0.4478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.6660, grad_fn=<AddBackward0>)\n",
            "tensor(36.1138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 978\n",
            "tensor(0.4484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.4113, grad_fn=<AddBackward0>)\n",
            "tensor(38.8597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 979\n",
            "tensor(0.4496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.8387, grad_fn=<AddBackward0>)\n",
            "tensor(38.2883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 980\n",
            "tensor(0.4501, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.2070, grad_fn=<AddBackward0>)\n",
            "tensor(37.6571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 981\n",
            "tensor(0.4496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.6168, grad_fn=<AddBackward0>)\n",
            "tensor(35.0664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 982\n",
            "tensor(0.4490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(42.7707, grad_fn=<AddBackward0>)\n",
            "tensor(43.2197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 983\n",
            "tensor(0.4490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.7400, grad_fn=<AddBackward0>)\n",
            "tensor(40.1890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 984\n",
            "tensor(0.4504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.4476, grad_fn=<AddBackward0>)\n",
            "tensor(37.8980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 985\n",
            "tensor(0.4509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.4605, grad_fn=<AddBackward0>)\n",
            "tensor(36.9114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 986\n",
            "tensor(0.4494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.6243, grad_fn=<AddBackward0>)\n",
            "tensor(32.0737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 987\n",
            "tensor(0.4464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(38.6309, grad_fn=<AddBackward0>)\n",
            "tensor(39.0773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 988\n",
            "tensor(0.4426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.5971, grad_fn=<AddBackward0>)\n",
            "tensor(33.0396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 989\n",
            "tensor(0.4405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(34.1601, grad_fn=<AddBackward0>)\n",
            "tensor(34.6006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 990\n",
            "tensor(0.4422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.8407, grad_fn=<AddBackward0>)\n",
            "tensor(33.2829, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 991\n",
            "tensor(0.4435, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.4621, grad_fn=<AddBackward0>)\n",
            "tensor(32.9056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 992\n",
            "tensor(0.4416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(35.4018, grad_fn=<AddBackward0>)\n",
            "tensor(35.8434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 993\n",
            "tensor(0.4394, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(36.4799, grad_fn=<AddBackward0>)\n",
            "tensor(36.9194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 994\n",
            "tensor(0.4380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(45.4894, grad_fn=<AddBackward0>)\n",
            "tensor(45.9274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 995\n",
            "tensor(0.4384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.8769, grad_fn=<AddBackward0>)\n",
            "tensor(40.3153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 996\n",
            "tensor(0.4380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(31.1101, grad_fn=<AddBackward0>)\n",
            "tensor(31.5480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 997\n",
            "tensor(0.4384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(37.2093, grad_fn=<AddBackward0>)\n",
            "tensor(37.6478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 998\n",
            "tensor(0.4371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(39.4162, grad_fn=<AddBackward0>)\n",
            "tensor(39.8534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "step 999\n",
            "tensor(0.4341, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(32.2733, grad_fn=<AddBackward0>)\n",
            "tensor(32.7073, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-bCfgvJAfQY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}